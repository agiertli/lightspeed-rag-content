<html><body><section class="chapter" id="about-openshift-container-platform-monitoring"><div class="titlepage"><div><div><h2 class="title">Chapter 1. About OpenShift Container Platform monitoring</h2></div></div></div><section class="section" id="about-ocp-monitoring"><div class="titlepage"><div><div><h3 class="title">1.1. About OpenShift Container Platform monitoring</h3></div></div></div><p>
				OpenShift Container Platform includes a preconfigured, preinstalled, and self-updating monitoring stack that provides monitoring for core platform components. You also have the option to <a class="link" href="#enabling-monitoring-for-user-defined-projects-uwm_preparing-to-configure-the-monitoring-stack-uwm" title="4.1.2. Enabling monitoring for user-defined projects">enable monitoring for user-defined projects</a>.
			</p><p>
				A cluster administrator can <a class="link" href="#preparing-to-configure-the-monitoring-stack" title="3.1. Preparing to configure core platform monitoring stack">configure the monitoring stack</a> with the supported configurations. OpenShift Container Platform delivers monitoring best practices out of the box.
			</p><p>
				A set of alerts are included by default that immediately notify administrators about issues with a cluster. Default dashboards in the OpenShift Container Platform web console include visual representations of cluster metrics to help you to quickly understand the state of your cluster. With the OpenShift Container Platform web console, you can <a class="link" href="#accessing-metrics-as-an-administrator" title="5.1. Accessing metrics as an administrator">access metrics</a> and <a class="link" href="#managing-alerts-as-an-administrator" title="6.1. Managing alerts as an Administrator">manage alerts</a>.
			</p><p>
				After installing OpenShift Container Platform, cluster administrators can optionally enable monitoring for user-defined projects. By using this feature, cluster administrators, developers, and other users can specify how services and pods are monitored in their own projects. As a cluster administrator, you can find answers to common problems such as user metrics unavailability and high consumption of disk space by Prometheus in <a class="link" href="#troubleshooting-monitoring-issues" title="Chapter 7. Troubleshooting monitoring issues">Troubleshooting monitoring issues</a>.
			</p></section><section class="section" id="monitoring-stack-architecture"><div class="titlepage"><div><div><h3 class="title">1.2. Monitoring stack architecture</h3></div></div></div><p>
				The OpenShift Container Platform monitoring stack is based on the <a class="link" href="https://prometheus.io/">Prometheus</a> open source project and its wider ecosystem. The monitoring stack includes default monitoring components and components for monitoring user-defined projects.
			</p><section class="section" id="understanding-the-monitoring-stack_monitoring-stack-architecture"><div class="titlepage"><div><div><h4 class="title">1.2.1. Understanding the monitoring stack</h4></div></div></div><p>
					The monitoring stack includes the following components:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
<span class="strong strong"><strong>Default platform monitoring components</strong></span>. A set of platform monitoring components are installed in the <code class="literal">openshift-monitoring</code> project by default during an OpenShift Container Platform installation. This provides monitoring for core cluster components including Kubernetes services. The default monitoring stack also enables remote health monitoring for clusters.
						</p><p class="simpara">
							These components are illustrated in the <span class="strong strong"><strong>Installed by default</strong></span> section in the following diagram.
						</p></li><li class="listitem">
<span class="strong strong"><strong>Components for monitoring user-defined projects</strong></span>. After optionally enabling monitoring for user-defined projects, additional monitoring components are installed in the <code class="literal">openshift-user-workload-monitoring</code> project. This provides monitoring for user-defined projects. These components are illustrated in the <span class="strong strong"><strong>User</strong></span> section in the following diagram.
						</li></ul></div><p>
<span class="inlinemediaobject"><img alt="OpenShift Container Platform monitoring architecture" src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.18-Monitoring-en-US/images/12da7e6ce924595ca9b401f4f964447c/monitoring-architecture.png"/></span>
</p></section><section class="section" id="default-monitoring-components_monitoring-stack-architecture"><div class="titlepage"><div><div><h4 class="title">1.2.2. Default monitoring components</h4></div></div></div><p>
					By default, the OpenShift Container Platform 4.18 monitoring stack includes these components:
				</p><rh-table id="idm140059304528144"><table class="lt-4-cols lt-7-rows"><caption>Table 1.1. Default monitoring stack components</caption><colgroup><col class="col_1" style="width: 50%; "/><!--Empty--><col class="col_2" style="width: 50%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059303733920" scope="col" valign="top">Component</th><th align="left" id="idm140059303732832" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059303733920" valign="top"> <p>
									Cluster Monitoring Operator
								</p>
</td><td align="left" headers="idm140059303732832" valign="top"> <p>
									The Cluster Monitoring Operator (CMO) is a central component of the monitoring stack. It deploys, manages, and automatically updates Prometheus and Alertmanager instances, Thanos Querier, Telemeter Client, and metrics targets. The CMO is deployed by the Cluster Version Operator (CVO).
								</p>
</td></tr><tr><td align="left" headers="idm140059303733920" valign="top"> <p>
									Prometheus Operator
								</p>
</td><td align="left" headers="idm140059303732832" valign="top"> <p>
									The Prometheus Operator (PO) in the <code class="literal">openshift-monitoring</code> project creates, configures, and manages platform Prometheus instances and Alertmanager instances. It also automatically generates monitoring target configurations based on Kubernetes label queries.
								</p>
</td></tr><tr><td align="left" headers="idm140059303733920" valign="top"> <p>
									Prometheus
								</p>
</td><td align="left" headers="idm140059303732832" valign="top"> <p>
									Prometheus is the monitoring system on which the OpenShift Container Platform monitoring stack is based. Prometheus is a time-series database and a rule evaluation engine for metrics. Prometheus sends alerts to Alertmanager for processing.
								</p>
</td></tr><tr><td align="left" headers="idm140059303733920" valign="top"> <p>
									Metrics Server
								</p>
</td><td align="left" headers="idm140059303732832" valign="top"> <p>
									The Metrics Server component (MS in the preceding diagram) collects resource metrics and exposes them in the <code class="literal">metrics.k8s.io</code> Metrics API service for use by other tools and APIs, which frees the core platform Prometheus stack from handling this functionality. Note that with the OpenShift Container Platform 4.16 release, Metrics Server replaces Prometheus Adapter.
								</p>
</td></tr><tr><td align="left" headers="idm140059303733920" valign="top"> <p>
									Alertmanager
								</p>
</td><td align="left" headers="idm140059303732832" valign="top"> <p>
									The Alertmanager service handles alerts received from Prometheus. Alertmanager is also responsible for sending the alerts to external notification systems.
								</p>
</td></tr><tr><td align="left" headers="idm140059303733920" valign="top"> <p>
									kube-state-metrics agent
								</p>
</td><td align="left" headers="idm140059303732832" valign="top"> <p>
									The kube-state-metrics exporter agent (KSM in the preceding diagram) converts Kubernetes objects to metrics that Prometheus can use.
								</p>
</td></tr><tr><td align="left" headers="idm140059303733920" valign="top"> <p>
									monitoring-plugin
								</p>
</td><td align="left" headers="idm140059303732832" valign="top"> <p>
									The monitoring-plugin dynamic plugin component deploys the monitoring pages in the <span class="strong strong"><strong>Observe</strong></span> section of the OpenShift Container Platform web console. You can use Cluster Monitoring Operator config map settings to manage monitoring-plugin resources for the web console pages.
								</p>
</td></tr><tr><td align="left" headers="idm140059303733920" valign="top"> <p>
									openshift-state-metrics agent
								</p>
</td><td align="left" headers="idm140059303732832" valign="top"> <p>
									The openshift-state-metrics exporter (OSM in the preceding diagram) expands upon kube-state-metrics by adding metrics for OpenShift Container Platform-specific resources.
								</p>
</td></tr><tr><td align="left" headers="idm140059303733920" valign="top"> <p>
									node-exporter agent
								</p>
</td><td align="left" headers="idm140059303732832" valign="top"> <p>
									The node-exporter agent (NE in the preceding diagram) collects metrics about every node in a cluster. The node-exporter agent is deployed on every node.
								</p>
</td></tr><tr><td align="left" headers="idm140059303733920" valign="top"> <p>
									Thanos Querier
								</p>
</td><td align="left" headers="idm140059303732832" valign="top"> <p>
									Thanos Querier aggregates and optionally deduplicates core OpenShift Container Platform metrics and metrics for user-defined projects under a single, multi-tenant interface.
								</p>
</td></tr><tr><td align="left" headers="idm140059303733920" valign="top"> <p>
									Telemeter Client
								</p>
</td><td align="left" headers="idm140059303732832" valign="top"> <p>
									Telemeter Client sends a subsection of the data from platform Prometheus instances to Red Hat to facilitate Remote Health Monitoring for clusters.
								</p>
</td></tr></tbody></table></rh-table><p>
					All of the components in the monitoring stack are monitored by the stack and are automatically updated when OpenShift Container Platform is updated.
				</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						All components of the monitoring stack use the TLS security profile settings that are centrally configured by a cluster administrator. If you configure a monitoring stack component that uses TLS security settings, the component uses the TLS security profile settings that already exist in the <code class="literal">tlsSecurityProfile</code> field in the global OpenShift Container Platform <code class="literal">apiservers.config.openshift.io/cluster</code> resource.
					</p></div></rh-alert><section class="section" id="default-monitoring-targets_monitoring-stack-architecture"><div class="titlepage"><div><div><h5 class="title">1.2.2.1. Default monitoring targets</h5></div></div></div><p>
						In addition to the components of the stack itself, the default monitoring stack monitors additional platform components.
					</p><p>
						The following are examples of monitoring targets:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								CoreDNS
							</li><li class="listitem">
								etcd
							</li><li class="listitem">
								HAProxy
							</li><li class="listitem">
								Image registry
							</li><li class="listitem">
								Kubelets
							</li><li class="listitem">
								Kubernetes API server
							</li><li class="listitem">
								Kubernetes controller manager
							</li><li class="listitem">
								Kubernetes scheduler
							</li><li class="listitem">
								OpenShift API server
							</li><li class="listitem">
								OpenShift Controller Manager
							</li><li class="listitem">
								Operator Lifecycle Manager (OLM)
							</li></ul></div><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									The exact list of targets can vary depending on your cluster capabilities and installed components.
								</li><li class="listitem">
									Each OpenShift Container Platform component is responsible for its monitoring configuration. For problems with the monitoring of an OpenShift Container Platform component, open a <a class="link" href="https://issues.redhat.com/secure/CreateIssueDetails!init.jspa?pid=12332330&amp;summary=Monitoring_issue&amp;issuetype=1&amp;priority=10200&amp;versions=12417854">Jira issue</a> against that component, not against the general monitoring component.
								</li></ul></div></div></rh-alert><p>
						Other OpenShift Container Platform framework components might be exposing metrics as well. For details, see their respective documentation.
					</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#getting-detailed-information-about-a-target_accessing-metrics-as-an-administrator" title="5.1.3. Getting detailed information about a metrics target">Getting detailed information about a metrics target</a>
</li></ul></div></section></section><section class="section" id="components-for-monitoring-user-defined-projects_monitoring-stack-architecture"><div class="titlepage"><div><div><h4 class="title">1.2.3. Components for monitoring user-defined projects</h4></div></div></div><p>
					OpenShift Container Platform includes an optional enhancement to the monitoring stack that enables you to monitor services and pods in user-defined projects. This feature includes the following components:
				</p><rh-table id="idm140059305476208"><table class="lt-4-cols lt-7-rows"><caption>Table 1.2. Components for monitoring user-defined projects</caption><colgroup><col class="col_1" style="width: 50%; "/><!--Empty--><col class="col_2" style="width: 50%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059305471344" scope="col" valign="top">Component</th><th align="left" id="idm140059301722640" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059305471344" valign="top"> <p>
									Prometheus Operator
								</p>
</td><td align="left" headers="idm140059301722640" valign="top"> <p>
									The Prometheus Operator (PO) in the <code class="literal">openshift-user-workload-monitoring</code> project creates, configures, and manages Prometheus and Thanos Ruler instances in the same project.
								</p>
</td></tr><tr><td align="left" headers="idm140059305471344" valign="top"> <p>
									Prometheus
								</p>
</td><td align="left" headers="idm140059301722640" valign="top"> <p>
									Prometheus is the monitoring system through which monitoring is provided for user-defined projects. Prometheus sends alerts to Alertmanager for processing.
								</p>
</td></tr><tr><td align="left" headers="idm140059305471344" valign="top"> <p>
									Thanos Ruler
								</p>
</td><td align="left" headers="idm140059301722640" valign="top"> <p>
									The Thanos Ruler is a rule evaluation engine for Prometheus that is deployed as a separate process. In OpenShift Container Platform , Thanos Ruler provides rule and alerting evaluation for the monitoring of user-defined projects.
								</p>
</td></tr><tr><td align="left" headers="idm140059305471344" valign="top"> <p>
									Alertmanager
								</p>
</td><td align="left" headers="idm140059301722640" valign="top"> <p>
									The Alertmanager service handles alerts received from Prometheus and Thanos Ruler. Alertmanager is also responsible for sending user-defined alerts to external notification systems. Deploying this service is optional.
								</p>
</td></tr></tbody></table></rh-table><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						The components in the preceding table are deployed after monitoring is enabled for user-defined projects.
					</p></div></rh-alert><p>
					All of these components are monitored by the stack and are automatically updated when OpenShift Container Platform is updated.
				</p><section class="section" id="monitoring-targets-for-user-defined-projects_monitoring-stack-architecture"><div class="titlepage"><div><div><h5 class="title">1.2.3.1. Monitoring targets for user-defined projects</h5></div></div></div><p>
						When monitoring is enabled for user-defined projects, you can monitor:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Metrics provided through service endpoints in user-defined projects.
							</li><li class="listitem">
								Pods running in user-defined projects.
							</li></ul></div></section></section><section class="section" id="monitoring-stack-in-ha-clusters_monitoring-stack-architecture"><div class="titlepage"><div><div><h4 class="title">1.2.4. The monitoring stack in high-availability clusters</h4></div></div></div><p>
					By default, in multi-node clusters, the following components run in high-availability (HA) mode to prevent data loss and service interruption:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Prometheus
						</li><li class="listitem">
							Alertmanager
						</li><li class="listitem">
							Thanos Ruler
						</li><li class="listitem">
							Thanos Querier
						</li><li class="listitem">
							Metrics Server
						</li><li class="listitem">
							Monitoring plugin
						</li></ul></div><p>
					The component is replicated across two pods, each running on a separate node. This means that the monitoring stack can tolerate the loss of one pod.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Prometheus in HA mode</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Both replicas independently scrape the same targets and evaluate the same rules.
									</li><li class="listitem">
										The replicas do not communicate with each other. Therefore, data might differ between the pods.
									</li></ul></div></dd><dt><span class="term">Alertmanager in HA mode</span></dt><dd><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										The two replicas synchronize notification and silence states with each other. This ensures that each notification is sent at least once.
									</li><li class="listitem">
										If the replicas fail to communicate or if there is an issue on the receiving side, notifications are still sent, but they might be duplicated.
									</li></ul></div></dd></dl></div><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
						Prometheus, Alertmanager, and Thanos Ruler are stateful components. To ensure high availability, you must configure them with persistent storage.
					</p></div></rh-alert><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/operators/#osdk-ha-sno">High-availability or single-node cluster detection and support</a>
</li><li class="listitem">
<a class="link" href="#configuring-persistent-storage_storing-and-recording-data" title="3.3.1. Configuring persistent storage">Configuring persistent storage</a>
</li><li class="listitem">
<a class="link" href="#configuring-performance-and-scalability" title="3.2. Configuring performance and scalability for core platform monitoring">Configuring performance and scalability</a>
</li></ul></div></section><section class="section" id="monitoring-common-terms_monitoring-stack-architecture"><div class="titlepage"><div><div><h4 class="title">1.2.5. Glossary of common terms for OpenShift Container Platform monitoring</h4></div></div></div><p>
					This glossary defines common terms that are used in OpenShift Container Platform architecture.
				</p><div class="variablelist"><dl class="variablelist"><dt><span class="term">Alertmanager</span></dt><dd>
								Alertmanager handles alerts received from Prometheus. Alertmanager is also responsible for sending the alerts to external notification systems.
							</dd><dt><span class="term">Alerting rules</span></dt><dd>
								Alerting rules contain a set of conditions that outline a particular state within a cluster. Alerts are triggered when those conditions are true. An alerting rule can be assigned a severity that defines how the alerts are routed.
							</dd><dt><span class="term">Cluster Monitoring Operator</span></dt><dd>
								The Cluster Monitoring Operator (CMO) is a central component of the monitoring stack. It deploys and manages Prometheus instances such as, the Thanos Querier, the Telemeter Client, and metrics targets to ensure that they are up to date. The CMO is deployed by the Cluster Version Operator (CVO).
							</dd><dt><span class="term">Cluster Version Operator</span></dt><dd>
								The Cluster Version Operator (CVO) manages the lifecycle of cluster Operators, many of which are installed in OpenShift Container Platform by default.
							</dd><dt><span class="term">config map</span></dt><dd>
								A config map provides a way to inject configuration data into pods. You can reference the data stored in a config map in a volume of type <code class="literal">ConfigMap</code>. Applications running in a pod can use this data.
							</dd><dt><span class="term">Container</span></dt><dd>
								A container is a lightweight and executable image that includes software and all its dependencies. Containers virtualize the operating system. As a result, you can run containers anywhere from a data center to a public or private cloud as well as a developer’s laptop.
							</dd><dt><span class="term">custom resource (CR)</span></dt><dd>
								A CR is an extension of the Kubernetes API. You can create custom resources.
							</dd><dt><span class="term">etcd</span></dt><dd>
								etcd is the key-value store for OpenShift Container Platform, which stores the state of all resource objects.
							</dd><dt><span class="term">Fluentd</span></dt><dd><p class="simpara">
								Fluentd is a log collector that resides on each OpenShift Container Platform node. It gathers application, infrastructure, and audit logs and forwards them to different outputs.
							</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
									Fluentd is deprecated and is planned to be removed in a future release. Red Hat provides bug fixes and support for this feature during the current release lifecycle, but this feature no longer receives enhancements. As an alternative to Fluentd, you can use Vector instead.
								</p></div></rh-alert></dd><dt><span class="term">Kubelets</span></dt><dd>
								Runs on nodes and reads the container manifests. Ensures that the defined containers have started and are running.
							</dd><dt><span class="term">Kubernetes API server</span></dt><dd>
								Kubernetes API server validates and configures data for the API objects.
							</dd><dt><span class="term">Kubernetes controller manager</span></dt><dd>
								Kubernetes controller manager governs the state of the cluster.
							</dd><dt><span class="term">Kubernetes scheduler</span></dt><dd>
								Kubernetes scheduler allocates pods to nodes.
							</dd><dt><span class="term">labels</span></dt><dd>
								Labels are key-value pairs that you can use to organize and select subsets of objects such as a pod.
							</dd><dt><span class="term">Metrics Server</span></dt><dd>
								The Metrics Server monitoring component collects resource metrics and exposes them in the <code class="literal">metrics.k8s.io</code> Metrics API service for use by other tools and APIs, which frees the core platform Prometheus stack from handling this functionality.
							</dd><dt><span class="term">node</span></dt><dd>
								A worker machine in the OpenShift Container Platform cluster. A node is either a virtual machine (VM) or a physical machine.
							</dd><dt><span class="term">Operator</span></dt><dd>
								The preferred method of packaging, deploying, and managing a Kubernetes application in an OpenShift Container Platform cluster. An Operator takes human operational knowledge and encodes it into software that is packaged and shared with customers.
							</dd><dt><span class="term">Operator Lifecycle Manager (OLM)</span></dt><dd>
								OLM helps you install, update, and manage the lifecycle of Kubernetes native applications. OLM is an open source toolkit designed to manage Operators in an effective, automated, and scalable way.
							</dd><dt><span class="term">Persistent storage</span></dt><dd>
								Stores the data even after the device is shut down. Kubernetes uses persistent volumes to store the application data.
							</dd><dt><span class="term">Persistent volume claim (PVC)</span></dt><dd>
								You can use a PVC to mount a PersistentVolume into a Pod. You can access the storage without knowing the details of the cloud environment.
							</dd><dt><span class="term">pod</span></dt><dd>
								The pod is the smallest logical unit in Kubernetes. A pod is comprised of one or more containers to run in a worker node.
							</dd><dt><span class="term">Prometheus</span></dt><dd>
								Prometheus is the monitoring system on which the OpenShift Container Platform monitoring stack is based. Prometheus is a time-series database and a rule evaluation engine for metrics. Prometheus sends alerts to Alertmanager for processing.
							</dd><dt><span class="term">Prometheus Operator</span></dt><dd>
								The Prometheus Operator (PO) in the <code class="literal">openshift-monitoring</code> project creates, configures, and manages platform Prometheus and Alertmanager instances. It also automatically generates monitoring target configurations based on Kubernetes label queries.
							</dd><dt><span class="term">Silences</span></dt><dd>
								A silence can be applied to an alert to prevent notifications from being sent when the conditions for an alert are true. You can mute an alert after the initial notification, while you work on resolving the underlying issue.
							</dd><dt><span class="term">storage</span></dt><dd>
								OpenShift Container Platform supports many types of storage, both for on-premise and cloud providers. You can manage container storage for persistent and non-persistent data in an OpenShift Container Platform cluster.
							</dd><dt><span class="term">Thanos Ruler</span></dt><dd>
								The Thanos Ruler is a rule evaluation engine for Prometheus that is deployed as a separate process. In OpenShift Container Platform, Thanos Ruler provides rule and alerting evaluation for the monitoring of user-defined projects.
							</dd><dt><span class="term">Vector</span></dt><dd>
								Vector is a log collector that deploys to each OpenShift Container Platform node. It collects log data from each node, transforms the data, and forwards it to configured outputs.
							</dd><dt><span class="term">web console</span></dt><dd>
								A user interface (UI) to manage OpenShift Container Platform.
							</dd></dl></div></section><section class="section _additional-resources" id="additional-resources_monitoring-stack-architecture"><div class="titlepage"><div><div><h4 class="title">1.2.6. Additional resources</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/support/#about-remote-health-monitoring">About remote health monitoring</a>
</li><li class="listitem">
<a class="link" href="#granting-users-permission-to-monitor-user-defined-projects_preparing-to-configure-the-monitoring-stack-uwm" title="4.1.4. Granting users permissions for monitoring for user-defined projects">Granting users permissions for monitoring for user-defined projects</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/security_and_compliance/#tls-security-profiles">Configuring TLS security profiles</a>
</li></ul></div></section></section><section class="section" id="key-concepts"><div class="titlepage"><div><div><h3 class="title">1.3. Understanding the monitoring stack - key concepts</h3></div></div></div><p>
				Get familiar with the OpenShift Container Platform monitoring concepts and terms. Learn about how you can improve performance and scale of your cluster, store and record data, manage metrics and alerts, and more.
			</p><section class="section" id="about-performance-and-scalability_key-concepts"><div class="titlepage"><div><div><h4 class="title">1.3.1. About performance and scalability</h4></div></div></div><p>
					You can optimize the performance and scale of your clusters. You can configure the default monitoring stack by performing any of the following actions:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Control the placement and distribution of monitoring components:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									Use node selectors to move components to specific nodes.
								</li><li class="listitem">
									Assign tolerations to enable moving components to tainted nodes.
								</li></ul></div></li><li class="listitem">
							Use pod topology spread constraints.
						</li><li class="listitem">
							Set the body size limit for metrics scraping.
						</li><li class="listitem">
							Manage CPU and memory resources.
						</li><li class="listitem">
							Use metrics collection profiles.
						</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#configuring-performance-and-scalability" title="3.2. Configuring performance and scalability for core platform monitoring">Configuring performance and scalability for core platform monitoring</a>
</li><li class="listitem">
<a class="link" href="#configuring-performance-and-scalability-uwm" title="4.2. Configuring performance and scalability for user workload monitoring">Configuring performance and scalability for user workload monitoring</a>
</li></ul></div><section class="section" id="using-node-selectors-to-move-monitoring-components_key-concepts"><div class="titlepage"><div><div><h5 class="title">1.3.1.1. Using node selectors to move monitoring components</h5></div></div></div><p>
						By using the <code class="literal">nodeSelector</code> constraint with labeled nodes, you can move any of the monitoring stack components to specific nodes. By doing so, you can control the placement and distribution of the monitoring components across a cluster.
					</p><p>
						By controlling placement and distribution of monitoring components, you can optimize system resource use, improve performance, and separate workloads based on specific requirements or policies.
					</p><h6 id="how-node-selectors-work-with-other-constraints">How node selectors work with other constraints</h6><p>
						If you move monitoring components by using node selector constraints, be aware that other constraints to control pod scheduling might exist for a cluster:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Topology spread constraints might be in place to control pod placement.
							</li><li class="listitem">
								Hard anti-affinity rules are in place for Prometheus, Alertmanager, and other monitoring components to ensure that multiple pods for these components are always spread across different nodes and are therefore always highly available.
							</li></ul></div><p>
						When scheduling pods onto nodes, the pod scheduler tries to satisfy all existing constraints when determining pod placement. That is, all constraints compound when the pod scheduler determines which pods will be placed on which nodes.
					</p><p>
						Therefore, if you configure a node selector constraint but existing constraints cannot all be satisfied, the pod scheduler cannot match all constraints and will not schedule a pod for placement onto a node.
					</p><p>
						To maintain resilience and high availability for monitoring components, ensure that enough nodes are available and match all constraints when you configure a node selector constraint to move a component.
					</p></section><section class="section" id="using-pod-topology-spread-constraints-for-monitoring_key-concepts"><div class="titlepage"><div><div><h5 class="title">1.3.1.2. About pod topology spread constraints for monitoring</h5></div></div></div><p>
						You can use pod topology spread constraints to control how the monitoring pods are spread across a network topology when OpenShift Container Platform pods are deployed in multiple availability zones.
					</p><p>
						Pod topology spread constraints are suitable for controlling pod scheduling within hierarchical topologies in which nodes are spread across different infrastructure levels, such as regions and zones within those regions. Additionally, by being able to schedule pods in different zones, you can improve network latency in certain scenarios.
					</p><p>
						You can configure pod topology spread constraints for all the pods deployed by the Cluster Monitoring Operator to control how pod replicas are scheduled to nodes across zones. This ensures that the pods are highly available and run more efficiently, because workloads are spread across nodes in different data centers or hierarchical infrastructure zones.
					</p></section><section class="section" id="about-specifying-limits-and-requests-for-monitoring-components_key-concepts"><div class="titlepage"><div><div><h5 class="title">1.3.1.3. About specifying limits and requests for monitoring components</h5></div></div></div><p>
						You can configure resource limits and requests for the following core platform monitoring components:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Alertmanager
							</li><li class="listitem">
								kube-state-metrics
							</li><li class="listitem">
								monitoring-plugin
							</li><li class="listitem">
								node-exporter
							</li><li class="listitem">
								openshift-state-metrics
							</li><li class="listitem">
								Prometheus
							</li><li class="listitem">
								Metrics Server
							</li><li class="listitem">
								Prometheus Operator and its admission webhook service
							</li><li class="listitem">
								Telemeter Client
							</li><li class="listitem">
								Thanos Querier
							</li></ul></div><p>
						You can configure resource limits and requests for the following components that monitor user-defined projects:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Alertmanager
							</li><li class="listitem">
								Prometheus
							</li><li class="listitem">
								Thanos Ruler
							</li></ul></div><p>
						By defining the resource limits, you limit a container’s resource usage, which prevents the container from exceeding the specified maximum values for CPU and memory resources.
					</p><p>
						By defining the resource requests, you specify that a container can be scheduled only on a node that has enough CPU and memory resources available to match the requested resources.
					</p></section><section class="section" id="configuring-metrics-collection-profiles_key-concepts"><div class="titlepage"><div><div><h5 class="title">1.3.1.4. About metrics collection profiles</h5></div></div></div><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
							Metrics collection profile is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
						</p><p>
							For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
						</p></div></rh-alert><p>
						By default, Prometheus collects metrics exposed by all default metrics targets in OpenShift Container Platform components. However, you might want Prometheus to collect fewer metrics from a cluster in certain scenarios:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								If cluster administrators require only alert, telemetry, and console metrics and do not require other metrics to be available.
							</li><li class="listitem">
								If a cluster increases in size, and the increased size of the default metrics data collected now requires a significant increase in CPU and memory resources.
							</li></ul></div><p>
						You can use a metrics collection profile to collect either the default amount of metrics data or a minimal amount of metrics data. When you collect minimal metrics data, basic monitoring features such as alerting continue to work. At the same time, the CPU and memory resources required by Prometheus decrease.
					</p><p>
						You can enable one of two metrics collection profiles:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<span class="strong strong"><strong>full</strong></span>: Prometheus collects metrics data exposed by all platform components. This setting is the default.
							</li><li class="listitem">
<span class="strong strong"><strong>minimal</strong></span>: Prometheus collects only the metrics data required for platform alerts, recording rules, telemetry, and console dashboards.
							</li></ul></div></section></section><section class="section" id="about-storing-and-recording-data_key-concepts"><div class="titlepage"><div><div><h4 class="title">1.3.2. About storing and recording data</h4></div></div></div><p>
					You can store and record data to help you protect the data and use them for troubleshooting. You can configure the default monitoring stack by performing any of the following actions:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Configure persistent storage:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									Protect your metrics and alerting data from data loss by storing them in a persistent volume (PV). As a result, they can survive pods being restarted or recreated.
								</li><li class="listitem">
									Avoid getting duplicate notifications and losing silences for alerts when the Alertmanager pods are restarted.
								</li></ul></div></li><li class="listitem">
							Modify the retention time and size for Prometheus and Thanos Ruler metrics data.
						</li><li class="listitem"><p class="simpara">
							Configure logging to help you troubleshoot issues with your cluster:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									Configure audit logs for Metrics Server.
								</li><li class="listitem">
									Set log levels for monitoring.
								</li><li class="listitem">
									Enable the query logging for Prometheus and Thanos Querier.
								</li></ul></div></li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#storing-and-recording-data" title="3.3. Storing and recording data for core platform monitoring">Storing and recording data for core platform monitoring</a>
</li><li class="listitem">
<a class="link" href="#storing-and-recording-data-uwm" title="4.3. Storing and recording data for user workload monitoring">Storing and recording data for user workload monitoring</a>
</li></ul></div><section class="section" id="retention-time-and-size-for-prometheus-metrics-data_key-concepts"><div class="titlepage"><div><div><h5 class="title">1.3.2.1. Retention time and size for Prometheus metrics</h5></div></div></div><p>
						By default, Prometheus retains metrics data for the following durations:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<span class="strong strong"><strong>Core platform monitoring</strong></span>: 15 days
							</li><li class="listitem">
<span class="strong strong"><strong>Monitoring for user-defined projects</strong></span>: 24 hours
							</li></ul></div><p>
						You can modify the retention time for the Prometheus instance to change how soon the data is deleted. You can also set the maximum amount of disk space the retained metrics data uses. If the data reaches this size limit, Prometheus deletes the oldest data first until the disk space used is again below the limit.
					</p><p>
						Note the following behaviors of these data retention settings:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								The size-based retention policy applies to all data block directories in the <code class="literal">/prometheus</code> directory, including persistent blocks, write-ahead log (WAL) data, and m-mapped chunks.
							</li><li class="listitem">
								Data in the <code class="literal">/wal</code> and <code class="literal">/head_chunks</code> directories counts toward the retention size limit, but Prometheus never purges data from these directories based on size- or time-based retention policies. Thus, if you set a retention size limit lower than the maximum size set for the <code class="literal">/wal</code> and <code class="literal">/head_chunks</code> directories, you have configured the system not to retain any data blocks in the <code class="literal">/prometheus</code> data directories.
							</li><li class="listitem">
								The size-based retention policy is applied only when Prometheus cuts a new data block, which occurs every two hours after the WAL contains at least three hours of data.
							</li><li class="listitem">
								If you do not explicitly define values for either <code class="literal">retention</code> or <code class="literal">retentionSize</code>, retention time defaults to 15 days for core platform monitoring and 24 hours for user-defined project monitoring. Retention size is not set.
							</li><li class="listitem">
								If you define values for both <code class="literal">retention</code> and <code class="literal">retentionSize</code>, both values apply. If any data blocks exceed the defined retention time or the defined size limit, Prometheus purges these data blocks.
							</li><li class="listitem">
								If you define a value for <code class="literal">retentionSize</code> and do not define <code class="literal">retention</code>, only the <code class="literal">retentionSize</code> value applies.
							</li><li class="listitem">
								If you do not define a value for <code class="literal">retentionSize</code> and only define a value for <code class="literal">retention</code>, only the <code class="literal">retention</code> value applies.
							</li><li class="listitem">
								If you set the <code class="literal">retentionSize</code> or <code class="literal">retention</code> value to <code class="literal">0</code>, the default settings apply. The default settings set retention time to 15 days for core platform monitoring and 24 hours for user-defined project monitoring. By default, retention size is not set.
							</li></ul></div><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							Data compaction occurs every two hours. Therefore, a persistent volume (PV) might fill up before compaction, potentially exceeding the <code class="literal">retentionSize</code> limit. In such cases, the <code class="literal">KubePersistentVolumeFillingUp</code> alert fires until the space on a PV is lower than the <code class="literal">retentionSize</code> limit.
						</p></div></rh-alert></section></section><section class="section" id="understanding-metrics_key-concepts"><div class="titlepage"><div><div><h4 class="title">1.3.3. Understanding metrics</h4></div></div></div><p class="_abstract _abstract">
					In OpenShift Container Platform 4.18, cluster components are monitored by scraping metrics exposed through service endpoints. You can also configure metrics collection for user-defined projects. Metrics enable you to monitor how cluster components and your own workloads are performing.
				</p><p>
					You can define the metrics that you want to provide for your own workloads by using Prometheus client libraries at the application level.
				</p><p>
					In OpenShift Container Platform, metrics are exposed through an HTTP service endpoint under the <code class="literal">/metrics</code> canonical name. You can list all available metrics for a service by running a <code class="literal">curl</code> query against <code class="literal">http://&lt;endpoint&gt;/metrics</code>. For instance, you can expose a route to the <code class="literal">prometheus-example-app</code> example application and then run the following to view all of its available metrics:
				</p><pre class="programlisting language-terminal">$ curl http://&lt;example_app_endpoint&gt;/metrics</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
</p><pre class="programlisting language-terminal"># HELP http_requests_total Count of all HTTP requests
# TYPE http_requests_total counter
http_requests_total{code="200",method="get"} 4
http_requests_total{code="404",method="get"} 2
# HELP version Version information about this binary
# TYPE version gauge
version{version="v0.1.0"} 1</pre>
<p></p></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#configuring-metrics" title="3.4. Configuring metrics for core platform monitoring">Configuring metrics for core platform monitoring</a>
</li><li class="listitem">
<a class="link" href="#configuring-metrics-uwm" title="4.4. Configuring metrics for user workload monitoring">Configuring metrics for user workload monitoring</a>
</li><li class="listitem">
<a class="link" href="#accessing-metrics-as-an-administrator" title="5.1. Accessing metrics as an administrator">Accessing metrics as an administrator</a>
</li><li class="listitem">
<a class="link" href="#accessing-metrics-as-a-developer" title="5.2. Accessing metrics as a developer">Accessing metrics as a developer</a>
</li></ul></div><section class="section" id="controlling-the-impact-of-unbound-attributes-in-user-defined-projects_key-concepts"><div class="titlepage"><div><div><h5 class="title">1.3.3.1. Controlling the impact of unbound metrics attributes in user-defined projects</h5></div></div></div><p>
						Developers can create labels to define attributes for metrics in the form of key-value pairs. The number of potential key-value pairs corresponds to the number of possible values for an attribute. An attribute that has an unlimited number of potential values is called an unbound attribute. For example, a <code class="literal">customer_id</code> attribute is unbound because it has an infinite number of possible values.
					</p><p>
						Every assigned key-value pair has a unique time series. The use of many unbound attributes in labels can result in an exponential increase in the number of time series created. This can impact Prometheus performance and can consume a lot of disk space.
					</p><p>
						Cluster administrators can use the following measures to control the impact of unbound metrics attributes in user-defined projects:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Limit the number of samples that can be accepted per target scrape in user-defined projects
							</li><li class="listitem">
								Limit the number of scraped labels, the length of label names, and the length of label values
							</li><li class="listitem">
								Configure the intervals between consecutive scrapes and between Prometheus rule evaluations
							</li><li class="listitem">
								Create alerts that fire when a scrape sample threshold is reached or when the target cannot be scraped
							</li></ul></div><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							Limiting scrape samples can help prevent the issues caused by adding many unbound attributes to labels. Developers can also prevent the underlying cause by limiting the number of unbound attributes that they define for metrics. Using attributes that are bound to a limited set of possible values reduces the number of potential key-value pair combinations.
						</p></div></rh-alert></section><section class="section" id="adding-cluster-id-labels-to-metrics_key-concepts"><div class="titlepage"><div><div><h5 class="title">1.3.3.2. Adding cluster ID labels to metrics</h5></div></div></div><p>
						If you manage multiple OpenShift Container Platform clusters and use the remote write feature to send metrics data from these clusters to an external storage location, you can add cluster ID labels to identify the metrics data coming from different clusters. You can then query these labels to identify the source cluster for a metric and distinguish that data from similar metrics data sent by other clusters.
					</p><p>
						This way, if you manage many clusters for multiple customers and send metrics data to a single centralized storage system, you can use cluster ID labels to query metrics for a particular cluster or customer.
					</p><p>
						Creating and using cluster ID labels involves three general steps:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Configuring the write relabel settings for remote write storage.
							</li><li class="listitem">
								Adding cluster ID labels to the metrics.
							</li><li class="listitem">
								Querying these labels to identify the source cluster or customer for a metric.
							</li></ul></div></section></section><section class="section" id="about-monitoring-dashboards_key-concepts"><div class="titlepage"><div><div><h4 class="title">1.3.4. About monitoring dashboards</h4></div></div></div><p>
					OpenShift Container Platform provides a set of monitoring dashboards that help you understand the state of cluster components and user-defined workloads.
				</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#reviewing-monitoring-dashboards-admin_accessing-metrics-as-an-administrator" title="5.1.4. Reviewing monitoring dashboards as a cluster administrator">Reviewing monitoring dashboards as a cluster administrator</a>
</li><li class="listitem">
<a class="link" href="#reviewing-monitoring-dashboards-developer_accessing-metrics-as-a-developer" title="5.2.3. Reviewing monitoring dashboards as a developer">Reviewing monitoring dashboards as a developer</a>
</li></ul></div><section class="section" id="mon-dashboards-adm-perspective_key-concepts"><div class="titlepage"><div><div><h5 class="title">1.3.4.1. Monitoring dashboards in the Administrator perspective</h5></div></div></div><p>
						Use the <span class="strong strong"><strong>Administrator</strong></span> perspective to access dashboards for the core OpenShift Container Platform components, including the following items:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								API performance
							</li><li class="listitem">
								etcd
							</li><li class="listitem">
								Kubernetes compute resources
							</li><li class="listitem">
								Kubernetes network resources
							</li><li class="listitem">
								Prometheus
							</li><li class="listitem">
								USE method dashboards relating to cluster and node performance
							</li><li class="listitem">
								Node performance metrics
							</li></ul></div><div class="figure" id="idm140059297765712"><p class="title"><strong>Figure 1.1. Example dashboard in the Administrator perspective</strong></p><div class="figure-contents"><div class="mediaobject"><img alt="monitoring dashboard administrator" src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.18-Monitoring-en-US/images/5adbef58ccc58b56aab269cc0aee9af7/monitoring-dashboard-administrator.png"/></div></div></div></section><section class="section" id="mon-dashboards-dev-perspective_key-concepts"><div class="titlepage"><div><div><h5 class="title">1.3.4.2. Monitoring dashboards in the Developer perspective</h5></div></div></div><p>
						In the <span class="strong strong"><strong>Developer</strong></span> perspective, you can access only the Kubernetes compute resources dashboards:
					</p><div class="figure" id="idm140059303058368"><p class="title"><strong>Figure 1.2. Example dashboard in the Developer perspective</strong></p><div class="figure-contents"><div class="mediaobject"><img alt="observe dashboard developer" src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.18-Monitoring-en-US/images/5573176931cb849685ad9e3869ddc165/observe-dashboard-developer.png"/></div></div></div></section></section><section class="section" id="about-managing-alerts_key-concepts"><div class="titlepage"><div><div><h4 class="title">1.3.5. Managing alerts</h4></div></div></div><p>
					In the OpenShift Container Platform, the Alerting UI enables you to manage alerts, silences, and alerting rules.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<span class="strong strong"><strong>Alerting rules</strong></span>. Alerting rules contain a set of conditions that outline a particular state within a cluster. Alerts are triggered when those conditions are true. An alerting rule can be assigned a severity that defines how the alerts are routed.
						</li><li class="listitem">
<span class="strong strong"><strong>Alerts</strong></span>. An alert is fired when the conditions defined in an alerting rule are true. Alerts provide a notification that a set of circumstances are apparent within an OpenShift Container Platform cluster.
						</li><li class="listitem">
<span class="strong strong"><strong>Silences</strong></span>. A silence can be applied to an alert to prevent notifications from being sent when the conditions for an alert are true. You can mute an alert after the initial notification, while you work on resolving the issue.
						</li></ul></div><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						The alerts, silences, and alerting rules that are available in the Alerting UI relate to the projects that you have access to. For example, if you are logged in as a user with the <code class="literal">cluster-admin</code> role, you can access all alerts, silences, and alerting rules.
					</p></div></rh-alert><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#configuring-alerts-and-notifications" title="3.5. Configuring alerts and notifications for core platform monitoring">Configuring alerts and notifications for core platform monitoring</a>
</li><li class="listitem">
<a class="link" href="#configuring-alerts-and-notifications-uwm" title="4.5. Configuring alerts and notifications for user workload monitoring">Configuring alerts and notifications for user workload monitoring</a>
</li><li class="listitem">
<a class="link" href="#managing-alerts-as-an-administrator" title="6.1. Managing alerts as an Administrator">Managing alerts as an Administrator</a>
</li><li class="listitem">
<a class="link" href="#managing-alerts-as-a-developer" title="6.2. Managing alerts as a Developer">Managing alerts as a Developer</a>
</li></ul></div><section class="section" id="managing-silences_key-concepts"><div class="titlepage"><div><div><h5 class="title">1.3.5.1. Managing silences</h5></div></div></div><p>
						You can create a silence for an alert in the OpenShift Container Platform web console in both the <span class="strong strong"><strong>Administrator</strong></span> and <span class="strong strong"><strong>Developer</strong></span> perspectives. After you create a silence, you will not receive notifications about an alert when the alert fires.
					</p><p>
						Creating silences is useful in scenarios where you have received an initial alert notification, and you do not want to receive further notifications during the time in which you resolve the underlying issue causing the alert to fire.
					</p><p>
						When creating a silence, you must specify whether it becomes active immediately or at a later time. You must also set a duration period after which the silence expires.
					</p><p>
						After you create silences, you can view, edit, and expire them.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							When you create silences, they are replicated across Alertmanager pods. However, if you do not configure persistent storage for Alertmanager, silences might be lost. This can happen, for example, if all Alertmanager pods restart at the same time.
						</p></div></rh-alert></section><section class="section" id="managing-core-platform-alerting-rules_key-concepts"><div class="titlepage"><div><div><h5 class="title">1.3.5.2. Managing alerting rules for core platform monitoring</h5></div></div></div><p>
						The OpenShift Container Platform monitoring includes a large set of default alerting rules for platform metrics. As a cluster administrator, you can customize this set of rules in two ways:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Modify the settings for existing platform alerting rules by adjusting thresholds or by adding and modifying labels. For example, you can change the <code class="literal">severity</code> label for an alert from <code class="literal">warning</code> to <code class="literal">critical</code> to help you route and triage issues flagged by an alert.
							</li><li class="listitem">
								Define and add new custom alerting rules by constructing a query expression based on core platform metrics in the <code class="literal">openshift-monitoring</code> namespace.
							</li></ul></div><div class="itemizedlist"><p class="title"><strong>Core platform alerting rule considerations</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								New alerting rules must be based on the default OpenShift Container Platform monitoring metrics.
							</li><li class="listitem">
								You must create the <code class="literal">AlertingRule</code> and <code class="literal">AlertRelabelConfig</code> objects in the <code class="literal">openshift-monitoring</code> namespace.
							</li><li class="listitem">
								You can only add and modify alerting rules. You cannot create new recording rules or modify existing recording rules.
							</li><li class="listitem">
								If you modify existing platform alerting rules by using an <code class="literal">AlertRelabelConfig</code> object, your modifications are not reflected in the Prometheus alerts API. Therefore, any dropped alerts still appear in the OpenShift Container Platform web console even though they are no longer forwarded to Alertmanager. Additionally, any modifications to alerts, such as a changed <code class="literal">severity</code> label, do not appear in the web console.
							</li></ul></div></section><section class="section" id="tips-for-optimizing-alerting-rules-for-core-platform-monitoring_key-concepts"><div class="titlepage"><div><div><h5 class="title">1.3.5.3. Tips for optimizing alerting rules for core platform monitoring</h5></div></div></div><p>
						If you customize core platform alerting rules to meet your organization’s specific needs, follow these guidelines to help ensure that the customized rules are efficient and effective.
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<span class="strong strong"><strong>Minimize the number of new rules</strong></span>. Create only rules that are essential to your specific requirements. By minimizing the number of rules, you create a more manageable and focused alerting system in your monitoring environment.
							</li><li class="listitem">
<span class="strong strong"><strong>Focus on symptoms rather than causes</strong></span>. Create rules that notify users of symptoms instead of underlying causes. This approach ensures that users are promptly notified of a relevant symptom so that they can investigate the root cause after an alert has triggered. This tactic also significantly reduces the overall number of rules you need to create.
							</li><li class="listitem">
<span class="strong strong"><strong>Plan and assess your needs before implementing changes</strong></span>. First, decide what symptoms are important and what actions you want users to take if these symptoms occur. Then, assess existing rules and decide if you can modify any of them to meet your needs instead of creating entirely new rules for each symptom. By modifying existing rules and creating new ones judiciously, you help to streamline your alerting system.
							</li><li class="listitem">
<span class="strong strong"><strong>Provide clear alert messaging</strong></span>. When you create alert messages, describe the symptom, possible causes, and recommended actions. Include unambiguous, concise explanations along with troubleshooting steps or links to more information. Doing so helps users quickly assess the situation and respond appropriately.
							</li><li class="listitem">
<span class="strong strong"><strong>Include severity levels</strong></span>. Assign severity levels to your rules to indicate how a user needs to react when a symptom occurs and triggers an alert. For example, classifying an alert as <span class="strong strong"><strong>Critical</strong></span> signals that an individual or a critical response team needs to respond immediately. By defining severity levels, you help users know how to respond to an alert and help ensure that the most urgent issues receive prompt attention.
							</li></ul></div></section><section class="section" id="about-creating-alerting-rules-for-user-defined-projects_key-concepts"><div class="titlepage"><div><div><h5 class="title">1.3.5.4. Creating alerting rules for user-defined projects</h5></div></div></div><p>
						In OpenShift Container Platform, you can create alerting rules for user-defined projects. Those alerting rules will trigger alerts based on the values of the chosen metrics.
					</p><p>
						If you create alerting rules for a user-defined project, consider the following key behaviors and important limitations when you define the new rules:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								A user-defined alerting rule can include metrics exposed by its own project in addition to the default metrics from core platform monitoring. You cannot include metrics from another user-defined project.
							</p><p class="simpara">
								For example, an alerting rule for the <code class="literal">ns1</code> user-defined project can use metrics exposed by the <code class="literal">ns1</code> project in addition to core platform metrics, such as CPU and memory metrics. However, the rule cannot include metrics from a different <code class="literal">ns2</code> user-defined project.
							</p></li><li class="listitem">
								By default, when you create an alerting rule, the <code class="literal">namespace</code> label is enforced on it even if a rule with the same name exists in another project. To create alerting rules that are not bound to their project of origin, see "Creating cross-project alerting rules for user-defined projects".
							</li><li class="listitem"><p class="simpara">
								To reduce latency and to minimize the load on core platform monitoring components, you can add the <code class="literal">openshift.io/prometheus-rule-evaluation-scope: leaf-prometheus</code> label to a rule. This label forces only the Prometheus instance deployed in the <code class="literal">openshift-user-workload-monitoring</code> project to evaluate the alerting rule and prevents the Thanos Ruler instance from doing so.
							</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
									If an alerting rule has this label, your alerting rule can use only those metrics exposed by your user-defined project. Alerting rules you create based on default platform metrics might not trigger alerts.
								</p></div></rh-alert></li></ul></div></section><section class="section" id="managing-alerting-rules-for-user-defined-projects_key-concepts"><div class="titlepage"><div><div><h5 class="title">1.3.5.5. Managing alerting rules for user-defined projects</h5></div></div></div><p>
						In OpenShift Container Platform, you can view, edit, and remove alerting rules in user-defined projects.
					</p><div class="itemizedlist"><p class="title"><strong>Alerting rule considerations</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								The default alerting rules are used specifically for the OpenShift Container Platform cluster.
							</li><li class="listitem">
								Some alerting rules intentionally have identical names. They send alerts about the same event with different thresholds, different severity, or both.
							</li><li class="listitem">
								Inhibition rules prevent notifications for lower severity alerts that are firing when a higher severity alert is also firing.
							</li></ul></div></section><section class="section" id="optimizing-alerting-for-user-defined-projects_key-concepts"><div class="titlepage"><div><div><h5 class="title">1.3.5.6. Optimizing alerting for user-defined projects</h5></div></div></div><p>
						You can optimize alerting for your own projects by considering the following recommendations when creating alerting rules:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<span class="strong strong"><strong>Minimize the number of alerting rules that you create for your project</strong></span>. Create alerting rules that notify you of conditions that impact you. It is more difficult to notice relevant alerts if you generate many alerts for conditions that do not impact you.
							</li><li class="listitem">
<span class="strong strong"><strong>Create alerting rules for symptoms instead of causes</strong></span>. Create alerting rules that notify you of conditions regardless of the underlying cause. The cause can then be investigated. You will need many more alerting rules if each relates only to a specific cause. Some causes are then likely to be missed.
							</li><li class="listitem">
<span class="strong strong"><strong>Plan before you write your alerting rules</strong></span>. Determine what symptoms are important to you and what actions you want to take if they occur. Then build an alerting rule for each symptom.
							</li><li class="listitem">
<span class="strong strong"><strong>Provide clear alert messaging</strong></span>. State the symptom and recommended actions in the alert message.
							</li><li class="listitem">
<span class="strong strong"><strong>Include severity levels in your alerting rules</strong></span>. The severity of an alert depends on how you need to react if the reported symptom occurs. For example, a critical alert should be triggered if a symptom requires immediate attention by an individual or a critical response team.
							</li></ul></div></section><section class="section" id="searching-alerts-silences-and-alerting-rules_key-concepts"><div class="titlepage"><div><div><h5 class="title">1.3.5.7. Searching and filtering alerts, silences, and alerting rules</h5></div></div></div><p>
						You can filter the alerts, silences, and alerting rules that are displayed in the Alerting UI. This section provides a description of each of the available filtering options.
					</p><section class="section" id="understanding-alert-filters_key-concepts"><div class="titlepage"><div><div><h5 class="title">1.3.5.7.1. Understanding alert filters</h5></div></div></div><p>
							In the <span class="strong strong"><strong>Administrator</strong></span> perspective, the <span class="strong strong"><strong>Alerts</strong></span> page in the Alerting UI provides details about alerts relating to default OpenShift Container Platform and user-defined projects. The page includes a summary of severity, state, and source for each alert. The time at which an alert went into its current state is also shown.
						</p><p>
							You can filter by alert state, severity, and source. By default, only <span class="strong strong"><strong>Platform</strong></span> alerts that are <span class="strong strong"><strong>Firing</strong></span> are displayed. The following describes each alert filtering option:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
<span class="strong strong"><strong>State</strong></span> filters:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
<span class="strong strong"><strong>Firing</strong></span>. The alert is firing because the alert condition is true and the optional <code class="literal">for</code> duration has passed. The alert continues to fire while the condition remains true.
										</li><li class="listitem">
<span class="strong strong"><strong>Pending</strong></span>. The alert is active but is waiting for the duration that is specified in the alerting rule before it fires.
										</li><li class="listitem">
<span class="strong strong"><strong>Silenced</strong></span>. The alert is now silenced for a defined time period. Silences temporarily mute alerts based on a set of label selectors that you define. Notifications are not sent for alerts that match all the listed values or regular expressions.
										</li></ul></div></li><li class="listitem"><p class="simpara">
<span class="strong strong"><strong>Severity</strong></span> filters:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
<span class="strong strong"><strong>Critical</strong></span>. The condition that triggered the alert could have a critical impact. The alert requires immediate attention when fired and is typically paged to an individual or to a critical response team.
										</li><li class="listitem">
<span class="strong strong"><strong>Warning</strong></span>. The alert provides a warning notification about something that might require attention to prevent a problem from occurring. Warnings are typically routed to a ticketing system for non-immediate review.
										</li><li class="listitem">
<span class="strong strong"><strong>Info</strong></span>. The alert is provided for informational purposes only.
										</li><li class="listitem">
<span class="strong strong"><strong>None</strong></span>. The alert has no defined severity.
										</li><li class="listitem">
											You can also create custom severity definitions for alerts relating to user-defined projects.
										</li></ul></div></li><li class="listitem"><p class="simpara">
<span class="strong strong"><strong>Source</strong></span> filters:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
<span class="strong strong"><strong>Platform</strong></span>. Platform-level alerts relate only to default OpenShift Container Platform projects. These projects provide core OpenShift Container Platform functionality.
										</li><li class="listitem">
<span class="strong strong"><strong>User</strong></span>. User alerts relate to user-defined projects. These alerts are user-created and are customizable. User-defined workload monitoring can be enabled postinstallation to provide observability into your own workloads.
										</li></ul></div></li></ul></div></section><section class="section" id="understanding-silence-filters_key-concepts"><div class="titlepage"><div><div><h5 class="title">1.3.5.7.2. Understanding silence filters</h5></div></div></div><p>
							In the <span class="strong strong"><strong>Administrator</strong></span> perspective, the <span class="strong strong"><strong>Silences</strong></span> page in the Alerting UI provides details about silences applied to alerts in default OpenShift Container Platform and user-defined projects. The page includes a summary of the state of each silence and the time at which a silence ends.
						</p><p>
							You can filter by silence state. By default, only <span class="strong strong"><strong>Active</strong></span> and <span class="strong strong"><strong>Pending</strong></span> silences are displayed. The following describes each silence state filter option:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
<span class="strong strong"><strong>State</strong></span> filters:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
<span class="strong strong"><strong>Active</strong></span>. The silence is active and the alert will be muted until the silence is expired.
										</li><li class="listitem">
<span class="strong strong"><strong>Pending</strong></span>. The silence has been scheduled and it is not yet active.
										</li><li class="listitem">
<span class="strong strong"><strong>Expired</strong></span>. The silence has expired and notifications will be sent if the conditions for an alert are true.
										</li></ul></div></li></ul></div></section><section class="section" id="understanding-alerting-rule-filters_key-concepts"><div class="titlepage"><div><div><h5 class="title">1.3.5.7.3. Understanding alerting rule filters</h5></div></div></div><p>
							In the <span class="strong strong"><strong>Administrator</strong></span> perspective, the <span class="strong strong"><strong>Alerting rules</strong></span> page in the Alerting UI provides details about alerting rules relating to default OpenShift Container Platform and user-defined projects. The page includes a summary of the state, severity, and source for each alerting rule.
						</p><p>
							You can filter alerting rules by alert state, severity, and source. By default, only <span class="strong strong"><strong>Platform</strong></span> alerting rules are displayed. The following describes each alerting rule filtering option:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
<span class="strong strong"><strong>Alert state</strong></span> filters:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
<span class="strong strong"><strong>Firing</strong></span>. The alert is firing because the alert condition is true and the optional <code class="literal">for</code> duration has passed. The alert continues to fire while the condition remains true.
										</li><li class="listitem">
<span class="strong strong"><strong>Pending</strong></span>. The alert is active but is waiting for the duration that is specified in the alerting rule before it fires.
										</li><li class="listitem">
<span class="strong strong"><strong>Silenced</strong></span>. The alert is now silenced for a defined time period. Silences temporarily mute alerts based on a set of label selectors that you define. Notifications are not sent for alerts that match all the listed values or regular expressions.
										</li><li class="listitem">
<span class="strong strong"><strong>Not Firing</strong></span>. The alert is not firing.
										</li></ul></div></li><li class="listitem"><p class="simpara">
<span class="strong strong"><strong>Severity</strong></span> filters:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
<span class="strong strong"><strong>Critical</strong></span>. The conditions defined in the alerting rule could have a critical impact. When true, these conditions require immediate attention. Alerts relating to the rule are typically paged to an individual or to a critical response team.
										</li><li class="listitem">
<span class="strong strong"><strong>Warning</strong></span>. The conditions defined in the alerting rule might require attention to prevent a problem from occurring. Alerts relating to the rule are typically routed to a ticketing system for non-immediate review.
										</li><li class="listitem">
<span class="strong strong"><strong>Info</strong></span>. The alerting rule provides informational alerts only.
										</li><li class="listitem">
<span class="strong strong"><strong>None</strong></span>. The alerting rule has no defined severity.
										</li><li class="listitem">
											You can also create custom severity definitions for alerting rules relating to user-defined projects.
										</li></ul></div></li><li class="listitem"><p class="simpara">
<span class="strong strong"><strong>Source</strong></span> filters:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
<span class="strong strong"><strong>Platform</strong></span>. Platform-level alerting rules relate only to default OpenShift Container Platform projects. These projects provide core OpenShift Container Platform functionality.
										</li><li class="listitem">
<span class="strong strong"><strong>User</strong></span>. User-defined workload alerting rules relate to user-defined projects. These alerting rules are user-created and are customizable. User-defined workload monitoring can be enabled postinstallation to provide observability into your own workloads.
										</li></ul></div></li></ul></div></section><section class="section" id="searching-filtering-alerts-dev-perspective_key-concepts"><div class="titlepage"><div><div><h5 class="title">1.3.5.7.4. Searching and filtering alerts, silences, and alerting rules in the Developer perspective</h5></div></div></div><p>
							In the <span class="strong strong"><strong>Developer</strong></span> perspective, the <span class="strong strong"><strong>Alerts</strong></span> page in the Alerting UI provides a combined view of alerts and silences relating to the selected project. A link to the governing alerting rule is provided for each displayed alert.
						</p><p>
							In this view, you can filter by alert state and severity. By default, all alerts in the selected project are displayed if you have permission to access the project. These filters are the same as those described for the <span class="strong strong"><strong>Administrator</strong></span> perspective.
						</p></section></section></section><section class="section" id="understanding-alert-routing-for-user-defined-projects_key-concepts"><div class="titlepage"><div><div><h4 class="title">1.3.6. Understanding alert routing for user-defined projects</h4></div></div></div><p class="_abstract _abstract">
					As a cluster administrator, you can enable alert routing for user-defined projects. With this feature, you can allow users with the <code class="literal">alert-routing-edit</code> cluster role to configure alert notification routing and receivers for user-defined projects. These notifications are routed by the default Alertmanager instance or, if enabled, an optional Alertmanager instance dedicated to user-defined monitoring.
				</p><p>
					Users can then create and configure user-defined alert routing by creating or editing the <code class="literal">AlertmanagerConfig</code> objects for their user-defined projects without the help of an administrator.
				</p><p>
					After a user has defined alert routing for a user-defined project, user-defined alert notifications are routed as follows:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							To the <code class="literal">alertmanager-main</code> pods in the <code class="literal">openshift-monitoring</code> namespace if using the default platform Alertmanager instance.
						</li><li class="listitem">
							To the <code class="literal">alertmanager-user-workload</code> pods in the <code class="literal">openshift-user-workload-monitoring</code> namespace if you have enabled a separate instance of Alertmanager for user-defined projects.
						</li></ul></div><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						Review the following limitations of alert routing for user-defined projects:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								For user-defined alerting rules, user-defined routing is scoped to the namespace in which the resource is defined. For example, a routing configuration in namespace <code class="literal">ns1</code> only applies to <code class="literal">PrometheusRules</code> resources in the same namespace.
							</li><li class="listitem">
								When a namespace is excluded from user-defined monitoring, <code class="literal">AlertmanagerConfig</code> resources in the namespace cease to be part of the Alertmanager configuration.
							</li></ul></div></div></rh-alert><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#enabling-alert-routing-for-user-defined-projects_preparing-to-configure-the-monitoring-stack-uwm" title="4.1.3. Enabling alert routing for user-defined projects">Enabling alert routing for user-defined projects</a>
</li></ul></div></section><section class="section" id="sending-notifications-to-external-systems_key-concepts"><div class="titlepage"><div><div><h4 class="title">1.3.7. Sending notifications to external systems</h4></div></div></div><p>
					In OpenShift Container Platform 4.18, firing alerts can be viewed in the Alerting UI. Alerts are not configured by default to be sent to any notification systems. You can configure OpenShift Container Platform to send alerts to the following receiver types:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							PagerDuty
						</li><li class="listitem">
							Webhook
						</li><li class="listitem">
							Email
						</li><li class="listitem">
							Slack
						</li><li class="listitem">
							Microsoft Teams
						</li></ul></div><p>
					Routing alerts to receivers enables you to send timely notifications to the appropriate teams when failures occur. For example, critical alerts require immediate attention and are typically paged to an individual or a critical response team. Alerts that provide non-critical warning notifications might instead be routed to a ticketing system for non-immediate review.
				</p><div class="formalpara"><p class="title"><strong>Checking that alerting is operational by using the watchdog alert</strong></p><p>
						OpenShift Container Platform monitoring includes a watchdog alert that fires continuously. Alertmanager repeatedly sends watchdog alert notifications to configured notification providers. The provider is usually configured to notify an administrator when it stops receiving the watchdog alert. This mechanism helps you quickly identify any communication issues between Alertmanager and the notification provider.
					</p></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#configuring-alert-notifications_configuring-alerts-and-notifications" title="3.5.4. Configuring alert notifications">Configuring alert notifications for core platform monitoring</a>
</li><li class="listitem">
<a class="link" href="#configuring-alert-notifications_configuring-alerts-and-notifications-uwm" title="4.5.4. Configuring alert notifications">Configuring alert notifications for user workload monitoring</a>
</li></ul></div></section></section></section><section class="chapter" id="getting-started"><div class="titlepage"><div><div><h2 class="title">Chapter 2. Getting started</h2></div></div></div><section class="section" id="maintenance-and-support-for-monitoring"><div class="titlepage"><div><div><h3 class="title">2.1. Maintenance and support for monitoring</h3></div></div></div><p>
				Not all configuration options for the monitoring stack are exposed. The only supported way of configuring OpenShift Container Platform monitoring is by configuring the Cluster Monitoring Operator (CMO) using the options described in the <a class="link" href="#cluster-monitoring-operator-configuration-reference" title="8.1. Cluster Monitoring Operator configuration reference">Config map reference for the Cluster Monitoring Operator</a>. <span class="strong strong"><strong>Do not use other configurations, as they are unsupported.</strong></span>
</p><p>
				Configuration paradigms might change across Prometheus releases, and such cases can only be handled gracefully if all configuration possibilities are controlled. If you use configurations other than those described in the <a class="link" href="#cluster-monitoring-operator-configuration-reference" title="8.1. Cluster Monitoring Operator configuration reference">Config map reference for the Cluster Monitoring Operator</a>, your changes will disappear because the CMO automatically reconciles any differences and resets any unsupported changes back to the originally defined state by default and by design.
			</p><section class="section" id="support-considerations_maintenance-and-support-for-monitoring"><div class="titlepage"><div><div><h4 class="title">2.1.1. Support considerations for monitoring</h4></div></div></div><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						Backward compatibility for metrics, recording rules, or alerting rules is not guaranteed.
					</p></div></rh-alert><p>
					The following modifications are explicitly not supported:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<span class="strong strong"><strong>Creating additional <code class="literal">ServiceMonitor</code>, <code class="literal">PodMonitor</code>, and <code class="literal">PrometheusRule</code> objects in the <code class="literal">openshift-*</code> and <code class="literal">kube-*</code> projects.</strong></span>
</li><li class="listitem"><p class="simpara">
<span class="strong strong"><strong>Modifying any resources or objects deployed in the <code class="literal">openshift-monitoring</code> or <code class="literal">openshift-user-workload-monitoring</code> projects.</strong></span> The resources created by the OpenShift Container Platform monitoring stack are not meant to be used by any other resources, as there are no guarantees about their backward compatibility.
						</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
								The Alertmanager configuration is deployed as the <code class="literal">alertmanager-main</code> secret resource in the <code class="literal">openshift-monitoring</code> namespace. If you have enabled a separate Alertmanager instance for user-defined alert routing, an Alertmanager configuration is also deployed as the <code class="literal">alertmanager-user-workload</code> secret resource in the <code class="literal">openshift-user-workload-monitoring</code> namespace. To configure additional routes for any instance of Alertmanager, you need to decode, modify, and then encode that secret. This procedure is a supported exception to the preceding statement.
							</p></div></rh-alert></li><li class="listitem">
<span class="strong strong"><strong>Modifying resources of the stack.</strong></span> The OpenShift Container Platform monitoring stack ensures its resources are always in the state it expects them to be. If they are modified, the stack will reset them.
						</li><li class="listitem">
<span class="strong strong"><strong>Deploying user-defined workloads to <code class="literal">openshift-*</code>, and <code class="literal">kube-*</code> projects.</strong></span> These projects are reserved for Red Hat provided components and they should not be used for user-defined workloads.
						</li><li class="listitem">
<span class="strong strong"><strong>Enabling symptom based monitoring by using the <code class="literal">Probe</code> custom resource definition (CRD) in Prometheus Operator.</strong></span>
</li><li class="listitem">
<span class="strong strong"><strong>Manually deploying monitoring resources into namespaces that have the <code class="literal">openshift.io/cluster-monitoring: "true"</code> label.</strong></span>
</li><li class="listitem">
<span class="strong strong"><strong>Adding the <code class="literal">openshift.io/cluster-monitoring: "true"</code> label to namespaces.</strong></span> This label is reserved only for the namespaces with core OpenShift Container Platform components and Red Hat certified components.
						</li><li class="listitem">
<span class="strong strong"><strong>Installing custom Prometheus instances on OpenShift Container Platform.</strong></span> A custom instance is a Prometheus custom resource (CR) managed by the Prometheus Operator.
						</li></ul></div></section><section class="section" id="support-policy-for-monitoring-operators_maintenance-and-support-for-monitoring"><div class="titlepage"><div><div><h4 class="title">2.1.2. Support policy for monitoring Operators</h4></div></div></div><p>
					Monitoring Operators ensure that OpenShift Container Platform monitoring resources function as designed and tested. If Cluster Version Operator (CVO) control of an Operator is overridden, the Operator does not respond to configuration changes, reconcile the intended state of cluster objects, or receive updates.
				</p><p>
					While overriding CVO control for an Operator can be helpful during debugging, this is unsupported and the cluster administrator assumes full control of the individual component configurations and upgrades.
				</p><div class="formalpara"><p class="title"><strong>Overriding the Cluster Version Operator</strong></p><p>
						The <code class="literal">spec.overrides</code> parameter can be added to the configuration for the CVO to allow administrators to provide a list of overrides to the behavior of the CVO for a component. Setting the <code class="literal">spec.overrides[].unmanaged</code> parameter to <code class="literal">true</code> for a component blocks cluster upgrades and alerts the administrator after a CVO override has been set:
					</p></div><pre class="programlisting language-terminal">Disabling ownership via cluster version overrides prevents upgrades. Please remove overrides before continuing.</pre><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
						Setting a CVO override puts the entire cluster in an unsupported state and prevents the monitoring stack from being reconciled to its intended state. This impacts the reliability features built into Operators and prevents updates from being received. Reported issues must be reproduced after removing any overrides for support to proceed.
					</p></div></rh-alert></section><section class="section" id="support-version-matrix-for-monitoring-components_maintenance-and-support-for-monitoring"><div class="titlepage"><div><div><h4 class="title">2.1.3. Support version matrix for monitoring components</h4></div></div></div><p>
					The following matrix contains information about versions of monitoring components for OpenShift Container Platform 4.12 and later releases:
				</p><rh-table id="idm140059306881408"><table class="gt-8-cols lt-7-rows"><caption>Table 2.1. OpenShift Container Platform and component versions</caption><colgroup><col class="col_1" style="width: 11%; "/><!--Empty--><col class="col_2" style="width: 11%; "/><!--Empty--><col class="col_3" style="width: 11%; "/><!--Empty--><col class="col_4" style="width: 11%; "/><!--Empty--><col class="col_5" style="width: 11%; "/><!--Empty--><col class="col_6" style="width: 11%; "/><!--Empty--><col class="col_7" style="width: 11%; "/><!--Empty--><col class="col_8" style="width: 11%; "/><!--Empty--><col class="col_9" style="width: 11%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059297777696" scope="col" valign="top">OpenShift Container Platform</th><th align="left" id="idm140059299885568" scope="col" valign="top">Prometheus Operator</th><th align="left" id="idm140059299884480" scope="col" valign="top">Prometheus</th><th align="left" id="idm140059299883392" scope="col" valign="top">Metrics Server</th><th align="left" id="idm140059299882304" scope="col" valign="top">Alertmanager</th><th align="left" id="idm140059299881216" scope="col" valign="top">kube-state-metrics agent</th><th align="left" id="idm140059299880160" scope="col" valign="top">monitoring-plugin</th><th align="left" id="idm140059297979536" scope="col" valign="top">node-exporter agent</th><th align="left" id="idm140059297978448" scope="col" valign="top">Thanos</th></tr></thead><tbody><tr><td align="left" headers="idm140059297777696" valign="top"> <p>
									4.18
								</p>
</td><td align="left" headers="idm140059299885568" valign="top"> <p>
									0.78.1
								</p>
</td><td align="left" headers="idm140059299884480" valign="top"> <p>
									2.55.1
								</p>
</td><td align="left" headers="idm140059299883392" valign="top"> <p>
									0.7.2
								</p>
</td><td align="left" headers="idm140059299882304" valign="top"> <p>
									0.27.0
								</p>
</td><td align="left" headers="idm140059299881216" valign="top"> <p>
									2.13.0
								</p>
</td><td align="left" headers="idm140059299880160" valign="top"> <p>
									1.0.0
								</p>
</td><td align="left" headers="idm140059297979536" valign="top"> <p>
									1.8.2
								</p>
</td><td align="left" headers="idm140059297978448" valign="top"> <p>
									0.36.1
								</p>
</td></tr><tr><td align="left" headers="idm140059297777696" valign="top"> <p>
									4.17
								</p>
</td><td align="left" headers="idm140059299885568" valign="top"> <p>
									0.75.2
								</p>
</td><td align="left" headers="idm140059299884480" valign="top"> <p>
									2.53.1
								</p>
</td><td align="left" headers="idm140059299883392" valign="top"> <p>
									0.7.1
								</p>
</td><td align="left" headers="idm140059299882304" valign="top"> <p>
									0.27.0
								</p>
</td><td align="left" headers="idm140059299881216" valign="top"> <p>
									2.13.0
								</p>
</td><td align="left" headers="idm140059299880160" valign="top"> <p>
									1.0.0
								</p>
</td><td align="left" headers="idm140059297979536" valign="top"> <p>
									1.8.2
								</p>
</td><td align="left" headers="idm140059297978448" valign="top"> <p>
									0.35.1
								</p>
</td></tr><tr><td align="left" headers="idm140059297777696" valign="top"> <p>
									4.16
								</p>
</td><td align="left" headers="idm140059299885568" valign="top"> <p>
									0.73.2
								</p>
</td><td align="left" headers="idm140059299884480" valign="top"> <p>
									2.52.0
								</p>
</td><td align="left" headers="idm140059299883392" valign="top"> <p>
									0.7.1
								</p>
</td><td align="left" headers="idm140059299882304" valign="top"> <p>
									0.26.0
								</p>
</td><td align="left" headers="idm140059299881216" valign="top"> <p>
									2.12.0
								</p>
</td><td align="left" headers="idm140059299880160" valign="top"> <p>
									1.0.0
								</p>
</td><td align="left" headers="idm140059297979536" valign="top"> <p>
									1.8.0
								</p>
</td><td align="left" headers="idm140059297978448" valign="top"> <p>
									0.35.0
								</p>
</td></tr><tr><td align="left" headers="idm140059297777696" valign="top"> <p>
									4.15
								</p>
</td><td align="left" headers="idm140059299885568" valign="top"> <p>
									0.70.0
								</p>
</td><td align="left" headers="idm140059299884480" valign="top"> <p>
									2.48.0
								</p>
</td><td align="left" headers="idm140059299883392" valign="top"> <p>
									0.6.4
								</p>
</td><td align="left" headers="idm140059299882304" valign="top"> <p>
									0.26.0
								</p>
</td><td align="left" headers="idm140059299881216" valign="top"> <p>
									2.10.1
								</p>
</td><td align="left" headers="idm140059299880160" valign="top"> <p>
									1.0.0
								</p>
</td><td align="left" headers="idm140059297979536" valign="top"> <p>
									1.7.0
								</p>
</td><td align="left" headers="idm140059297978448" valign="top"> <p>
									0.32.5
								</p>
</td></tr><tr><td align="left" headers="idm140059297777696" valign="top"> <p>
									4.14
								</p>
</td><td align="left" headers="idm140059299885568" valign="top"> <p>
									0.67.1
								</p>
</td><td align="left" headers="idm140059299884480" valign="top"> <p>
									2.46.0
								</p>
</td><td align="left" headers="idm140059299883392" valign="top"> <p>
									N/A
								</p>
</td><td align="left" headers="idm140059299882304" valign="top"> <p>
									0.25.0
								</p>
</td><td align="left" headers="idm140059299881216" valign="top"> <p>
									2.9.2
								</p>
</td><td align="left" headers="idm140059299880160" valign="top"> <p>
									1.0.0
								</p>
</td><td align="left" headers="idm140059297979536" valign="top"> <p>
									1.6.1
								</p>
</td><td align="left" headers="idm140059297978448" valign="top"> <p>
									0.30.2
								</p>
</td></tr><tr><td align="left" headers="idm140059297777696" valign="top"> <p>
									4.13
								</p>
</td><td align="left" headers="idm140059299885568" valign="top"> <p>
									0.63.0
								</p>
</td><td align="left" headers="idm140059299884480" valign="top"> <p>
									2.42.0
								</p>
</td><td align="left" headers="idm140059299883392" valign="top"> <p>
									N/A
								</p>
</td><td align="left" headers="idm140059299882304" valign="top"> <p>
									0.25.0
								</p>
</td><td align="left" headers="idm140059299881216" valign="top"> <p>
									2.8.1
								</p>
</td><td align="left" headers="idm140059299880160" valign="top"> <p>
									N/A
								</p>
</td><td align="left" headers="idm140059297979536" valign="top"> <p>
									1.5.0
								</p>
</td><td align="left" headers="idm140059297978448" valign="top"> <p>
									0.30.2
								</p>
</td></tr><tr><td align="left" headers="idm140059297777696" valign="top"> <p>
									4.12
								</p>
</td><td align="left" headers="idm140059299885568" valign="top"> <p>
									0.60.1
								</p>
</td><td align="left" headers="idm140059299884480" valign="top"> <p>
									2.39.1
								</p>
</td><td align="left" headers="idm140059299883392" valign="top"> <p>
									N/A
								</p>
</td><td align="left" headers="idm140059299882304" valign="top"> <p>
									0.24.0
								</p>
</td><td align="left" headers="idm140059299881216" valign="top"> <p>
									2.6.0
								</p>
</td><td align="left" headers="idm140059299880160" valign="top"> <p>
									N/A
								</p>
</td><td align="left" headers="idm140059297979536" valign="top"> <p>
									1.4.0
								</p>
</td><td align="left" headers="idm140059297978448" valign="top"> <p>
									0.28.1
								</p>
</td></tr></tbody></table></rh-table><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						The openshift-state-metrics agent and Telemeter Client are OpenShift-specific components. Therefore, their versions correspond with the versions of OpenShift Container Platform.
					</p></div></rh-alert></section></section><section class="section" id="core-platform-monitoring-first-steps"><div class="titlepage"><div><div><h3 class="title">2.2. Core platform monitoring first steps</h3></div></div></div><p>
				After OpenShift Container Platform is installed, core platform monitoring components immediately begin collecting metrics, which you can query and view. The default in-cluster monitoring stack includes the core platform Prometheus instance that collects metrics from your cluster and the core Alertmanager instance that routes alerts, among other components. Depending on who will use the monitoring stack and for what purposes, as a cluster administrator, you can further configure these monitoring components to suit the needs of different users in various scenarios.
			</p><section class="section" id="configuring-core-platform-monitoring-postinstallation-steps_core-platform-monitoring-first-steps"><div class="titlepage"><div><div><h4 class="title">2.2.1. Configuring core platform monitoring: Postinstallation steps</h4></div></div></div><p>
					After OpenShift Container Platform is installed, cluster administrators typically configure core platform monitoring to suit their needs. These activities include setting up storage and configuring options for Prometheus, Alertmanager, and other monitoring components.
				</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						By default, in a newly installed OpenShift Container Platform system, users can query and view collected metrics. You need only configure an alert receiver if you want users to receive alert notifications. Any other configuration options listed here are optional.
					</p></div></rh-alert><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<a class="link" href="#creating-cluster-monitoring-configmap_preparing-to-configure-the-monitoring-stack" title="3.1.2. Creating a cluster monitoring config map">Create the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object</a> if it does not exist.
						</li><li class="listitem">
<a class="link" href="#configuring-alert-notifications_configuring-alerts-and-notifications" title="3.5.4. Configuring alert notifications">Configure notifications for default platform alerts</a> so that Alertmanager can send alerts to an external notification system such as email, Slack, or PagerDuty.
						</li><li class="listitem"><p class="simpara">
							For shorter term data retention, <a class="link" href="#configuring-persistent-storage_storing-and-recording-data" title="3.3.1. Configuring persistent storage">configure persistent storage</a> for Prometheus and Alertmanager to store metrics and alert data. Specify the metrics data retention parameters for Prometheus and Thanos Ruler.
						</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										In multi-node clusters, you must configure persistent storage for Prometheus, Alertmanager, and Thanos Ruler to ensure high availability.
									</li><li class="listitem">
										By default, in a newly installed OpenShift Container Platform system, the monitoring <code class="literal">ClusterOperator</code> resource reports a <code class="literal">PrometheusDataPersistenceNotConfigured</code> status message to remind you that storage is not configured.
									</li></ul></div></div></rh-alert></li><li class="listitem"><p class="simpara">
							For longer term data retention, <a class="link" href="#configuring-remote-write-storage_configuring-metrics" title="3.4.1. Configuring remote write storage">configure the remote write feature</a> to enable Prometheus to send ingested metrics to remote systems for storage.
						</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
								Be sure to <a class="link" href="#creating-cluster-id-labels-for-metrics_configuring-metrics" title="3.4.2. Creating cluster ID labels for metrics">add cluster ID labels to metrics</a> for use with your remote write storage configuration.
							</p></div></rh-alert></li><li class="listitem">
<a class="link" href="#granting-users-permissions-for-core-platform-monitoring_preparing-to-configure-the-monitoring-stack" title="3.1.3. Granting users permissions for core platform monitoring">Grant monitoring cluster roles</a> to any non-administrator users that need to access certain monitoring features.
						</li><li class="listitem">
<a class="link" href="#assigning-tolerations-to-monitoring-components_configuring-performance-and-scalability" title="3.2.1.2. Assigning tolerations to monitoring components">Assign tolerations</a> to monitoring stack components so that administrators can move them to tainted nodes.
						</li><li class="listitem">
<a class="link" href="#setting-the-body-size-limit-for-metrics-scraping_configuring-performance-and-scalability" title="3.2.2. Setting the body size limit for metrics scraping">Set the body size limit</a> for metrics collection to help avoid situations in which Prometheus consumes excessive amounts of memory when scraped targets return a response that contains a large amount of data.
						</li><li class="listitem">
<a class="link" href="#managing-alerting-rules-for-core-platform-monitoring_managing-alerts-as-an-administrator" title="6.1.4. Managing alerting rules for core platform monitoring">Modify or create alerting rules</a> for your cluster. These rules specify the conditions that trigger alerts, such as high CPU or memory usage, network latency, and so forth.
						</li><li class="listitem">
<a class="link" href="#managing-cpu-and-memory-resources-for-monitoring-components_configuring-performance-and-scalability" title="3.2.3. Managing CPU and memory resources for monitoring components">Specify resource limits and requests for monitoring components</a> to ensure that the containers that run monitoring components have enough CPU and memory resources.
						</li></ul></div><p>
					With the monitoring stack configured to suit your needs, Prometheus collects metrics from the specified services and stores these metrics according to your settings. You can go to the <span class="strong strong"><strong>Observe</strong></span> pages in the OpenShift Container Platform web console to view and query collected metrics, manage alerts, identify performance bottlenecks, and scale resources as needed:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<a class="link" href="#reviewing-monitoring-dashboards-admin_accessing-metrics-as-an-administrator" title="5.1.4. Reviewing monitoring dashboards as a cluster administrator">View dashboards</a> to visualize collected metrics, troubleshoot alerts, and monitor other information about your cluster.
						</li><li class="listitem">
<a class="link" href="#querying-metrics-for-all-projects-with-mon-dashboard_accessing-metrics-as-an-administrator" title="5.1.2. Querying metrics for all projects with the OpenShift Container Platform web console">Query collected metrics</a> by creating PromQL queries or using predefined queries.
						</li></ul></div></section></section><section class="section" id="user-workload-monitoring-first-steps"><div class="titlepage"><div><div><h3 class="title">2.3. User workload monitoring first steps</h3></div></div></div><p>
				As a cluster administrator, you can optionally enable monitoring for user-defined projects in addition to core platform monitoring. Non-administrator users such as developers can then monitor their own projects outside of core platform monitoring.
			</p><p>
				Cluster administrators typically complete the following activities to configure user-defined projects so that users can view collected metrics, query these metrics, and receive alerts for their own projects:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<a class="link" href="#enabling-monitoring-for-user-defined-projects-uwm_preparing-to-configure-the-monitoring-stack-uwm" title="4.1.2. Enabling monitoring for user-defined projects">Enable user workload monitoring</a>.
					</li><li class="listitem">
<a class="link" href="#granting-users-permission-to-monitor-user-defined-projects_preparing-to-configure-the-monitoring-stack-uwm" title="4.1.4. Granting users permissions for monitoring for user-defined projects">Grant non-administrator users permissions to monitor user-defined projects</a> by assigning the <code class="literal">monitoring-rules-view</code>, <code class="literal">monitoring-rules-edit</code>, or <code class="literal">monitoring-edit</code> cluster roles.
					</li><li class="listitem">
<a class="link" href="#granting-users-permission-to-configure-alert-routing-for-user-defined-projects_preparing-to-configure-the-monitoring-stack-uwm" title="4.1.3.3. Granting users permission to configure alert routing for user-defined projects">Assign the <code class="literal">user-workload-monitoring-config-edit</code> role</a> to grant non-administrator users permission to configure user-defined projects.
					</li><li class="listitem">
<a class="link" href="#enabling-alert-routing-for-user-defined-projects_preparing-to-configure-the-monitoring-stack-uwm" title="4.1.3. Enabling alert routing for user-defined projects">Enable alert routing for user-defined projects</a> so that developers and other users can configure custom alerts and alert routing for their projects.
					</li><li class="listitem">
						If needed, configure alert routing for user-defined projects to <a class="link" href="#enabling-a-separate-alertmanager-instance-for-user-defined-alert-routing_preparing-to-configure-the-monitoring-stack-uwm" title="4.1.3.2. Enabling a separate Alertmanager instance for user-defined alert routing">use an optional Alertmanager instance dedicated for use only by user-defined projects</a>.
					</li><li class="listitem">
<a class="link" href="#configuring-alert-notifications_configuring-alerts-and-notifications-uwm" title="4.5.4. Configuring alert notifications">Configure notifications for user-defined alerts</a>.
					</li><li class="listitem">
						If you use the platform Alertmanager instance for user-defined alert routing, <a class="link" href="#configuring-different-alert-receivers-for-default-platform-alerts-and-user-defined-alerts_configuring-alerts-and-notifications-uwm" title="4.5.4.3. Configuring different alert receivers for default platform alerts and user-defined alerts">configure different alert receivers</a> for default platform alerts and user-defined alerts.
					</li></ul></div></section><section class="section" id="developer-and-non-administrator-steps"><div class="titlepage"><div><div><h3 class="title">2.4. Developer and non-administrator steps</h3></div></div></div><p>
				After monitoring for user-defined projects is enabled and configured, developers and other non-administrator users can then perform the following activities to set up and use monitoring for their own projects:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<a class="link" href="#setting-up-metrics-collection-for-user-defined-projects_configuring-metrics-uwm" title="4.4.3. Setting up metrics collection for user-defined projects">Deploy and monitor services</a>.
					</li><li class="listitem">
<a class="link" href="#managing-alerting-rules-for-user-defined-projects-uwm_managing-alerts-as-a-developer" title="6.2.4. Managing alerting rules for user-defined projects">Create and manage alerting rules</a>.
					</li><li class="listitem">
<a class="link" href="#managing-alerts-as-a-developer" title="6.2. Managing alerts as a Developer">Receive and manage alerts</a> for your projects.
					</li><li class="listitem">
						If granted the <code class="literal">alert-routing-edit</code> cluster role, <a class="link" href="#configuring-alert-routing-for-user-defined-projects_configuring-alerts-and-notifications-uwm" title="4.5.4.1. Configuring alert routing for user-defined projects">configure alert routing</a>.
					</li><li class="listitem">
<a class="link" href="#reviewing-monitoring-dashboards-developer_accessing-metrics-as-a-developer" title="5.2.3. Reviewing monitoring dashboards as a developer">View dashboards</a> by using the OpenShift Container Platform web console.
					</li><li class="listitem">
<a class="link" href="#querying-metrics-for-user-defined-projects-with-mon-dashboard_accessing-metrics-as-a-developer" title="5.2.2. Querying metrics for user-defined projects with the OpenShift Container Platform web console">Query the collected metrics</a> by creating PromQL queries or using predefined queries.
					</li></ul></div></section></section><section class="chapter" id="configuring-core-platform-monitoring"><div class="titlepage"><div><div><h2 class="title">Chapter 3. Configuring core platform monitoring</h2></div></div></div><section class="section" id="preparing-to-configure-the-monitoring-stack"><div class="titlepage"><div><div><h3 class="title">3.1. Preparing to configure core platform monitoring stack</h3></div></div></div><p>
				The OpenShift Container Platform installation program provides only a low number of configuration options before installation. Configuring most OpenShift Container Platform framework components, including the cluster monitoring stack, happens after the installation.
			</p><p>
				This section explains which monitoring components can be configured and how to prepare for configuring the monitoring stack.
			</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Not all configuration parameters for the monitoring stack are exposed. Only the parameters and fields listed in the <a class="link" href="#cluster-monitoring-operator-configuration-reference" title="8.1. Cluster Monitoring Operator configuration reference">Config map reference for the Cluster Monitoring Operator</a> are supported for configuration.
						</li><li class="listitem">
							The monitoring stack imposes additional resource requirements. Consult the computing resources recommendations in <a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/scalability_and_performance/#scaling-cluster-monitoring-operator_recommended-infrastructure-practices">Scaling the Cluster Monitoring Operator</a> and verify that you have sufficient resources.
						</li></ul></div></div></rh-alert><section class="section" id="configurable-monitoring-components_preparing-to-configure-the-monitoring-stack"><div class="titlepage"><div><div><h4 class="title">3.1.1. Configurable monitoring components</h4></div></div></div><p>
					This table shows the monitoring components you can configure and the keys used to specify the components in the <code class="literal">cluster-monitoring-config</code> config map.
				</p><rh-table id="idm140059300490720"><table class="lt-4-cols lt-7-rows"><caption>Table 3.1. Configurable core platform monitoring components</caption><colgroup><col class="col_1" style="width: 50%; "/><!--Empty--><col class="col_2" style="width: 50%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059301366944" scope="col" valign="top">Component</th><th align="left" id="idm140059301365856" scope="col" valign="top">cluster-monitoring-config config map key</th></tr></thead><tbody><tr><td align="left" headers="idm140059301366944" valign="top"> <p>
									Prometheus Operator
								</p>
</td><td align="left" headers="idm140059301365856" valign="top"> <p>
<code class="literal">prometheusOperator</code>
</p>
</td></tr><tr><td align="left" headers="idm140059301366944" valign="top"> <p>
									Prometheus
								</p>
</td><td align="left" headers="idm140059301365856" valign="top"> <p>
<code class="literal">prometheusK8s</code>
</p>
</td></tr><tr><td align="left" headers="idm140059301366944" valign="top"> <p>
									Alertmanager
								</p>
</td><td align="left" headers="idm140059301365856" valign="top"> <p>
<code class="literal">alertmanagerMain</code>
</p>
</td></tr><tr><td align="left" headers="idm140059301366944" valign="top"> <p>
									Thanos Querier
								</p>
</td><td align="left" headers="idm140059301365856" valign="top"> <p>
<code class="literal">thanosQuerier</code>
</p>
</td></tr><tr><td align="left" headers="idm140059301366944" valign="top"> <p>
									kube-state-metrics
								</p>
</td><td align="left" headers="idm140059301365856" valign="top"> <p>
<code class="literal">kubeStateMetrics</code>
</p>
</td></tr><tr><td align="left" headers="idm140059301366944" valign="top"> <p>
									monitoring-plugin
								</p>
</td><td align="left" headers="idm140059301365856" valign="top"> <p>
<code class="literal">monitoringPlugin</code>
</p>
</td></tr><tr><td align="left" headers="idm140059301366944" valign="top"> <p>
									openshift-state-metrics
								</p>
</td><td align="left" headers="idm140059301365856" valign="top"> <p>
<code class="literal">openshiftStateMetrics</code>
</p>
</td></tr><tr><td align="left" headers="idm140059301366944" valign="top"> <p>
									Telemeter Client
								</p>
</td><td align="left" headers="idm140059301365856" valign="top"> <p>
<code class="literal">telemeterClient</code>
</p>
</td></tr><tr><td align="left" headers="idm140059301366944" valign="top"> <p>
									Metrics Server
								</p>
</td><td align="left" headers="idm140059301365856" valign="top"> <p>
<code class="literal">metricsServer</code>
</p>
</td></tr></tbody></table></rh-table><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
						Different configuration changes to the <code class="literal">ConfigMap</code> object result in different outcomes:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								The pods are not redeployed. Therefore, there is no service outage.
							</li><li class="listitem"><p class="simpara">
								The affected pods are redeployed:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										For single-node clusters, this results in temporary service outage.
									</li><li class="listitem">
										For multi-node clusters, because of high-availability, the affected pods are gradually rolled out and the monitoring stack remains available.
									</li><li class="listitem">
										Configuring and resizing a persistent volume always results in a service outage, regardless of high availability.
									</li></ul></div></li></ul></div><p>
						Each procedure that requires a change in the config map includes its expected outcome.
					</p></div></rh-alert></section><section class="section" id="creating-cluster-monitoring-configmap_preparing-to-configure-the-monitoring-stack"><div class="titlepage"><div><div><h4 class="title">3.1.2. Creating a cluster monitoring config map</h4></div></div></div><p>
					You can configure the core OpenShift Container Platform monitoring components by creating and updating the <code class="literal">cluster-monitoring-config</code> config map in the <code class="literal">openshift-monitoring</code> project. The Cluster Monitoring Operator (CMO) then configures the core components of the monitoring stack.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Check whether the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object exists:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring get configmap cluster-monitoring-config</pre></li><li class="listitem"><p class="simpara">
							If the <code class="literal">ConfigMap</code> object does not exist:
						</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Create the following YAML manifest. In this example the file is called <code class="literal">cluster-monitoring-config.yaml</code>:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |</pre></li><li class="listitem"><p class="simpara">
									Apply the configuration to create the <code class="literal">ConfigMap</code> object:
								</p><pre class="programlisting language-terminal">$ oc apply -f cluster-monitoring-config.yaml</pre></li></ol></div></li></ol></div></section><section class="section" id="granting-users-permissions-for-core-platform-monitoring_preparing-to-configure-the-monitoring-stack"><div class="titlepage"><div><div><h4 class="title">3.1.3. Granting users permissions for core platform monitoring</h4></div></div></div><p>
					As a cluster administrator, you can monitor all core OpenShift Container Platform and user-defined projects.
				</p><p>
					You can also grant developers and other users different permissions for core platform monitoring. You can grant the permissions by assigning one of the following monitoring roles or cluster roles:
				</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059299505760" scope="col" valign="top">Name</th><th align="left" id="idm140059306647104" scope="col" valign="top">Description</th><th align="left" id="idm140059306646016" scope="col" valign="top">Project</th></tr></thead><tbody><tr><td align="left" headers="idm140059299505760" valign="top"> <p>
<code class="literal">cluster-monitoring-metrics-api</code>
</p>
</td><td align="left" headers="idm140059306647104" valign="top"> <p>
									Users with this role have the ability to access Thanos Querier API endpoints. Additionally, it grants access to the core platform Prometheus API and user-defined Thanos Ruler API endpoints.
								</p>
</td><td align="left" headers="idm140059306646016" valign="top"> <p>
<code class="literal">openshift-monitoring</code>
</p>
</td></tr><tr><td align="left" headers="idm140059299505760" valign="top"> <p>
<code class="literal">cluster-monitoring-operator-alert-customization</code>
</p>
</td><td align="left" headers="idm140059306647104" valign="top"> <p>
									Users with this role can manage <code class="literal">AlertingRule</code> and <code class="literal">AlertRelabelConfig</code> resources for core platform monitoring. These permissions are required for the alert customization feature.
								</p>
</td><td align="left" headers="idm140059306646016" valign="top"> <p>
<code class="literal">openshift-monitoring</code>
</p>
</td></tr><tr><td align="left" headers="idm140059299505760" valign="top"> <p>
<code class="literal">monitoring-alertmanager-edit</code>
</p>
</td><td align="left" headers="idm140059306647104" valign="top"> <p>
									Users with this role can manage the Alertmanager API for core platform monitoring. They can also manage alert silences in the <span class="strong strong"><strong>Administrator</strong></span> perspective of the OpenShift Container Platform web console.
								</p>
</td><td align="left" headers="idm140059306646016" valign="top"> <p>
<code class="literal">openshift-monitoring</code>
</p>
</td></tr><tr><td align="left" headers="idm140059299505760" valign="top"> <p>
<code class="literal">monitoring-alertmanager-view</code>
</p>
</td><td align="left" headers="idm140059306647104" valign="top"> <p>
									Users with this role can monitor the Alertmanager API for core platform monitoring. They can also view alert silences in the <span class="strong strong"><strong>Administrator</strong></span> perspective of the OpenShift Container Platform web console.
								</p>
</td><td align="left" headers="idm140059306646016" valign="top"> <p>
<code class="literal">openshift-monitoring</code>
</p>
</td></tr><tr><td align="left" headers="idm140059299505760" valign="top"> <p>
<code class="literal">cluster-monitoring-view</code>
</p>
</td><td align="left" headers="idm140059306647104" valign="top"> <p>
									Users with this cluster role have the same access rights as <code class="literal">cluster-monitoring-metrics-api</code> role, with additional permissions, providing access to the <code class="literal">/federate</code> endpoint for the user-defined Prometheus.
								</p>
</td><td align="left" headers="idm140059306646016" valign="top"> <p>
									Must be bound with <code class="literal">ClusterRoleBinding</code> to gain access to the <code class="literal">/federate</code> endpoint for the user-defined Prometheus.
								</p>
</td></tr></tbody></table></rh-table><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#resources-reference-for-the-cluster-monitoring-operator_accessing-monitoring-apis-by-using-the-cli" title="5.3.5. Resources reference for the Cluster Monitoring Operator">Resources reference for the Cluster Monitoring Operator</a>
</li><li class="listitem">
<a class="link" href="#cmo-services-resources_accessing-monitoring-apis-by-using-the-cli" title="5.3.5.2. CMO services resources">CMO services resources</a>
</li></ul></div><section class="section" id="granting-user-permissions-using-the-web-console_preparing-to-configure-the-monitoring-stack"><div class="titlepage"><div><div><h5 class="title">3.1.3.1. Granting user permissions by using the web console</h5></div></div></div><p>
						You can grant users permissions for the <code class="literal">openshift-monitoring</code> project or their own projects, by using the OpenShift Container Platform web console.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
							</li><li class="listitem">
								The user account that you are assigning the role to already exists.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								In the <span class="strong strong"><strong>Administrator</strong></span> perspective of the OpenShift Container Platform web console, go to <span class="strong strong"><strong>User Management</strong></span> → <span class="strong strong"><strong>RoleBindings</strong></span> → <span class="strong strong"><strong>Create binding</strong></span>.
							</li><li class="listitem">
								In the <span class="strong strong"><strong>Binding Type</strong></span> section, select the <span class="strong strong"><strong>Namespace Role Binding</strong></span> type.
							</li><li class="listitem">
								In the <span class="strong strong"><strong>Name</strong></span> field, enter a name for the role binding.
							</li><li class="listitem"><p class="simpara">
								In the <span class="strong strong"><strong>Namespace</strong></span> field, select the project where you want to grant the access.
							</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
									The monitoring role or cluster role permissions that you grant to a user by using this procedure apply only to the project that you select in the <span class="strong strong"><strong>Namespace</strong></span> field.
								</p></div></rh-alert></li><li class="listitem">
								Select a monitoring role or cluster role from the <span class="strong strong"><strong>Role Name</strong></span> list.
							</li><li class="listitem">
								In the <span class="strong strong"><strong>Subject</strong></span> section, select <span class="strong strong"><strong>User</strong></span>.
							</li><li class="listitem">
								In the <span class="strong strong"><strong>Subject Name</strong></span> field, enter the name of the user.
							</li><li class="listitem">
								Select <span class="strong strong"><strong>Create</strong></span> to apply the role binding.
							</li></ol></div></section><section class="section" id="granting-user-permissions-using-the-cli_preparing-to-configure-the-monitoring-stack"><div class="titlepage"><div><div><h5 class="title">3.1.3.2. Granting user permissions by using the CLI</h5></div></div></div><p>
						You can grant users permissions for the <code class="literal">openshift-monitoring</code> project or their own projects, by using the OpenShift CLI (<code class="literal">oc</code>).
					</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
							Whichever role or cluster role you choose, you must bind it against a specific project as a cluster administrator.
						</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
							</li><li class="listitem">
								The user account that you are assigning the role to already exists.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								To assign a monitoring role to a user for a project, enter the following command:
							</p><pre class="programlisting language-terminal">$ oc adm policy add-role-to-user &lt;role&gt; &lt;user&gt; -n &lt;namespace&gt; --role-namespace &lt;namespace&gt; <span id="CO1-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO1-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Substitute <code class="literal">&lt;role&gt;</code> with the wanted monitoring role, <code class="literal">&lt;user&gt;</code> with the user to whom you want to assign the role, and <code class="literal">&lt;namespace&gt;</code> with the project where you want to grant the access.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								To assign a monitoring cluster role to a user for a project, enter the following command:
							</p><pre class="programlisting language-terminal">$ oc adm policy add-cluster-role-to-user &lt;cluster-role&gt; &lt;user&gt; -n &lt;namespace&gt; <span id="CO2-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO2-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Substitute <code class="literal">&lt;cluster-role&gt;</code> with the wanted monitoring cluster role, <code class="literal">&lt;user&gt;</code> with the user to whom you want to assign the cluster role, and <code class="literal">&lt;namespace&gt;</code> with the project where you want to grant the access.
									</div></dd></dl></div></li></ul></div></section></section></section><section class="section" id="configuring-performance-and-scalability"><div class="titlepage"><div><div><h3 class="title">3.2. Configuring performance and scalability for core platform monitoring</h3></div></div></div><p>
				You can configure the monitoring stack to optimize the performance and scale of your clusters. The following documentation provides information about how to distribute the monitoring components and control the impact of the monitoring stack on CPU and memory resources.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<a class="link" href="#about-performance-and-scalability_key-concepts" title="1.3.1. About performance and scalability">About performance and scalability</a>
</li></ul></div><section class="section" id="controlling-placement-and-distribution-of-monitoing-components_configuring-performance-and-scalability"><div class="titlepage"><div><div><h4 class="title">3.2.1. Controlling the placement and distribution of monitoring components</h4></div></div></div><p>
					You can move the monitoring stack components to specific nodes:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Use the <code class="literal">nodeSelector</code> constraint with labeled nodes to move any of the monitoring stack components to specific nodes.
						</li><li class="listitem">
							Assign tolerations to enable moving components to tainted nodes.
						</li></ul></div><p>
					By doing so, you control the placement and distribution of the monitoring components across a cluster.
				</p><p>
					By controlling placement and distribution of monitoring components, you can optimize system resource use, improve performance, and separate workloads based on specific requirements or policies.
				</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#using-node-selectors-to-move-monitoring-components_key-concepts" title="1.3.1.1. Using node selectors to move monitoring components">Using node selectors to move monitoring components</a>
</li></ul></div><section class="section" id="moving-monitoring-components-to-different-nodes_configuring-performance-and-scalability"><div class="titlepage"><div><div><h5 class="title">3.2.1.1. Moving monitoring components to different nodes</h5></div></div></div><p>
						To specify the nodes in your cluster on which monitoring stack components will run, configure the <code class="literal">nodeSelector</code> constraint for the components in the <code class="literal">cluster-monitoring-config</code> config map to match labels assigned to the nodes.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							You cannot add a node selector constraint directly to an existing scheduled pod.
						</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
							</li><li class="listitem">
								You have created the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								If you have not done so yet, add a label to the nodes on which you want to run the monitoring components:
							</p><pre class="programlisting language-terminal">$ oc label nodes &lt;node_name&gt; &lt;node_label&gt; <span id="CO3-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO3-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Replace <code class="literal">&lt;node_name&gt;</code> with the name of the node where you want to add the label. Replace <code class="literal">&lt;node_label&gt;</code> with the name of the wanted label.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Edit the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object in the <code class="literal">openshift-monitoring</code> project:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</pre></li><li class="listitem"><p class="simpara">
								Specify the node labels for the <code class="literal">nodeSelector</code> constraint for the component under <code class="literal">data/config.yaml</code>:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    # ...
    &lt;component&gt;: <span id="CO4-1"><!--Empty--></span><span class="callout">1</span>
      nodeSelector:
        &lt;node_label_1&gt; <span id="CO4-2"><!--Empty--></span><span class="callout">2</span>
        &lt;node_label_2&gt; <span id="CO4-3"><!--Empty--></span><span class="callout">3</span>
    # ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO4-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Substitute <code class="literal">&lt;component&gt;</code> with the appropriate monitoring stack component name.
									</div></dd><dt><a href="#CO4-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Substitute <code class="literal">&lt;node_label_1&gt;</code> with the label you added to the node.
									</div></dd><dt><a href="#CO4-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Optional: Specify additional labels. If you specify additional labels, the pods for the component are only scheduled on the nodes that contain all of the specified labels.
									</div></dd></dl></div><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
									If monitoring components remain in a <code class="literal">Pending</code> state after configuring the <code class="literal">nodeSelector</code> constraint, check the pod events for errors relating to taints and tolerations.
								</p></div></rh-alert></li><li class="listitem">
								Save the file to apply the changes. The components specified in the new configuration are automatically moved to the new nodes, and the pods affected by the new configuration are redeployed.
							</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#preparing-to-configure-the-monitoring-stack" title="3.1. Preparing to configure core platform monitoring stack">Preparing to configure core platform monitoring stack</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/nodes/#nodes-nodes-working-updating_nodes-nodes-working">Understanding how to update labels on nodes</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/nodes/#nodes-scheduler-node-selectors">Placing pods on specific nodes using node selectors</a>
</li><li class="listitem">
<a class="link" href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector">nodeSelector</a> (Kubernetes documentation)
							</li></ul></div></section><section class="section" id="assigning-tolerations-to-monitoring-components_configuring-performance-and-scalability"><div class="titlepage"><div><div><h5 class="title">3.2.1.2. Assigning tolerations to monitoring components</h5></div></div></div><p>
						You can assign tolerations to any of the monitoring stack components to enable moving them to tainted nodes.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
							</li><li class="listitem">
								You have created the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Edit the <code class="literal">cluster-monitoring-config</code> config map in the <code class="literal">openshift-monitoring</code> project:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</pre></li><li class="listitem"><p class="simpara">
								Specify <code class="literal">tolerations</code> for the component:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    &lt;component&gt;:
      tolerations:
        &lt;toleration_specification&gt;</pre><p class="simpara">
								Substitute <code class="literal">&lt;component&gt;</code> and <code class="literal">&lt;toleration_specification&gt;</code> accordingly.
							</p><p class="simpara">
								For example, <code class="literal">oc adm taint nodes node1 key1=value1:NoSchedule</code> adds a taint to <code class="literal">node1</code> with the key <code class="literal">key1</code> and the value <code class="literal">value1</code>. This prevents monitoring components from deploying pods on <code class="literal">node1</code> unless a toleration is configured for that taint. The following example configures the <code class="literal">alertmanagerMain</code> component to tolerate the example taint:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    alertmanagerMain:
      tolerations:
      - key: "key1"
        operator: "Equal"
        value: "value1"
        effect: "NoSchedule"</pre></li><li class="listitem">
								Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.
							</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#preparing-to-configure-the-monitoring-stack" title="3.1. Preparing to configure core platform monitoring stack">Preparing to configure core platform monitoring stack</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/nodes/#nodes-scheduler-taints-tolerations">Controlling pod placement using node taints</a>
</li><li class="listitem">
<a class="link" href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/">Taints and Tolerations</a> (Kubernetes documentation)
							</li></ul></div></section></section><section class="section" id="setting-the-body-size-limit-for-metrics-scraping_configuring-performance-and-scalability"><div class="titlepage"><div><div><h4 class="title">3.2.2. Setting the body size limit for metrics scraping</h4></div></div></div><p>
					By default, no limit exists for the uncompressed body size for data returned from scraped metrics targets. You can set a body size limit to help avoid situations in which Prometheus consumes excessive amounts of memory when scraped targets return a response that contains a large amount of data. In addition, by setting a body size limit, you can reduce the impact that a malicious target might have on Prometheus and on the cluster as a whole.
				</p><p>
					After you set a value for <code class="literal">enforcedBodySizeLimit</code>, the alert <code class="literal">PrometheusScrapeBodySizeLimitHit</code> fires when at least one Prometheus scrape target replies with a response body larger than the configured value.
				</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						If metrics data scraped from a target has an uncompressed body size exceeding the configured size limit, the scrape fails. Prometheus then considers this target to be down and sets its <code class="literal">up</code> metric value to <code class="literal">0</code>, which can trigger the <code class="literal">TargetDown</code> alert.
					</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object in the <code class="literal">openshift-monitoring</code> namespace:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</pre></li><li class="listitem"><p class="simpara">
							Add a value for <code class="literal">enforcedBodySizeLimit</code> to <code class="literal">data/config.yaml/prometheusK8s</code> to limit the body size that can be accepted per target scrape:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |-
    prometheusK8s:
      enforcedBodySizeLimit: 40MB <span id="CO5-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO5-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify the maximum body size for scraped metrics targets. This <code class="literal">enforcedBodySizeLimit</code> example limits the uncompressed size per target scrape to 40 megabytes. Valid numeric values use the Prometheus data size format: B (bytes), KB (kilobytes), MB (megabytes), GB (gigabytes), TB (terabytes), PB (petabytes), and EB (exabytes). The default value is <code class="literal">0</code>, which specifies no limit. You can also set the value to <code class="literal">automatic</code> to calculate the limit automatically based on cluster capacity.
								</div></dd></dl></div></li><li class="listitem">
							Save the file to apply the changes. The new configuration is applied automatically.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config">scrape_config configuration</a> (Prometheus documentation)
						</li></ul></div></section><section class="section" id="managing-cpu-and-memory-resources-for-monitoring-components_configuring-performance-and-scalability"><div class="titlepage"><div><div><h4 class="title">3.2.3. Managing CPU and memory resources for monitoring components</h4></div></div></div><p>
					You can ensure that the containers that run monitoring components have enough CPU and memory resources by specifying values for resource limits and requests for those components.
				</p><p>
					You can configure these limits and requests for core platform monitoring components in the <code class="literal">openshift-monitoring</code> namespace.
				</p><section class="section" id="specifying-limits-and-resource-requests-for-monitoring-components_configuring-performance-and-scalability"><div class="titlepage"><div><div><h5 class="title">3.2.3.1. Specifying limits and requests</h5></div></div></div><p>
						To configure CPU and memory resources, specify values for resource limits and requests in the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object in the <code class="literal">openshift-monitoring</code> namespace.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
							</li><li class="listitem">
								You have created the <code class="literal">ConfigMap</code> object named <code class="literal">cluster-monitoring-config</code>.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Edit the <code class="literal">cluster-monitoring-config</code> config map in the <code class="literal">openshift-monitoring</code> project:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</pre></li><li class="listitem"><p class="simpara">
								Add values to define resource limits and requests for each component you want to configure.
							</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
									Ensure that the value set for a limit is always higher than the value set for a request. Otherwise, an error will occur, and the container will not run.
								</p></div></rh-alert><div class="formalpara"><p class="title"><strong>Example of setting resource limits and requests</strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    alertmanagerMain:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
    prometheusK8s:
      resources:
        limits:
          cpu: 500m
          memory: 3Gi
        requests:
          cpu: 200m
          memory: 500Mi
    thanosQuerier:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
    prometheusOperator:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
    metricsServer:
      resources:
        requests:
          cpu: 10m
          memory: 50Mi
        limits:
          cpu: 50m
          memory: 500Mi
    kubeStateMetrics:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
    telemeterClient:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
    openshiftStateMetrics:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
    nodeExporter:
      resources:
        limits:
          cpu: 50m
          memory: 150Mi
        requests:
          cpu: 20m
          memory: 50Mi
    monitoringPlugin:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
    prometheusOperatorAdmissionWebhook:
      resources:
        limits:
          cpu: 50m
          memory: 100Mi
        requests:
          cpu: 20m
          memory: 50Mi</pre>
<p></p></div></li><li class="listitem">
								Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.
							</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#about-specifying-limits-and-requests-for-monitoring-components_key-concepts" title="1.3.1.3. About specifying limits and requests for monitoring components">About specifying limits and requests</a>
</li><li class="listitem">
<a class="link" href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits">Kubernetes requests and limits documentation</a> (Kubernetes documentation)
							</li></ul></div></section></section><section class="section" id="choosing-a-metrics-collection-profile_configuring-performance-and-scalability"><div class="titlepage"><div><div><h4 class="title">3.2.4. Choosing a metrics collection profile</h4></div></div></div><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
						Metrics collection profile is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
					</p><p>
						For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.
					</p></div></rh-alert><p>
					To choose a metrics collection profile for core OpenShift Container Platform monitoring components, edit the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have enabled Technology Preview features by using the <code class="literal">FeatureGate</code> custom resource (CR).
						</li><li class="listitem">
							You have created the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object.
						</li><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object in the <code class="literal">openshift-monitoring</code> project:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</pre></li><li class="listitem"><p class="simpara">
							Add the metrics collection profile setting under <code class="literal">data/config.yaml/prometheusK8s</code>:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      collectionProfile: &lt;metrics_collection_profile_name&gt; <span id="CO6-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO6-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The name of the metrics collection profile. The available values are <code class="literal">full</code> or <code class="literal">minimal</code>. If you do not specify a value or if the <code class="literal">collectionProfile</code> key name does not exist in the config map, the default setting of <code class="literal">full</code> is used.
								</div></dd></dl></div><p class="simpara">
							The following example sets the metrics collection profile to <code class="literal">minimal</code> for the core platform instance of Prometheus:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      collectionProfile: <span class="strong strong"><strong>minimal</strong></span></pre></li><li class="listitem">
							Save the file to apply the changes. The new configuration is applied automatically.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#configuring-metrics-collection-profiles_key-concepts" title="1.3.1.4. About metrics collection profiles">About metrics collection profiles</a>
</li><li class="listitem">
<a class="link" href="#viewing-a-list-of-available-metrics_accessing-metrics-as-an-administrator" title="5.1.1. Viewing a list of available metrics">Viewing a list of available metrics</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/nodes/#nodes-cluster-enabling">Enabling features using feature gates</a>
</li></ul></div></section><section class="section" id="configuring-pod-topology-spread-constraints_configuring-performance-and-scalability"><div class="titlepage"><div><div><h4 class="title">3.2.5. Configuring pod topology spread constraints</h4></div></div></div><p>
					You can configure pod topology spread constraints for all the pods deployed by the Cluster Monitoring Operator to control how pod replicas are scheduled to nodes across zones. This ensures that the pods are highly available and run more efficiently, because workloads are spread across nodes in different data centers or hierarchical infrastructure zones.
				</p><p>
					You can configure pod topology spread constraints for monitoring pods by using the <code class="literal">cluster-monitoring-config</code> config map.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
						</li><li class="listitem">
							You have created the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">cluster-monitoring-config</code> config map in the <code class="literal">openshift-monitoring</code> project:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</pre></li><li class="listitem"><p class="simpara">
							Add the following settings under the <code class="literal">data/config.yaml</code> field to configure pod topology spread constraints:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    &lt;component&gt;: <span id="CO7-1"><!--Empty--></span><span class="callout">1</span>
      topologySpreadConstraints:
      - maxSkew: &lt;n&gt; <span id="CO7-2"><!--Empty--></span><span class="callout">2</span>
        topologyKey: &lt;key&gt; <span id="CO7-3"><!--Empty--></span><span class="callout">3</span>
        whenUnsatisfiable: &lt;value&gt; <span id="CO7-4"><!--Empty--></span><span class="callout">4</span>
        labelSelector: <span id="CO7-5"><!--Empty--></span><span class="callout">5</span>
          &lt;match_option&gt;</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO7-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify a name of the component for which you want to set up pod topology spread constraints.
								</div></dd><dt><a href="#CO7-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify a numeric value for <code class="literal">maxSkew</code>, which defines the degree to which pods are allowed to be unevenly distributed.
								</div></dd><dt><a href="#CO7-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify a key of node labels for <code class="literal">topologyKey</code>. Nodes that have a label with this key and identical values are considered to be in the same topology. The scheduler tries to put a balanced number of pods into each domain.
								</div></dd><dt><a href="#CO7-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Specify a value for <code class="literal">whenUnsatisfiable</code>. Available options are <code class="literal">DoNotSchedule</code> and <code class="literal">ScheduleAnyway</code>. Specify <code class="literal">DoNotSchedule</code> if you want the <code class="literal">maxSkew</code> value to define the maximum difference allowed between the number of matching pods in the target topology and the global minimum. Specify <code class="literal">ScheduleAnyway</code> if you want the scheduler to still schedule the pod but to give higher priority to nodes that might reduce the skew.
								</div></dd><dt><a href="#CO7-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Specify <code class="literal">labelSelector</code> to find matching pods. Pods that match this label selector are counted to determine the number of pods in their corresponding topology domain.
								</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example configuration for Prometheus</strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: monitoring
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: prometheus</pre>
<p></p></div></li><li class="listitem">
							Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#using-pod-topology-spread-constraints-for-monitoring_key-concepts" title="1.3.1.2. About pod topology spread constraints for monitoring">About pod topology spread constraints for monitoring</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/nodes/#nodes-scheduler-pod-topology-spread-constraints-about">Controlling pod placement by using pod topology spread constraints</a>
</li><li class="listitem">
<a class="link" href="https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/">Pod Topology Spread Constraints</a> (Kubernetes documentation)
						</li></ul></div></section></section><section class="section" id="storing-and-recording-data"><div class="titlepage"><div><div><h3 class="title">3.3. Storing and recording data for core platform monitoring</h3></div></div></div><p>
				Store and record your metrics and alerting data, configure logs to specify which activities are recorded, control how long Prometheus retains stored data, and set the maximum amount of disk space for the data. These actions help you protect your data and use them for troubleshooting.
			</p><section class="section" id="configuring-persistent-storage_storing-and-recording-data"><div class="titlepage"><div><div><h4 class="title">3.3.1. Configuring persistent storage</h4></div></div></div><p>
					Run cluster monitoring with persistent storage to gain the following benefits:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Protect your metrics and alerting data from data loss by storing them in a persistent volume (PV). As a result, they can survive pods being restarted or recreated.
						</li><li class="listitem">
							Avoid getting duplicate notifications and losing silences for alerts when the Alertmanager pods are restarted.
						</li></ul></div><p>
					For production environments, it is highly recommended to configure persistent storage.
				</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
						In multi-node clusters, you must configure persistent storage for Prometheus, Alertmanager, and Thanos Ruler to ensure high availability.
					</p></div></rh-alert><section class="section" id="persistent-storage-prerequisites_storing-and-recording-data"><div class="titlepage"><div><div><h5 class="title">3.3.1.1. Persistent storage prerequisites</h5></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Dedicate sufficient persistent storage to ensure that the disk does not become full.
							</li><li class="listitem"><p class="simpara">
								Use <code class="literal">Filesystem</code> as the storage type value for the <code class="literal">volumeMode</code> parameter when you configure the persistent volume.
							</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
											Do not use a raw block volume, which is described with <code class="literal">volumeMode: Block</code> in the <code class="literal">PersistentVolume</code> resource. Prometheus cannot use raw block volumes.
										</li><li class="listitem">
											Prometheus does not support file systems that are not POSIX compliant. For example, some NFS file system implementations are not POSIX compliant. If you want to use an NFS file system for storage, verify with the vendor that their NFS implementation is fully POSIX compliant.
										</li></ul></div></div></rh-alert></li></ul></div></section><section class="section" id="configuring-a-persistent-volume-claim_storing-and-recording-data"><div class="titlepage"><div><div><h5 class="title">3.3.1.2. Configuring a persistent volume claim</h5></div></div></div><p>
						To use a persistent volume (PV) for monitoring components, you must configure a persistent volume claim (PVC).
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
							</li><li class="listitem">
								You have created the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Edit the <code class="literal">cluster-monitoring-config</code> config map in the <code class="literal">openshift-monitoring</code> project:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</pre></li><li class="listitem"><p class="simpara">
								Add your PVC configuration for the component under <code class="literal">data/config.yaml</code>:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    &lt;component&gt;: <span id="CO8-1"><!--Empty--></span><span class="callout">1</span>
      volumeClaimTemplate:
        spec:
          storageClassName: &lt;storage_class&gt; <span id="CO8-2"><!--Empty--></span><span class="callout">2</span>
          resources:
            requests:
              storage: &lt;amount_of_storage&gt; <span id="CO8-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO8-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specify the monitoring component for which you want to configure the PVC.
									</div></dd><dt><a href="#CO8-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specify an existing storage class. If a storage class is not specified, the default storage class is used.
									</div></dd><dt><a href="#CO8-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Specify the amount of required storage.
									</div></dd></dl></div><p class="simpara">
								The following example configures a PVC that claims persistent storage for Prometheus:
							</p><div class="formalpara"><p class="title"><strong>Example PVC configuration</strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      volumeClaimTemplate:
        spec:
          storageClassName: my-storage-class
          resources:
            requests:
              storage: 40Gi</pre>
<p></p></div></li><li class="listitem"><p class="simpara">
								Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed and the new storage configuration is applied.
							</p><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
									When you update the config map with a PVC configuration, the affected <code class="literal">StatefulSet</code> object is recreated, resulting in a temporary service outage.
								</p></div></rh-alert></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/storage/#understanding-persistent-storage">Understanding persistent storage</a>
</li><li class="listitem">
<a class="link" href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaims</a> (Kubernetes documentation)
							</li></ul></div></section><section class="section" id="resizing-a-persistent-volume_storing-and-recording-data"><div class="titlepage"><div><div><h5 class="title">3.3.1.3. Resizing a persistent volume</h5></div></div></div><p>
						You can resize a persistent volume (PV) for monitoring components, such as Prometheus or Alertmanager. You need to manually expand a persistent volume claim (PVC), and then update the config map in which the component is configured.
					</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
							You can only expand the size of the PVC. Shrinking the storage size is not possible.
						</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
							</li><li class="listitem">
								You have created the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object.
							</li><li class="listitem">
								You have configured at least one PVC for core OpenShift Container Platform monitoring components.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Manually expand a PVC with the updated storage request. For more information, see "Expanding persistent volume claims (PVCs) with a file system" in <span class="emphasis"><em>Expanding persistent volumes</em></span>.
							</li><li class="listitem"><p class="simpara">
								Edit the <code class="literal">cluster-monitoring-config</code> config map in the <code class="literal">openshift-monitoring</code> project:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</pre></li><li class="listitem"><p class="simpara">
								Add a new storage size for the PVC configuration for the component under <code class="literal">data/config.yaml</code>:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    &lt;component&gt;: <span id="CO9-1"><!--Empty--></span><span class="callout">1</span>
      volumeClaimTemplate:
        spec:
          resources:
            requests:
              storage: &lt;amount_of_storage&gt; <span id="CO9-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO9-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The component for which you want to change the storage size.
									</div></dd><dt><a href="#CO9-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specify the new size for the storage volume. It must be greater than the previous value.
									</div></dd></dl></div><p class="simpara">
								The following example sets the new PVC request to 100 gigabytes for the Prometheus instance:
							</p><div class="formalpara"><p class="title"><strong>Example storage configuration for <code class="literal">prometheusK8s</code></strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      volumeClaimTemplate:
        spec:
          resources:
            requests:
              storage: 100Gi</pre>
<p></p></div></li><li class="listitem"><p class="simpara">
								Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.
							</p><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
									When you update the config map with a new storage size, the affected <code class="literal">StatefulSet</code> object is recreated, resulting in a temporary service outage.
								</p></div></rh-alert></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/scalability_and_performance/#prometheus-database-storage-requirements_recommended-infrastructure-practices">Prometheus database storage requirements</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/storage/#expanding-pvc-filesystem_expanding-persistent-volumes">Expanding persistent volume claims (PVCs) with a file system</a>
</li></ul></div></section></section><section class="section" id="modifying-retention-time-and-size-for-prometheus-metrics-data_storing-and-recording-data"><div class="titlepage"><div><div><h4 class="title">3.3.2. Modifying retention time and size for Prometheus metrics data</h4></div></div></div><p>
					By default, Prometheus retains metrics data for 15 days for core platform monitoring. You can modify the retention time for the Prometheus instance to change when the data is deleted. You can also set the maximum amount of disk space the retained metrics data uses.
				</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						Data compaction occurs every two hours. Therefore, a persistent volume (PV) might fill up before compaction, potentially exceeding the <code class="literal">retentionSize</code> limit. In such cases, the <code class="literal">KubePersistentVolumeFillingUp</code> alert fires until the space on a PV is lower than the <code class="literal">retentionSize</code> limit.
					</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
						</li><li class="listitem">
							You have created the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">cluster-monitoring-config</code> config map in the <code class="literal">openshift-monitoring</code> project:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</pre></li><li class="listitem"><p class="simpara">
							Add the retention time and size configuration under <code class="literal">data/config.yaml</code>:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      retention: &lt;time_specification&gt; <span id="CO10-1"><!--Empty--></span><span class="callout">1</span>
      retentionSize: &lt;size_specification&gt; <span id="CO10-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO10-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The retention time: a number directly followed by <code class="literal">ms</code> (milliseconds), <code class="literal">s</code> (seconds), <code class="literal">m</code> (minutes), <code class="literal">h</code> (hours), <code class="literal">d</code> (days), <code class="literal">w</code> (weeks), or <code class="literal">y</code> (years). You can also combine time values for specific times, such as <code class="literal">1h30m15s</code>.
								</div></dd><dt><a href="#CO10-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The retention size: a number directly followed by <code class="literal">B</code> (bytes), <code class="literal">KB</code> (kilobytes), <code class="literal">MB</code> (megabytes), <code class="literal">GB</code> (gigabytes), <code class="literal">TB</code> (terabytes), <code class="literal">PB</code> (petabytes), and <code class="literal">EB</code> (exabytes).
								</div></dd></dl></div><p class="simpara">
							The following example sets the retention time to 24 hours and the retention size to 10 gigabytes for the Prometheus instance:
						</p><div class="formalpara"><p class="title"><strong>Example of setting retention time for Prometheus</strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      retention: 24h
      retentionSize: 10GB</pre>
<p></p></div></li><li class="listitem">
							Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#retention-time-and-size-for-prometheus-metrics-data_key-concepts" title="1.3.2.1. Retention time and size for Prometheus metrics">Retention time and size for Prometheus metrics</a>
</li><li class="listitem">
<a class="link" href="#preparing-to-configure-the-monitoring-stack" title="3.1. Preparing to configure core platform monitoring stack">Preparing to configure core platform monitoring stack</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/scalability_and_performance/#prometheus-database-storage-requirements_cluster-monitoring-operator">Prometheus database storage requirements</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/scalability_and_performance/#optimizing-storage">Recommended configurable storage technology</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/storage/#understanding-persistent-storage">Understanding persistent storage</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/scalability_and_performance/#optimizing-storage">Optimizing storage</a>
</li></ul></div></section><section class="section" id="configuring-audit-logs-for-metrics-server_storing-and-recording-data"><div class="titlepage"><div><div><h4 class="title">3.3.3. Configuring audit logs for Metrics Server</h4></div></div></div><p>
					You can configure audit logs for Metrics Server to help you troubleshoot issues with the server. Audit logs record the sequence of actions in a cluster. It can record user, application, or control plane activities.
				</p><p>
					You can set audit log rules, which determine what events are recorded and what data they should include. This can be achieved with the following audit profiles:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<span class="strong strong"><strong>Metadata (default)</strong></span>: This profile enables the logging of event metadata including user, timestamps, resource, and verb. It does not record request and response bodies.
						</li><li class="listitem">
<span class="strong strong"><strong>Request</strong></span>: This enables the logging of event metadata and request body, but it does not record response body. This configuration does not apply for non-resource requests.
						</li><li class="listitem">
<span class="strong strong"><strong>RequestResponse</strong></span>: This enables the logging of event metadata, and request and response bodies. This configuration does not apply for non-resource requests.
						</li><li class="listitem">
<span class="strong strong"><strong>None</strong></span>: None of the previously described events are recorded.
						</li></ul></div><p>
					You can configure the audit profiles by modifying the <code class="literal">cluster-monitoring-config</code> config map. The following example sets the profile to <code class="literal">Request</code>, allowing the logging of event metadata and request body for Metrics Server:
				</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    metricsServer:
      audit:
        profile: Request</pre></section><section class="section" id="setting-log-levels-for-monitoring-components_storing-and-recording-data"><div class="titlepage"><div><div><h4 class="title">3.3.4. Setting log levels for monitoring components</h4></div></div></div><p>
					You can configure the log level for Alertmanager, Prometheus Operator, Prometheus, and Thanos Querier.
				</p><p>
					The following log levels can be applied to the relevant component in the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<code class="literal">debug</code>. Log debug, informational, warning, and error messages.
						</li><li class="listitem">
<code class="literal">info</code>. Log informational, warning, and error messages.
						</li><li class="listitem">
<code class="literal">warn</code>. Log warning and error messages only.
						</li><li class="listitem">
<code class="literal">error</code>. Log error messages only.
						</li></ul></div><p>
					The default log level is <code class="literal">info</code>.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
						</li><li class="listitem">
							You have created the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">cluster-monitoring-config</code> config map in the <code class="literal">openshift-monitoring</code> project:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</pre></li><li class="listitem"><p class="simpara">
							Add <code class="literal">logLevel: &lt;log_level&gt;</code> for a component under <code class="literal">data/config.yaml</code>:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    &lt;component&gt;: <span id="CO11-1"><!--Empty--></span><span class="callout">1</span>
      logLevel: &lt;log_level&gt; <span id="CO11-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO11-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The monitoring stack component for which you are setting a log level. Available component values are <code class="literal">prometheusK8s</code>, <code class="literal">alertmanagerMain</code>, <code class="literal">prometheusOperator</code>, and <code class="literal">thanosQuerier</code>.
								</div></dd><dt><a href="#CO11-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The log level to set for the component. The available values are <code class="literal">error</code>, <code class="literal">warn</code>, <code class="literal">info</code>, and <code class="literal">debug</code>. The default value is <code class="literal">info</code>.
								</div></dd></dl></div></li><li class="listitem">
							Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.
						</li><li class="listitem"><p class="simpara">
							Confirm that the log level has been applied by reviewing the deployment or pod configuration in the related project. The following example checks the log level for the <code class="literal">prometheus-operator</code> deployment:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring get deploy prometheus-operator -o yaml | grep "log-level"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
</p><pre class="programlisting language-terminal">        - --log-level=debug</pre>
<p></p></div></li><li class="listitem"><p class="simpara">
							Check that the pods for the component are running. The following example lists the status of pods:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring get pods</pre><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
								If an unrecognized <code class="literal">logLevel</code> value is included in the <code class="literal">ConfigMap</code> object, the pods for the component might not restart successfully.
							</p></div></rh-alert></li></ol></div></section><section class="section" id="setting-query-log-file-for-prometheus_storing-and-recording-data"><div class="titlepage"><div><div><h4 class="title">3.3.5. Enabling the query log file for Prometheus</h4></div></div></div><p>
					You can configure Prometheus to write all queries that have been run by the engine to a log file.
				</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
						Because log rotation is not supported, only enable this feature temporarily when you need to troubleshoot an issue. After you finish troubleshooting, disable query logging by reverting the changes you made to the <code class="literal">ConfigMap</code> object to enable the feature.
					</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
						</li><li class="listitem">
							You have created the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">cluster-monitoring-config</code> config map in the <code class="literal">openshift-monitoring</code> project:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</pre></li><li class="listitem"><p class="simpara">
							Add the <code class="literal">queryLogFile</code> parameter for Prometheus under <code class="literal">data/config.yaml</code>:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      queryLogFile: &lt;path&gt; <span id="CO12-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO12-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Add the full path to the file in which queries will be logged.
								</div></dd></dl></div></li><li class="listitem">
							Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.
						</li><li class="listitem"><p class="simpara">
							Verify that the pods for the component are running. The following sample command lists the status of pods:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring get pods</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
</p><pre class="programlisting language-terminal">...
prometheus-operator-567c9bc75c-96wkj   2/2     Running   0          62m
prometheus-k8s-0                       6/6     Running   1          57m
prometheus-k8s-1                       6/6     Running   1          57m
thanos-querier-56c76d7df4-2xkpc        6/6     Running   0          57m
thanos-querier-56c76d7df4-j5p29        6/6     Running   0          57m
...</pre>
<p></p></div></li><li class="listitem"><p class="simpara">
							Read the query log:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring exec prometheus-k8s-0 -- cat &lt;path&gt;</pre><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
								Revert the setting in the config map after you have examined the logged query information.
							</p></div></rh-alert></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#preparing-to-configure-the-monitoring-stack" title="3.1. Preparing to configure core platform monitoring stack">Preparing to configure core platform monitoring stack</a>
</li></ul></div></section><section class="section" id="enabling-query-logging-for-thanos-querier_storing-and-recording-data"><div class="titlepage"><div><div><h4 class="title">3.3.6. Enabling query logging for Thanos Querier</h4></div></div></div><p>
					For default platform monitoring in the <code class="literal">openshift-monitoring</code> project, you can enable the Cluster Monitoring Operator (CMO) to log all queries run by Thanos Querier.
				</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
						Because log rotation is not supported, only enable this feature temporarily when you need to troubleshoot an issue. After you finish troubleshooting, disable query logging by reverting the changes you made to the <code class="literal">ConfigMap</code> object to enable the feature.
					</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
						</li><li class="listitem">
							You have created the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object.
						</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						You can enable query logging for Thanos Querier in the <code class="literal">openshift-monitoring</code> project:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object in the <code class="literal">openshift-monitoring</code> project:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</pre></li><li class="listitem"><p class="simpara">
							Add a <code class="literal">thanosQuerier</code> section under <code class="literal">data/config.yaml</code> and add values as shown in the following example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    thanosQuerier:
      enableRequestLogging: &lt;value&gt; <span id="CO13-1"><!--Empty--></span><span class="callout">1</span>
      logLevel: &lt;value&gt; <span id="CO13-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO13-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Set the value to <code class="literal">true</code> to enable logging and <code class="literal">false</code> to disable logging. The default value is <code class="literal">false</code>.
								</div></dd><dt><a href="#CO13-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Set the value to <code class="literal">debug</code>, <code class="literal">info</code>, <code class="literal">warn</code>, or <code class="literal">error</code>. If no value exists for <code class="literal">logLevel</code>, the log level defaults to <code class="literal">error</code>.
								</div></dd></dl></div></li><li class="listitem">
							Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.
						</li></ol></div><div class="orderedlist"><p class="title"><strong>Verification</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Verify that the Thanos Querier pods are running. The following sample command lists the status of pods in the <code class="literal">openshift-monitoring</code> project:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring get pods</pre></li><li class="listitem"><p class="simpara">
							Run a test query using the following sample commands as a model:
						</p><pre class="programlisting language-terminal">$ token=`oc create token prometheus-k8s -n openshift-monitoring`
$ oc -n openshift-monitoring exec -c prometheus prometheus-k8s-0 -- curl -k -H "Authorization: Bearer $token" 'https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query?query=cluster_version'</pre></li><li class="listitem"><p class="simpara">
							Run the following command to read the query log:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring logs &lt;thanos_querier_pod_name&gt; -c thanos-query</pre><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
								Because the <code class="literal">thanos-querier</code> pods are highly available (HA) pods, you might be able to see logs in only one pod.
							</p></div></rh-alert></li><li class="listitem">
							After you examine the logged query information, disable query logging by changing the <code class="literal">enableRequestLogging</code> value to <code class="literal">false</code> in the config map.
						</li></ol></div></section></section><section class="section" id="configuring-metrics"><div class="titlepage"><div><div><h3 class="title">3.4. Configuring metrics for core platform monitoring</h3></div></div></div><p>
				Configure the collection of metrics to monitor how cluster components and your own workloads are performing.
			</p><p>
				You can send ingested metrics to remote systems for long-term storage and add cluster ID labels to the metrics to identify the data coming from different clusters.
			</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#understanding-metrics_key-concepts" title="1.3.3. Understanding metrics">Understanding metrics</a>
</li></ul></div><section class="section" id="configuring-remote-write-storage_configuring-metrics"><div class="titlepage"><div><div><h4 class="title">3.4.1. Configuring remote write storage</h4></div></div></div><p>
					You can configure remote write storage to enable Prometheus to send ingested metrics to remote systems for long-term storage. Doing so has no impact on how or for how long Prometheus stores metrics.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
						</li><li class="listitem">
							You have created the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem"><p class="simpara">
							You have set up a remote write compatible endpoint (such as Thanos) and know the endpoint URL. See the <a class="link" href="https://prometheus.io/docs/operating/integrations/#remote-endpoints-and-storage">Prometheus remote endpoints and storage documentation</a> for information about endpoints that are compatible with the remote write feature.
						</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
								Red Hat only provides information for configuring remote write senders and does not offer guidance on configuring receiver endpoints. Customers are responsible for setting up their own endpoints that are remote-write compatible. Issues with endpoint receiver configurations are not included in Red Hat production support.
							</p></div></rh-alert></li><li class="listitem"><p class="simpara">
							You have set up authentication credentials in a <code class="literal">Secret</code> object for the remote write endpoint. You must create the secret in the <code class="literal">openshift-monitoring</code> namespace.
						</p><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
								To reduce security risks, use HTTPS and authentication to send metrics to an endpoint.
							</p></div></rh-alert></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">cluster-monitoring-config</code> config map in the <code class="literal">openshift-monitoring</code> project:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</pre></li><li class="listitem"><p class="simpara">
							Add a <code class="literal">remoteWrite:</code> section under <code class="literal">data/config.yaml/prometheusK8s</code>, as shown in the following example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com" <span id="CO14-1"><!--Empty--></span><span class="callout">1</span>
        &lt;endpoint_authentication_credentials&gt; <span id="CO14-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO14-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The URL of the remote write endpoint.
								</div></dd><dt><a href="#CO14-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The authentication method and credentials for the endpoint. Currently supported authentication methods are AWS Signature Version 4, authentication using HTTP in an <code class="literal">Authorization</code> request header, Basic authentication, OAuth 2.0, and TLS client. See <span class="emphasis"><em>Supported remote write authentication settings</em></span> for sample configurations of supported authentication methods.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Add write relabel configuration values after the authentication credentials:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        &lt;endpoint_authentication_credentials&gt;
        writeRelabelConfigs:
        - &lt;your_write_relabel_configs&gt; <span id="CO15-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO15-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Add configuration for metrics that you want to send to the remote endpoint.
								</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example of forwarding a single metric called <code class="literal">my_metric</code></strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        writeRelabelConfigs:
        - sourceLabels: [__name__]
          regex: 'my_metric'
          action: keep</pre>
<p></p></div><div class="formalpara"><p class="title"><strong>Example of forwarding metrics called <code class="literal">my_metric_1</code> and <code class="literal">my_metric_2</code> in <code class="literal">my_namespace</code> namespace</strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        writeRelabelConfigs:
        - sourceLabels: [__name__,namespace]
          regex: '(my_metric_1|my_metric_2);my_namespace'
          action: keep</pre>
<p></p></div></li><li class="listitem">
							Save the file to apply the changes. The new configuration is applied automatically.
						</li></ol></div><section class="section" id="supported-remote-write-authentication-settings_configuring-metrics"><div class="titlepage"><div><div><h5 class="title">3.4.1.1. Supported remote write authentication settings</h5></div></div></div><p>
						You can use different methods to authenticate with a remote write endpoint. Currently supported authentication methods are AWS Signature Version 4, basic authentication, authorization, OAuth 2.0, and TLS client. The following table provides details about supported authentication methods for use with remote write.
					</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059307580624" scope="col" valign="top">Authentication method</th><th align="left" id="idm140059307579536" scope="col" valign="top">Config map field</th><th align="left" id="idm140059297939568" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059307580624" valign="top"> <p>
										AWS Signature Version 4
									</p>
</td><td align="left" headers="idm140059307579536" valign="top"> <p>
<code class="literal">sigv4</code>
</p>
</td><td align="left" headers="idm140059297939568" valign="top"> <p>
										This method uses AWS Signature Version 4 authentication to sign requests. You cannot use this method simultaneously with authorization, OAuth 2.0, or Basic authentication.
									</p>
</td></tr><tr><td align="left" headers="idm140059307580624" valign="top"> <p>
										Basic authentication
									</p>
</td><td align="left" headers="idm140059307579536" valign="top"> <p>
<code class="literal">basicAuth</code>
</p>
</td><td align="left" headers="idm140059297939568" valign="top"> <p>
										Basic authentication sets the authorization header on every remote write request with the configured username and password.
									</p>
</td></tr><tr><td align="left" headers="idm140059307580624" valign="top"> <p>
										authorization
									</p>
</td><td align="left" headers="idm140059307579536" valign="top"> <p>
<code class="literal">authorization</code>
</p>
</td><td align="left" headers="idm140059297939568" valign="top"> <p>
										Authorization sets the <code class="literal">Authorization</code> header on every remote write request using the configured token.
									</p>
</td></tr><tr><td align="left" headers="idm140059307580624" valign="top"> <p>
										OAuth 2.0
									</p>
</td><td align="left" headers="idm140059307579536" valign="top"> <p>
<code class="literal">oauth2</code>
</p>
</td><td align="left" headers="idm140059297939568" valign="top"> <p>
										An OAuth 2.0 configuration uses the client credentials grant type. Prometheus fetches an access token from <code class="literal">tokenUrl</code> with the specified client ID and client secret to access the remote write endpoint. You cannot use this method simultaneously with authorization, AWS Signature Version 4, or Basic authentication.
									</p>
</td></tr><tr><td align="left" headers="idm140059307580624" valign="top"> <p>
										TLS client
									</p>
</td><td align="left" headers="idm140059307579536" valign="top"> <p>
<code class="literal">tlsConfig</code>
</p>
</td><td align="left" headers="idm140059297939568" valign="top"> <p>
										A TLS client configuration specifies the CA certificate, the client certificate, and the client key file information used to authenticate with the remote write endpoint server using TLS. The sample configuration assumes that you have already created a CA certificate file, a client certificate file, and a client key file.
									</p>
</td></tr></tbody></table></rh-table></section><section class="section" id="example-remote-write-authentication-settings_configuring-metrics"><div class="titlepage"><div><div><h5 class="title">3.4.1.2. Example remote write authentication settings</h5></div></div></div><p>
						The following samples show different authentication settings you can use to connect to a remote write endpoint. Each sample also shows how to configure a corresponding <code class="literal">Secret</code> object that contains authentication credentials and other relevant settings. Each sample configures authentication for use with default platform monitoring in the <code class="literal">openshift-monitoring</code> namespace.
					</p><section class="section" id="remote-write-sample-yaml-aws-sigv4_configuring-metrics"><div class="titlepage"><div><div><h5 class="title">3.4.1.2.1. Sample YAML for AWS Signature Version 4 authentication</h5></div></div></div><p>
							The following shows the settings for a <code class="literal">sigv4</code> secret named <code class="literal">sigv4-credentials</code> in the <code class="literal">openshift-monitoring</code> namespace.
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: sigv4-credentials
  namespace: openshift-monitoring
stringData:
  accessKey: &lt;AWS_access_key&gt; <span id="CO16-1"><!--Empty--></span><span class="callout">1</span>
  secretKey: &lt;AWS_secret_key&gt; <span id="CO16-2"><!--Empty--></span><span class="callout">2</span>
type: Opaque</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO16-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The AWS API access key.
								</div></dd><dt><a href="#CO16-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The AWS API secret key.
								</div></dd></dl></div><p>
							The following shows sample AWS Signature Version 4 remote write authentication settings that use a <code class="literal">Secret</code> object named <code class="literal">sigv4-credentials</code> in the <code class="literal">openshift-monitoring</code> namespace:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://authorization.example.com/api/write"
        sigv4:
          region: &lt;AWS_region&gt; <span id="CO17-1"><!--Empty--></span><span class="callout">1</span>
          accessKey:
            name: sigv4-credentials <span id="CO17-2"><!--Empty--></span><span class="callout">2</span>
            key: accessKey <span id="CO17-3"><!--Empty--></span><span class="callout">3</span>
          secretKey:
            name: sigv4-credentials <span id="CO17-4"><!--Empty--></span><span class="callout">4</span>
            key: secretKey <span id="CO17-5"><!--Empty--></span><span class="callout">5</span>
          profile: &lt;AWS_profile_name&gt; <span id="CO17-6"><!--Empty--></span><span class="callout">6</span>
          roleArn: &lt;AWS_role_arn&gt; <span id="CO17-7"><!--Empty--></span><span class="callout">7</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO17-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The AWS region.
								</div></dd><dt><a href="#CO17-2"><span class="callout">2</span></a> <a href="#CO17-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									The name of the <code class="literal">Secret</code> object containing the AWS API access credentials.
								</div></dd><dt><a href="#CO17-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The key that contains the AWS API access key in the specified <code class="literal">Secret</code> object.
								</div></dd><dt><a href="#CO17-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									The key that contains the AWS API secret key in the specified <code class="literal">Secret</code> object.
								</div></dd><dt><a href="#CO17-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									The name of the AWS profile that is being used to authenticate.
								</div></dd><dt><a href="#CO17-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									The unique identifier for the Amazon Resource Name (ARN) assigned to your role.
								</div></dd></dl></div></section><section class="section" id="remote-write-sample-yaml-basic-auth_configuring-metrics"><div class="titlepage"><div><div><h5 class="title">3.4.1.2.2. Sample YAML for Basic authentication</h5></div></div></div><p>
							The following shows sample Basic authentication settings for a <code class="literal">Secret</code> object named <code class="literal">rw-basic-auth</code> in the <code class="literal">openshift-monitoring</code> namespace:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: rw-basic-auth
  namespace: openshift-monitoring
stringData:
  user: &lt;basic_username&gt; <span id="CO18-1"><!--Empty--></span><span class="callout">1</span>
  password: &lt;basic_password&gt; <span id="CO18-2"><!--Empty--></span><span class="callout">2</span>
type: Opaque</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO18-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The username.
								</div></dd><dt><a href="#CO18-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The password.
								</div></dd></dl></div><p>
							The following sample shows a <code class="literal">basicAuth</code> remote write configuration that uses a <code class="literal">Secret</code> object named <code class="literal">rw-basic-auth</code> in the <code class="literal">openshift-monitoring</code> namespace. It assumes that you have already set up authentication credentials for the endpoint.
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://basicauth.example.com/api/write"
        basicAuth:
          username:
            name: rw-basic-auth <span id="CO19-1"><!--Empty--></span><span class="callout">1</span>
            key: user <span id="CO19-2"><!--Empty--></span><span class="callout">2</span>
          password:
            name: rw-basic-auth <span id="CO19-3"><!--Empty--></span><span class="callout">3</span>
            key: password <span id="CO19-4"><!--Empty--></span><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO19-1"><span class="callout">1</span></a> <a href="#CO19-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The name of the <code class="literal">Secret</code> object that contains the authentication credentials.
								</div></dd><dt><a href="#CO19-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The key that contains the username in the specified <code class="literal">Secret</code> object.
								</div></dd><dt><a href="#CO19-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									The key that contains the password in the specified <code class="literal">Secret</code> object.
								</div></dd></dl></div></section><section class="section" id="remote-write-sample-yaml-bearer-token_configuring-metrics"><div class="titlepage"><div><div><h5 class="title">3.4.1.2.3. Sample YAML for authentication with a bearer token using a <code class="literal">Secret</code> Object</h5></div></div></div><p>
							The following shows bearer token settings for a <code class="literal">Secret</code> object named <code class="literal">rw-bearer-auth</code> in the <code class="literal">openshift-monitoring</code> namespace:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: rw-bearer-auth
  namespace: openshift-monitoring
stringData:
  token: &lt;authentication_token&gt; <span id="CO20-1"><!--Empty--></span><span class="callout">1</span>
type: Opaque</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO20-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The authentication token.
								</div></dd></dl></div><p>
							The following shows sample bearer token config map settings that use a <code class="literal">Secret</code> object named <code class="literal">rw-bearer-auth</code> in the <code class="literal">openshift-monitoring</code> namespace:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    enableUserWorkload: true
    prometheusK8s:
      remoteWrite:
      - url: "https://authorization.example.com/api/write"
        authorization:
          type: Bearer <span id="CO21-1"><!--Empty--></span><span class="callout">1</span>
          credentials:
            name: rw-bearer-auth <span id="CO21-2"><!--Empty--></span><span class="callout">2</span>
            key: token <span id="CO21-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO21-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The authentication type of the request. The default value is <code class="literal">Bearer</code>.
								</div></dd><dt><a href="#CO21-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The name of the <code class="literal">Secret</code> object that contains the authentication credentials.
								</div></dd><dt><a href="#CO21-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The key that contains the authentication token in the specified <code class="literal">Secret</code> object.
								</div></dd></dl></div></section><section class="section" id="remote-write-sample-yaml-oauth-20_configuring-metrics"><div class="titlepage"><div><div><h5 class="title">3.4.1.2.4. Sample YAML for OAuth 2.0 authentication</h5></div></div></div><p>
							The following shows sample OAuth 2.0 settings for a <code class="literal">Secret</code> object named <code class="literal">oauth2-credentials</code> in the <code class="literal">openshift-monitoring</code> namespace:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: oauth2-credentials
  namespace: openshift-monitoring
stringData:
  id: &lt;oauth2_id&gt; <span id="CO22-1"><!--Empty--></span><span class="callout">1</span>
  secret: &lt;oauth2_secret&gt; <span id="CO22-2"><!--Empty--></span><span class="callout">2</span>
type: Opaque</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO22-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The Oauth 2.0 ID.
								</div></dd><dt><a href="#CO22-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The OAuth 2.0 secret.
								</div></dd></dl></div><p>
							The following shows an <code class="literal">oauth2</code> remote write authentication sample configuration that uses a <code class="literal">Secret</code> object named <code class="literal">oauth2-credentials</code> in the <code class="literal">openshift-monitoring</code> namespace:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://test.example.com/api/write"
        oauth2:
          clientId:
            secret:
              name: oauth2-credentials <span id="CO23-1"><!--Empty--></span><span class="callout">1</span>
              key: id <span id="CO23-2"><!--Empty--></span><span class="callout">2</span>
          clientSecret:
            name: oauth2-credentials <span id="CO23-3"><!--Empty--></span><span class="callout">3</span>
            key: secret <span id="CO23-4"><!--Empty--></span><span class="callout">4</span>
          tokenUrl: https://example.com/oauth2/token <span id="CO23-5"><!--Empty--></span><span class="callout">5</span>
          scopes: <span id="CO23-6"><!--Empty--></span><span class="callout">6</span>
          - &lt;scope_1&gt;
          - &lt;scope_2&gt;
          endpointParams: <span id="CO23-7"><!--Empty--></span><span class="callout">7</span>
            param1: &lt;parameter_1&gt;
            param2: &lt;parameter_2&gt;</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO23-1"><span class="callout">1</span></a> <a href="#CO23-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The name of the corresponding <code class="literal">Secret</code> object. Note that <code class="literal">ClientId</code> can alternatively refer to a <code class="literal">ConfigMap</code> object, although <code class="literal">clientSecret</code> must refer to a <code class="literal">Secret</code> object.
								</div></dd><dt><a href="#CO23-2"><span class="callout">2</span></a> <a href="#CO23-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									The key that contains the OAuth 2.0 credentials in the specified <code class="literal">Secret</code> object.
								</div></dd><dt><a href="#CO23-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									The URL used to fetch a token with the specified <code class="literal">clientId</code> and <code class="literal">clientSecret</code>.
								</div></dd><dt><a href="#CO23-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									The OAuth 2.0 scopes for the authorization request. These scopes limit what data the tokens can access.
								</div></dd><dt><a href="#CO23-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									The OAuth 2.0 authorization request parameters required for the authorization server.
								</div></dd></dl></div></section><section class="section" id="remote-write-sample-yaml-tls_configuring-metrics"><div class="titlepage"><div><div><h5 class="title">3.4.1.2.5. Sample YAML for TLS client authentication</h5></div></div></div><p>
							The following shows sample TLS client settings for a <code class="literal">tls</code> <code class="literal">Secret</code> object named <code class="literal">mtls-bundle</code> in the <code class="literal">openshift-monitoring</code> namespace.
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: mtls-bundle
  namespace: openshift-monitoring
data:
  ca.crt: &lt;ca_cert&gt; <span id="CO24-1"><!--Empty--></span><span class="callout">1</span>
  client.crt: &lt;client_cert&gt; <span id="CO24-2"><!--Empty--></span><span class="callout">2</span>
  client.key: &lt;client_key&gt; <span id="CO24-3"><!--Empty--></span><span class="callout">3</span>
type: tls</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO24-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The CA certificate in the Prometheus container with which to validate the server certificate.
								</div></dd><dt><a href="#CO24-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The client certificate for authentication with the server.
								</div></dd><dt><a href="#CO24-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The client key.
								</div></dd></dl></div><p>
							The following sample shows a <code class="literal">tlsConfig</code> remote write authentication configuration that uses a TLS <code class="literal">Secret</code> object named <code class="literal">mtls-bundle</code>.
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        tlsConfig:
          ca:
            secret:
              name: mtls-bundle <span id="CO25-1"><!--Empty--></span><span class="callout">1</span>
              key: ca.crt <span id="CO25-2"><!--Empty--></span><span class="callout">2</span>
          cert:
            secret:
              name: mtls-bundle <span id="CO25-3"><!--Empty--></span><span class="callout">3</span>
              key: client.crt <span id="CO25-4"><!--Empty--></span><span class="callout">4</span>
          keySecret:
            name: mtls-bundle <span id="CO25-5"><!--Empty--></span><span class="callout">5</span>
            key: client.key <span id="CO25-6"><!--Empty--></span><span class="callout">6</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO25-1"><span class="callout">1</span></a> <a href="#CO25-3"><span class="callout">3</span></a> <a href="#CO25-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									The name of the corresponding <code class="literal">Secret</code> object that contains the TLS authentication credentials. Note that <code class="literal">ca</code> and <code class="literal">cert</code> can alternatively refer to a <code class="literal">ConfigMap</code> object, though <code class="literal">keySecret</code> must refer to a <code class="literal">Secret</code> object.
								</div></dd><dt><a href="#CO25-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The key in the specified <code class="literal">Secret</code> object that contains the CA certificate for the endpoint.
								</div></dd><dt><a href="#CO25-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									The key in the specified <code class="literal">Secret</code> object that contains the client certificate for the endpoint.
								</div></dd><dt><a href="#CO25-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									The key in the specified <code class="literal">Secret</code> object that contains the client key secret.
								</div></dd></dl></div></section></section><section class="section" id="example-remote-write-queue-configuration_configuring-metrics"><div class="titlepage"><div><div><h5 class="title">3.4.1.3. Example remote write queue configuration</h5></div></div></div><p>
						You can use the <code class="literal">queueConfig</code> object for remote write to tune the remote write queue parameters. The following example shows the queue parameters with their default values for default platform monitoring in the <code class="literal">openshift-monitoring</code> namespace.
					</p><div class="formalpara"><p class="title"><strong>Example configuration of remote write parameters with default values</strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        &lt;endpoint_authentication_credentials&gt;
        queueConfig:
          capacity: 10000 <span id="CO26-1"><!--Empty--></span><span class="callout">1</span>
          minShards: 1 <span id="CO26-2"><!--Empty--></span><span class="callout">2</span>
          maxShards: 50 <span id="CO26-3"><!--Empty--></span><span class="callout">3</span>
          maxSamplesPerSend: 2000 <span id="CO26-4"><!--Empty--></span><span class="callout">4</span>
          batchSendDeadline: 5s <span id="CO26-5"><!--Empty--></span><span class="callout">5</span>
          minBackoff: 30ms <span id="CO26-6"><!--Empty--></span><span class="callout">6</span>
          maxBackoff: 5s <span id="CO26-7"><!--Empty--></span><span class="callout">7</span>
          retryOnRateLimit: false <span id="CO26-8"><!--Empty--></span><span class="callout">8</span>
          sampleAgeLimit: 0s <span id="CO26-9"><!--Empty--></span><span class="callout">9</span></pre>
<p></p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO26-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The number of samples to buffer per shard before they are dropped from the queue.
							</div></dd><dt><a href="#CO26-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The minimum number of shards.
							</div></dd><dt><a href="#CO26-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								The maximum number of shards.
							</div></dd><dt><a href="#CO26-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								The maximum number of samples per send.
							</div></dd><dt><a href="#CO26-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								The maximum time for a sample to wait in buffer.
							</div></dd><dt><a href="#CO26-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								The initial time to wait before retrying a failed request. The time gets doubled for every retry up to the <code class="literal">maxbackoff</code> time.
							</div></dd><dt><a href="#CO26-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								The maximum time to wait before retrying a failed request.
							</div></dd><dt><a href="#CO26-8"><span class="callout">8</span></a> </dt><dd><div class="para">
								Set this parameter to <code class="literal">true</code> to retry a request after receiving a 429 status code from the remote write storage.
							</div></dd><dt><a href="#CO26-9"><span class="callout">9</span></a> </dt><dd><div class="para">
								The samples that are older than the <code class="literal">sampleAgeLimit</code> limit are dropped from the queue. If the value is undefined or set to <code class="literal">0s</code>, the parameter is ignored.
							</div></dd></dl></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/monitoring_apis/#spec-remotewrite-2">Prometheus REST API reference for remote write</a>
</li><li class="listitem">
<a class="link" href="https://prometheus.io/docs/operating/integrations/#remote-endpoints-and-storage">Setting up remote write compatible endpoints</a> (Prometheus documentation)
							</li><li class="listitem">
<a class="link" href="https://prometheus.io/docs/practices/remote_write/#remote-write-tuning">Tuning remote write settings</a> (Prometheus documentation)
							</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/nodes/#nodes-pods-secrets-about_nodes-pods-secrets">Understanding secrets</a>
</li></ul></div></section></section><section class="section" id="creating-cluster-id-labels-for-metrics_configuring-metrics"><div class="titlepage"><div><div><h4 class="title">3.4.2. Creating cluster ID labels for metrics</h4></div></div></div><p>
					You can create cluster ID labels for metrics by adding the <code class="literal">write_relabel</code> settings for remote write storage in the <code class="literal">cluster-monitoring-config</code> config map in the <code class="literal">openshift-monitoring</code> namespace.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
						</li><li class="listitem">
							You have created the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have configured remote write storage.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">cluster-monitoring-config</code> config map in the <code class="literal">openshift-monitoring</code> project:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</pre></li><li class="listitem"><p class="simpara">
							In the <code class="literal">writeRelabelConfigs:</code> section under <code class="literal">data/config.yaml/prometheusK8s/remoteWrite</code>, add cluster ID relabel configuration values:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        &lt;endpoint_authentication_credentials&gt;
        writeRelabelConfigs: <span id="CO27-1"><!--Empty--></span><span class="callout">1</span>
          - &lt;relabel_config&gt; <span id="CO27-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO27-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Add a list of write relabel configurations for metrics that you want to send to the remote endpoint.
								</div></dd><dt><a href="#CO27-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Substitute the label configuration for the metrics sent to the remote write endpoint.
								</div></dd></dl></div><p class="simpara">
							The following sample shows how to forward a metric with the cluster ID label <code class="literal">cluster_id</code>:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        writeRelabelConfigs:
        - sourceLabels:
          - __tmp_openshift_cluster_id__ <span id="CO28-1"><!--Empty--></span><span class="callout">1</span>
          targetLabel: cluster_id <span id="CO28-2"><!--Empty--></span><span class="callout">2</span>
          action: replace <span id="CO28-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO28-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The system initially applies a temporary cluster ID source label named <code class="literal">__tmp_openshift_cluster_id__</code>. This temporary label gets replaced by the cluster ID label name that you specify.
								</div></dd><dt><a href="#CO28-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the name of the cluster ID label for metrics sent to remote write storage. If you use a label name that already exists for a metric, that value is overwritten with the name of this cluster ID label. For the label name, do not use <code class="literal">__tmp_openshift_cluster_id__</code>. The final relabeling step removes labels that use this name.
								</div></dd><dt><a href="#CO28-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The <code class="literal">replace</code> write relabel action replaces the temporary label with the target label for outgoing metrics. This action is the default and is applied if no action is specified.
								</div></dd></dl></div></li><li class="listitem">
							Save the file to apply the changes. The new configuration is applied automatically.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#adding-cluster-id-labels-to-metrics_key-concepts" title="1.3.3.2. Adding cluster ID labels to metrics">Adding cluster ID labels to metrics</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/support/#support-get-cluster-id_gathering-cluster-data">Obtaining your cluster ID</a>
</li></ul></div></section></section><section class="section" id="configuring-alerts-and-notifications"><div class="titlepage"><div><div><h3 class="title">3.5. Configuring alerts and notifications for core platform monitoring</h3></div></div></div><p>
				You can configure a local or external Alertmanager instance to route alerts from Prometheus to endpoint receivers. You can also attach custom labels to all time series and alerts to add useful metadata information.
			</p><section class="section" id="monitoring-configuring-external-alertmanagers_configuring-alerts-and-notifications"><div class="titlepage"><div><div><h4 class="title">3.5.1. Configuring external Alertmanager instances</h4></div></div></div><p>
					The OpenShift Container Platform monitoring stack includes a local Alertmanager instance that routes alerts from Prometheus.
				</p><p>
					You can add external Alertmanager instances to route alerts for core OpenShift Container Platform projects.
				</p><p>
					If you add the same external Alertmanager configuration for multiple clusters and disable the local instance for each cluster, you can then manage alert routing for multiple clusters by using a single external Alertmanager instance.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
						</li><li class="listitem">
							You have created the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">cluster-monitoring-config</code> config map in the <code class="literal">openshift-monitoring</code> project:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</pre></li><li class="listitem"><p class="simpara">
							Add an <code class="literal">additionalAlertmanagerConfigs</code> section with configuration details under <code class="literal">data/config.yaml/prometheusK8s</code>:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      additionalAlertmanagerConfigs:
      - &lt;alertmanager_specification&gt; <span id="CO29-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO29-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Substitute <code class="literal">&lt;alertmanager_specification&gt;</code> with authentication and other configuration details for additional Alertmanager instances. Currently supported authentication methods are bearer token (<code class="literal">bearerToken</code>) and client TLS (<code class="literal">tlsConfig</code>).
								</div></dd></dl></div><p class="simpara">
							The following sample config map configures an additional Alertmanager for Prometheus by using a bearer token with client TLS authentication:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      additionalAlertmanagerConfigs:
      - scheme: https
        pathPrefix: /
        timeout: "30s"
        apiVersion: v1
        bearerToken:
          name: alertmanager-bearer-token
          key: token
        tlsConfig:
          key:
            name: alertmanager-tls
            key: tls.key
          cert:
            name: alertmanager-tls
            key: tls.crt
          ca:
            name: alertmanager-tls
            key: tls.ca
        staticConfigs:
        - external-alertmanager1-remote.com
        - external-alertmanager1-remote2.com</pre></li><li class="listitem">
							Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.
						</li></ol></div><section class="section" id="monitoring-disabling-the-local-alertmanager_configuring-alerts-and-notifications"><div class="titlepage"><div><div><h5 class="title">3.5.1.1. Disabling the local Alertmanager</h5></div></div></div><p>
						A local Alertmanager that routes alerts from Prometheus instances is enabled by default in the <code class="literal">openshift-monitoring</code> project of the OpenShift Container Platform monitoring stack.
					</p><p>
						If you do not need the local Alertmanager, you can disable it by configuring the <code class="literal">cluster-monitoring-config</code> config map in the <code class="literal">openshift-monitoring</code> project.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
							</li><li class="listitem">
								You have created the <code class="literal">cluster-monitoring-config</code> config map.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Edit the <code class="literal">cluster-monitoring-config</code> config map in the <code class="literal">openshift-monitoring</code> project:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</pre></li><li class="listitem"><p class="simpara">
								Add <code class="literal">enabled: false</code> for the <code class="literal">alertmanagerMain</code> component under <code class="literal">data/config.yaml</code>:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    alertmanagerMain:
      enabled: false</pre></li><li class="listitem">
								Save the file to apply the changes. The Alertmanager instance is disabled automatically when you apply the change.
							</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="https://prometheus.io/docs/alerting/latest/alertmanager/">Alertmanager</a> (Prometheus documentation)
							</li><li class="listitem">
<a class="link" href="#managing-alerts-as-an-administrator" title="6.1. Managing alerts as an Administrator">Managing alerts as an Administrator</a>
</li></ul></div></section></section><section class="section" id="monitoring-configuring-secrets-for-alertmanager_configuring-alerts-and-notifications"><div class="titlepage"><div><div><h4 class="title">3.5.2. Configuring secrets for Alertmanager</h4></div></div></div><p>
					The OpenShift Container Platform monitoring stack includes Alertmanager, which routes alerts from Prometheus to endpoint receivers. If you need to authenticate with a receiver so that Alertmanager can send alerts to it, you can configure Alertmanager to use a secret that contains authentication credentials for the receiver.
				</p><p>
					For example, you can configure Alertmanager to use a secret to authenticate with an endpoint receiver that requires a certificate issued by a private Certificate Authority (CA). You can also configure Alertmanager to use a secret to authenticate with a receiver that requires a password file for Basic HTTP authentication. In either case, authentication details are contained in the <code class="literal">Secret</code> object rather than in the <code class="literal">ConfigMap</code> object.
				</p><section class="section" id="monitoring-adding-a-secret-to-the-alertmanager-configuration_configuring-alerts-and-notifications"><div class="titlepage"><div><div><h5 class="title">3.5.2.1. Adding a secret to the Alertmanager configuration</h5></div></div></div><p>
						You can add secrets to the Alertmanager configuration by editing the <code class="literal">cluster-monitoring-config</code> config map in the <code class="literal">openshift-monitoring</code> project.
					</p><p>
						After you add a secret to the config map, the secret is mounted as a volume at <code class="literal">/etc/alertmanager/secrets/&lt;secret_name&gt;</code> within the <code class="literal">alertmanager</code> container for the Alertmanager pods.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
							</li><li class="listitem">
								You have created the <code class="literal">cluster-monitoring-config</code> config map.
							</li><li class="listitem">
								You have created the secret to be configured in Alertmanager in the <code class="literal">openshift-monitoring</code> project.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Edit the <code class="literal">cluster-monitoring-config</code> config map in the <code class="literal">openshift-monitoring</code> project:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</pre></li><li class="listitem"><p class="simpara">
								Add a <code class="literal">secrets:</code> section under <code class="literal">data/config.yaml/alertmanagerMain</code> with the following configuration:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    alertmanagerMain:
      secrets: <span id="CO30-1"><!--Empty--></span><span class="callout">1</span>
      - &lt;secret_name_1&gt; <span id="CO30-2"><!--Empty--></span><span class="callout">2</span>
      - &lt;secret_name_2&gt;</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO30-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										This section contains the secrets to be mounted into Alertmanager. The secrets must be located within the same namespace as the Alertmanager object.
									</div></dd><dt><a href="#CO30-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The name of the <code class="literal">Secret</code> object that contains authentication credentials for the receiver. If you add multiple secrets, place each one on a new line.
									</div></dd></dl></div><p class="simpara">
								The following sample config map settings configure Alertmanager to use two <code class="literal">Secret</code> objects named <code class="literal">test-secret-basic-auth</code> and <code class="literal">test-secret-api-token</code>:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    alertmanagerMain:
      secrets:
      - test-secret-basic-auth
      - test-secret-api-token</pre></li><li class="listitem">
								Save the file to apply the changes. The new configuration is applied automatically.
							</li></ol></div></section></section><section class="section" id="attaching-additional-labels-to-your-time-series-and-alerts_configuring-alerts-and-notifications"><div class="titlepage"><div><div><h4 class="title">3.5.3. Attaching additional labels to your time series and alerts</h4></div></div></div><p>
					You can attach custom labels to all time series and alerts leaving Prometheus by using the external labels feature of Prometheus.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
						</li><li class="listitem">
							You have created the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">cluster-monitoring-config</code> config map in the <code class="literal">openshift-monitoring</code> project:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</pre></li><li class="listitem"><p class="simpara">
							Define labels you want to add for every metric under <code class="literal">data/config.yaml</code>:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      externalLabels:
        &lt;key&gt;: &lt;value&gt; <span id="CO31-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO31-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Substitute <code class="literal">&lt;key&gt;: &lt;value&gt;</code> with key-value pairs where <code class="literal">&lt;key&gt;</code> is a unique name for the new label and <code class="literal">&lt;value&gt;</code> is its value.
								</div></dd></dl></div><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Do not use <code class="literal">prometheus</code> or <code class="literal">prometheus_replica</code> as key names, because they are reserved and will be overwritten.
									</li><li class="listitem">
										Do not use <code class="literal">cluster</code> or <code class="literal">managed_cluster</code> as key names. Using them can cause issues where you are unable to see data in the developer dashboards.
									</li></ul></div></div></rh-alert><p class="simpara">
							For example, to add metadata about the region and environment to all time series and alerts, use the following example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      externalLabels:
        region: eu
        environment: prod</pre></li><li class="listitem">
							Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#preparing-to-configure-the-monitoring-stack" title="3.1. Preparing to configure core platform monitoring stack">Preparing to configure core platform monitoring stack</a>
</li></ul></div></section><section class="section" id="configuring-alert-notifications_configuring-alerts-and-notifications"><div class="titlepage"><div><div><h4 class="title">3.5.4. Configuring alert notifications</h4></div></div></div><p>
					In OpenShift Container Platform 4.18, you can view firing alerts in the Alerting UI. You can configure Alertmanager to send notifications about default platform alerts by configuring alert receivers.
				</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
						Alertmanager does not send notifications by default. It is strongly recommended to configure Alertmanager to receive notifications by configuring alert receivers through the web console or through the <code class="literal">alertmanager-main</code> secret.
					</p></div></rh-alert><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#sending-notifications-to-external-systems_key-concepts" title="1.3.7. Sending notifications to external systems">Sending notifications to external systems</a>
</li><li class="listitem">
<a class="link" href="https://www.pagerduty.com/">PagerDuty</a> (PagerDuty official site)
						</li><li class="listitem">
<a class="link" href="https://www.pagerduty.com/docs/guides/prometheus-integration-guide/">Prometheus Integration Guide</a> (PagerDuty official site)
						</li><li class="listitem">
<a class="link" href="#support-version-matrix-for-monitoring-components_maintenance-and-support-for-monitoring" title="2.1.3. Support version matrix for monitoring components">Support version matrix for monitoring components</a>
</li><li class="listitem">
<a class="link" href="#enabling-alert-routing-for-user-defined-projects_preparing-to-configure-the-monitoring-stack-uwm" title="4.1.3. Enabling alert routing for user-defined projects">Enabling alert routing for user-defined projects</a>
</li></ul></div><section class="section" id="configuring-alert-routing-default-platform-alerts_configuring-alerts-and-notifications"><div class="titlepage"><div><div><h5 class="title">3.5.4.1. Configuring alert routing for default platform alerts</h5></div></div></div><p>
						You can configure Alertmanager to send notifications. Customize where and how Alertmanager sends notifications about default platform alerts by editing the default configuration in the <code class="literal">alertmanager-main</code> secret in the <code class="literal">openshift-monitoring</code> namespace.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							All features of a supported version of upstream Alertmanager are also supported in an OpenShift Container Platform Alertmanager configuration. To check all the configuration options of a supported version of upstream Alertmanager, see <a class="link" href="https://prometheus.io/docs/alerting/0.27/configuration/">Alertmanager configuration</a> (Prometheus documentation).
						</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Open the Alertmanager YAML configuration file:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
										To open the Alertmanager configuration from the CLI:
									</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
												Print the currently active Alertmanager configuration from the <code class="literal">alertmanager-main</code> secret into <code class="literal">alertmanager.yaml</code> file:
											</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring get secret alertmanager-main --template='{{ index .data "alertmanager.yaml" }}' | base64 --decode &gt; alertmanager.yaml</pre></li><li class="listitem">
												Open the <code class="literal">alertmanager.yaml</code> file.
											</li></ol></div></li><li class="listitem"><p class="simpara">
										To open the Alertmanager configuration from the OpenShift Container Platform web console:
									</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
												Go to the <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Cluster Settings</strong></span> → <span class="strong strong"><strong>Configuration</strong></span> → <span class="strong strong"><strong>Alertmanager</strong></span> → <span class="strong strong"><strong>YAML</strong></span> page of the web console.
											</li></ol></div></li></ul></div></li><li class="listitem"><p class="simpara">
								Edit the Alertmanager configuration by updating parameters in the YAML:
							</p><pre class="programlisting language-yaml">global:
  resolve_timeout: 5m
  http_config:
    proxy_from_environment: true <span id="CO32-1"><!--Empty--></span><span class="callout">1</span>
route:
  group_wait: 30s <span id="CO32-2"><!--Empty--></span><span class="callout">2</span>
  group_interval: 5m <span id="CO32-3"><!--Empty--></span><span class="callout">3</span>
  repeat_interval: 12h <span id="CO32-4"><!--Empty--></span><span class="callout">4</span>
  receiver: default
  routes:
  - matchers:
    - "alertname=Watchdog"
    repeat_interval: 2m
    receiver: watchdog
  - matchers:
    - "service=&lt;your_service&gt;" <span id="CO32-5"><!--Empty--></span><span class="callout">5</span>
    routes:
    - matchers:
      - &lt;your_matching_rules&gt; <span id="CO32-6"><!--Empty--></span><span class="callout">6</span>
      receiver: &lt;receiver&gt; <span id="CO32-7"><!--Empty--></span><span class="callout">7</span>
receivers:
- name: default
- name: watchdog
- name: &lt;receiver&gt;
  &lt;receiver_configuration&gt; <span id="CO32-8"><!--Empty--></span><span class="callout">8</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO32-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										If you configured an HTTP cluster-wide proxy, set the <code class="literal">proxy_from_environment</code> parameter to <code class="literal">true</code> to enable proxying for all alert receivers.
									</div></dd><dt><a href="#CO32-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specify how long Alertmanager waits while collecting initial alerts for a group of alerts before sending a notification.
									</div></dd><dt><a href="#CO32-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Specify how much time must elapse before Alertmanager sends a notification about new alerts added to a group of alerts for which an initial notification was already sent.
									</div></dd><dt><a href="#CO32-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										Specify the minimum amount of time that must pass before an alert notification is repeated. If you want a notification to repeat at each group interval, set the <code class="literal">repeat_interval</code> value to less than the <code class="literal">group_interval</code> value. The repeated notification can still be delayed, for example, when certain Alertmanager pods are restarted or rescheduled.
									</div></dd><dt><a href="#CO32-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										Specify the name of the service that fires the alerts.
									</div></dd><dt><a href="#CO32-6"><span class="callout">6</span></a> </dt><dd><div class="para">
										Specify labels to match your alerts.
									</div></dd><dt><a href="#CO32-7"><span class="callout">7</span></a> </dt><dd><div class="para">
										Specify the name of the receiver to use for the alerts.
									</div></dd><dt><a href="#CO32-8"><span class="callout">8</span></a> </dt><dd><div class="para">
										Specify the receiver configuration.
									</div></dd></dl></div><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
											Use the <code class="literal">matchers</code> key name to indicate the matchers that an alert has to fulfill to match the node. Do not use the <code class="literal">match</code> or <code class="literal">match_re</code> key names, which are both deprecated and planned for removal in a future release.
										</li><li class="listitem"><p class="simpara">
											If you define inhibition rules, use the following key names:
										</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
<code class="literal">target_matchers</code>: to indicate the target matchers
												</li><li class="listitem">
<code class="literal">source_matchers</code>: to indicate the source matchers
												</li></ul></div><p class="simpara">
											Do not use the <code class="literal">target_match</code>, <code class="literal">target_match_re</code>, <code class="literal">source_match</code>, or <code class="literal">source_match_re</code> key names, which are deprecated and planned for removal in a future release.
										</p></li></ul></div></div></rh-alert><div class="formalpara"><p class="title"><strong>Example of Alertmanager configuration with PagerDuty as an alert receiver</strong></p><p>
</p><pre class="programlisting language-yaml">global:
  resolve_timeout: 5m
  http_config:
    proxy_from_environment: true
route:
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h
  receiver: default
  routes:
  - matchers:
    - "alertname=Watchdog"
    repeat_interval: 2m
    receiver: watchdog
  - matchers: <span id="CO33-1"><!--Empty--></span><span class="callout">1</span>
    - "service=example-app"
    routes:
    - matchers:
      - "severity=critical"
      receiver: team-frontend-page
receivers:
- name: default
- name: watchdog
- name: team-frontend-page
  pagerduty_configs:
  - service_key: "&lt;your_key&gt;"
    http_config: <span id="CO33-2"><!--Empty--></span><span class="callout">2</span>
      proxy_from_environment: true
      authorization:
        credentials: xxxxxxxxxx</pre>
<p></p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO33-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Alerts of <code class="literal">critical</code> severity that are fired by the <code class="literal">example-app</code> service are sent through the <code class="literal">team-frontend-page</code> receiver. Typically, these types of alerts would be paged to an individual or a critical response team.
									</div></dd><dt><a href="#CO33-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Custom HTTP configuration for a specific receiver. If you configure the custom HTTP configuration for a specific alert receiver, that receiver does not inherit the global HTTP config settings.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Apply the new configuration in the file:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
										To apply the changes from the CLI, run the following command:
									</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring create secret generic alertmanager-main --from-file=alertmanager.yaml --dry-run=client -o=yaml |  oc -n openshift-monitoring replace secret --filename=-</pre></li><li class="listitem">
										To apply the changes from the OpenShift Container Platform web console, click <span class="strong strong"><strong>Save</strong></span>.
									</li></ul></div></li></ol></div></section><section class="section" id="configuring-alert-routing-console_configuring-alerts-and-notifications"><div class="titlepage"><div><div><h5 class="title">3.5.4.2. Configuring alert routing with the OpenShift Container Platform web console</h5></div></div></div><p>
						You can configure alert routing through the OpenShift Container Platform web console to ensure that you learn about important issues with your cluster.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							The OpenShift Container Platform web console provides fewer settings to configure alert routing than the <code class="literal">alertmanager-main</code> secret. To configure alert routing with the access to more configuration settings, see "Configuring alert routing for default platform alerts".
						</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								In the <span class="strong strong"><strong>Administrator</strong></span> perspective, go to <span class="strong strong"><strong>Administration</strong></span> → <span class="strong strong"><strong>Cluster Settings</strong></span> → <span class="strong strong"><strong>Configuration</strong></span> → <span class="strong strong"><strong>Alertmanager</strong></span>.
							</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
									Alternatively, you can go to the same page through the notification drawer. Select the bell icon at the top right of the OpenShift Container Platform web console and choose <span class="strong strong"><strong>Configure</strong></span> in the <span class="strong strong"><strong>AlertmanagerReceiverNotConfigured</strong></span> alert.
								</p></div></rh-alert></li><li class="listitem">
								Click <span class="strong strong"><strong>Create Receiver</strong></span> in the <span class="strong strong"><strong>Receivers</strong></span> section of the page.
							</li><li class="listitem">
								In the <span class="strong strong"><strong>Create Receiver</strong></span> form, add a <span class="strong strong"><strong>Receiver name</strong></span> and choose a <span class="strong strong"><strong>Receiver type</strong></span> from the list.
							</li><li class="listitem"><p class="simpara">
								Edit the receiver configuration:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
										For PagerDuty receivers:
									</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
												Choose an integration type and add a PagerDuty integration key.
											</li><li class="listitem">
												Add the URL of your PagerDuty installation.
											</li><li class="listitem">
												Click <span class="strong strong"><strong>Show advanced configuration</strong></span> if you want to edit the client and incident details or the severity specification.
											</li></ol></div></li><li class="listitem"><p class="simpara">
										For webhook receivers:
									</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
												Add the endpoint to send HTTP POST requests to.
											</li><li class="listitem">
												Click <span class="strong strong"><strong>Show advanced configuration</strong></span> if you want to edit the default option to send resolved alerts to the receiver.
											</li></ol></div></li><li class="listitem"><p class="simpara">
										For email receivers:
									</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
												Add the email address to send notifications to.
											</li><li class="listitem">
												Add SMTP configuration details, including the address to send notifications from, the smarthost and port number used for sending emails, the hostname of the SMTP server, and authentication details.
											</li><li class="listitem">
												Select whether TLS is required.
											</li><li class="listitem">
												Click <span class="strong strong"><strong>Show advanced configuration</strong></span> if you want to edit the default option not to send resolved alerts to the receiver or edit the body of email notifications configuration.
											</li></ol></div></li><li class="listitem"><p class="simpara">
										For Slack receivers:
									</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
												Add the URL of the Slack webhook.
											</li><li class="listitem">
												Add the Slack channel or user name to send notifications to.
											</li><li class="listitem">
												Select <span class="strong strong"><strong>Show advanced configuration</strong></span> if you want to edit the default option not to send resolved alerts to the receiver or edit the icon and username configuration. You can also choose whether to find and link channel names and usernames.
											</li></ol></div></li></ul></div></li><li class="listitem"><p class="simpara">
								By default, firing alerts with labels that match all of the selectors are sent to the receiver. If you want label values for firing alerts to be matched exactly before they are sent to the receiver, perform the following steps:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
										Add routing label names and values in the <span class="strong strong"><strong>Routing labels</strong></span> section of the form.
									</li><li class="listitem">
										Click <span class="strong strong"><strong>Add label</strong></span> to add further routing labels.
									</li></ol></div></li><li class="listitem">
								Click <span class="strong strong"><strong>Create</strong></span> to create the receiver.
							</li></ol></div></section><section class="section" id="configuring-different-alert-receivers-for-default-platform-alerts-and-user-defined-alerts_configuring-alerts-and-notifications"><div class="titlepage"><div><div><h5 class="title">3.5.4.3. Configuring different alert receivers for default platform alerts and user-defined alerts</h5></div></div></div><p>
						You can configure different alert receivers for default platform alerts and user-defined alerts to ensure the following results:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								All default platform alerts are sent to a receiver owned by the team in charge of these alerts.
							</li><li class="listitem">
								All user-defined alerts are sent to another receiver so that the team can focus only on platform alerts.
							</li></ul></div><p>
						You can achieve this by using the <code class="literal">openshift_io_alert_source="platform"</code> label that is added by the Cluster Monitoring Operator to all platform alerts:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Use the <code class="literal">openshift_io_alert_source="platform"</code> matcher to match default platform alerts.
							</li><li class="listitem">
								Use the <code class="literal">openshift_io_alert_source!="platform"</code> or <code class="literal">'openshift_io_alert_source=""'</code> matcher to match user-defined alerts.
							</li></ul></div><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							This configuration does not apply if you have enabled a separate instance of Alertmanager dedicated to user-defined alerts.
						</p></div></rh-alert></section></section></section></section><section class="chapter" id="configuring-user-workload-monitoring"><div class="titlepage"><div><div><h2 class="title">Chapter 4. Configuring user workload monitoring</h2></div></div></div><section class="section" id="preparing-to-configure-the-monitoring-stack-uwm"><div class="titlepage"><div><div><h3 class="title">4.1. Preparing to configure the user workload monitoring stack</h3></div></div></div><p>
				This section explains which user-defined monitoring components can be configured, how to enable user workload monitoring, and how to prepare for configuring the user workload monitoring stack.
			</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Not all configuration parameters for the monitoring stack are exposed. Only the parameters and fields listed in the <a class="link" href="#cluster-monitoring-operator-configuration-reference" title="8.1. Cluster Monitoring Operator configuration reference">Config map reference for the Cluster Monitoring Operator</a> are supported for configuration.
						</li><li class="listitem">
							The monitoring stack imposes additional resource requirements. Consult the computing resources recommendations in <a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/scalability_and_performance/#scaling-cluster-monitoring-operator_recommended-infrastructure-practices">Scaling the Cluster Monitoring Operator</a> and verify that you have sufficient resources.
						</li></ul></div></div></rh-alert><section class="section" id="configurable-monitoring-components_preparing-to-configure-the-monitoring-stack-uwm"><div class="titlepage"><div><div><h4 class="title">4.1.1. Configurable monitoring components</h4></div></div></div><p>
					This table shows the monitoring components you can configure and the keys used to specify the components in the <code class="literal">user-workload-monitoring-config</code> config map.
				</p><rh-table id="idm140059301143280"><table class="lt-4-cols lt-7-rows"><caption>Table 4.1. Configurable monitoring components for user-defined projects</caption><colgroup><col class="col_1" style="width: 50%; "/><!--Empty--><col class="col_2" style="width: 50%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059298905248" scope="col" valign="top">Component</th><th align="left" id="idm140059298904160" scope="col" valign="top">user-workload-monitoring-config config map key</th></tr></thead><tbody><tr><td align="left" headers="idm140059298905248" valign="top"> <p>
									Prometheus Operator
								</p>
</td><td align="left" headers="idm140059298904160" valign="top"> <p>
<code class="literal">prometheusOperator</code>
</p>
</td></tr><tr><td align="left" headers="idm140059298905248" valign="top"> <p>
									Prometheus
								</p>
</td><td align="left" headers="idm140059298904160" valign="top"> <p>
<code class="literal">prometheus</code>
</p>
</td></tr><tr><td align="left" headers="idm140059298905248" valign="top"> <p>
									Alertmanager
								</p>
</td><td align="left" headers="idm140059298904160" valign="top"> <p>
<code class="literal">alertmanager</code>
</p>
</td></tr><tr><td align="left" headers="idm140059298905248" valign="top"> <p>
									Thanos Ruler
								</p>
</td><td align="left" headers="idm140059298904160" valign="top"> <p>
<code class="literal">thanosRuler</code>
</p>
</td></tr></tbody></table></rh-table><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
						Different configuration changes to the <code class="literal">ConfigMap</code> object result in different outcomes:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								The pods are not redeployed. Therefore, there is no service outage.
							</li><li class="listitem"><p class="simpara">
								The affected pods are redeployed:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										For single-node clusters, this results in temporary service outage.
									</li><li class="listitem">
										For multi-node clusters, because of high-availability, the affected pods are gradually rolled out and the monitoring stack remains available.
									</li><li class="listitem">
										Configuring and resizing a persistent volume always results in a service outage, regardless of high availability.
									</li></ul></div></li></ul></div><p>
						Each procedure that requires a change in the config map includes its expected outcome.
					</p></div></rh-alert></section><section class="section" id="enabling-monitoring-for-user-defined-projects-uwm_preparing-to-configure-the-monitoring-stack-uwm"><div class="titlepage"><div><div><h4 class="title">4.1.2. Enabling monitoring for user-defined projects</h4></div></div></div><p>
					In OpenShift Container Platform, you can enable monitoring for user-defined projects in addition to the default platform monitoring. You can monitor your own projects in OpenShift Container Platform without the need for an additional monitoring solution. Using this feature centralizes monitoring for core platform components and user-defined projects.
				</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						Versions of Prometheus Operator installed using Operator Lifecycle Manager (OLM) are not compatible with user-defined monitoring. Therefore, custom Prometheus instances installed as a Prometheus custom resource (CR) managed by the OLM Prometheus Operator are not supported in OpenShift Container Platform.
					</p></div></rh-alert><section class="section" id="enabling-monitoring-for-user-defined-projects_preparing-to-configure-the-monitoring-stack-uwm"><div class="titlepage"><div><div><h5 class="title">4.1.2.1. Enabling monitoring for user-defined projects</h5></div></div></div><p>
						Cluster administrators can enable monitoring for user-defined projects by setting the <code class="literal">enableUserWorkload: true</code> field in the cluster monitoring <code class="literal">ConfigMap</code> object.
					</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
							You must remove any custom Prometheus instances before enabling monitoring for user-defined projects.
						</p></div></rh-alert><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							You must have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role to enable monitoring for user-defined projects in OpenShift Container Platform. Cluster administrators can then optionally grant users permission to configure the components that are responsible for monitoring user-defined projects.
						</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li><li class="listitem">
								You have created the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object.
							</li><li class="listitem"><p class="simpara">
								You have optionally created and configured the <code class="literal">user-workload-monitoring-config</code> <code class="literal">ConfigMap</code> object in the <code class="literal">openshift-user-workload-monitoring</code> project. You can add configuration options to this <code class="literal">ConfigMap</code> object for the components that monitor user-defined projects.
							</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
									Every time you save configuration changes to the <code class="literal">user-workload-monitoring-config</code> <code class="literal">ConfigMap</code> object, the pods in the <code class="literal">openshift-user-workload-monitoring</code> project are redeployed. It might sometimes take a while for these components to redeploy.
								</p></div></rh-alert></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Edit the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</pre></li><li class="listitem"><p class="simpara">
								Add <code class="literal">enableUserWorkload: true</code> under <code class="literal">data/config.yaml</code>:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    enableUserWorkload: true <span id="CO34-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO34-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										When set to <code class="literal">true</code>, the <code class="literal">enableUserWorkload</code> parameter enables monitoring for user-defined projects in a cluster.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Save the file to apply the changes. Monitoring for user-defined projects is then enabled automatically.
							</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
									If you enable monitoring for user-defined projects, the <code class="literal">user-workload-monitoring-config</code> <code class="literal">ConfigMap</code> object is created by default.
								</p></div></rh-alert></li><li class="listitem"><p class="simpara">
								Verify that the <code class="literal">prometheus-operator</code>, <code class="literal">prometheus-user-workload</code>, and <code class="literal">thanos-ruler-user-workload</code> pods are running in the <code class="literal">openshift-user-workload-monitoring</code> project. It might take a short while for the pods to start:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring get pod</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
</p><pre class="programlisting language-terminal">NAME                                   READY   STATUS        RESTARTS   AGE
prometheus-operator-6f7b748d5b-t7nbg   2/2     Running       0          3h
prometheus-user-workload-0             4/4     Running       1          3h
prometheus-user-workload-1             4/4     Running       1          3h
thanos-ruler-user-workload-0           3/3     Running       0          3h
thanos-ruler-user-workload-1           3/3     Running       0          3h</pre>
<p></p></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#user-workload-monitoring-first-steps" title="2.3. User workload monitoring first steps">User workload monitoring first steps</a>
</li></ul></div></section><section class="section" id="granting-users-permission-to-configure-monitoring-for-user-defined-projects_preparing-to-configure-the-monitoring-stack-uwm"><div class="titlepage"><div><div><h5 class="title">4.1.2.2. Granting users permission to configure monitoring for user-defined projects</h5></div></div></div><p>
						As a cluster administrator, you can assign the <code class="literal">user-workload-monitoring-config-edit</code> role to a user. This grants permission to configure and manage monitoring for user-defined projects without giving them permission to configure and manage core OpenShift Container Platform monitoring components.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
							</li><li class="listitem">
								The user account that you are assigning the role to already exists.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Assign the <code class="literal">user-workload-monitoring-config-edit</code> role to a user in the <code class="literal">openshift-user-workload-monitoring</code> project:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring adm policy add-role-to-user \
  user-workload-monitoring-config-edit &lt;user&gt; \
  --role-namespace openshift-user-workload-monitoring</pre></li><li class="listitem"><p class="simpara">
								Verify that the user is correctly assigned to the <code class="literal">user-workload-monitoring-config-edit</code> role by displaying the related role binding:
							</p><pre class="programlisting language-terminal">$ oc describe rolebinding &lt;role_binding_name&gt; -n openshift-user-workload-monitoring</pre><div class="formalpara"><p class="title"><strong>Example command</strong></p><p>
</p><pre class="programlisting language-terminal">$ oc describe rolebinding user-workload-monitoring-config-edit -n openshift-user-workload-monitoring</pre>
<p></p></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
</p><pre class="programlisting language-terminal">Name:         user-workload-monitoring-config-edit
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Role:
  Kind:  Role
  Name:  user-workload-monitoring-config-edit
Subjects:
  Kind  Name  Namespace
  ----  ----  ---------
  User  user1           <span id="CO35-1"><!--Empty--></span><span class="callout">1</span></pre>
<p></p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO35-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										In this example, <code class="literal">user1</code> is assigned to the <code class="literal">user-workload-monitoring-config-edit</code> role.
									</div></dd></dl></div></li></ol></div></section></section><section class="section" id="enabling-alert-routing-for-user-defined-projects_preparing-to-configure-the-monitoring-stack-uwm"><div class="titlepage"><div><div><h4 class="title">4.1.3. Enabling alert routing for user-defined projects</h4></div></div></div><p>
					In OpenShift Container Platform, an administrator can enable alert routing for user-defined projects. This process consists of the following steps:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
							Enable alert routing for user-defined projects:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
									Use the default platform Alertmanager instance.
								</li><li class="listitem">
									Use a separate Alertmanager instance only for user-defined projects.
								</li></ul></div></li><li class="listitem">
							Grant users permission to configure alert routing for user-defined projects.
						</li></ul></div><p>
					After you complete these steps, developers and other users can configure custom alerts and alert routing for their user-defined projects.
				</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#understanding-alert-routing-for-user-defined-projects_key-concepts" title="1.3.6. Understanding alert routing for user-defined projects">Understanding alert routing for user-defined projects</a>
</li></ul></div><section class="section" id="enabling-the-platform-alertmanager-instance-for-user-defined-alert-routing_preparing-to-configure-the-monitoring-stack-uwm"><div class="titlepage"><div><div><h5 class="title">4.1.3.1. Enabling the platform Alertmanager instance for user-defined alert routing</h5></div></div></div><p>
						You can allow users to create user-defined alert routing configurations that use the main platform instance of Alertmanager.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
							</li><li class="listitem">
								A cluster administrator has enabled monitoring for user-defined projects.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Edit the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</pre></li><li class="listitem"><p class="simpara">
								Add <code class="literal">enableUserAlertmanagerConfig: true</code> in the <code class="literal">alertmanagerMain</code> section under <code class="literal">data/config.yaml</code>:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    # ...
    alertmanagerMain:
      enableUserAlertmanagerConfig: true <span id="CO36-1"><!--Empty--></span><span class="callout">1</span>
    # ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO36-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Set the <code class="literal">enableUserAlertmanagerConfig</code> value to <code class="literal">true</code> to allow users to create user-defined alert routing configurations that use the main platform instance of Alertmanager.
									</div></dd></dl></div></li><li class="listitem">
								Save the file to apply the changes. The new configuration is applied automatically.
							</li></ol></div></section><section class="section" id="enabling-a-separate-alertmanager-instance-for-user-defined-alert-routing_preparing-to-configure-the-monitoring-stack-uwm"><div class="titlepage"><div><div><h5 class="title">4.1.3.2. Enabling a separate Alertmanager instance for user-defined alert routing</h5></div></div></div><p>
						In some clusters, you might want to deploy a dedicated Alertmanager instance for user-defined projects, which can help reduce the load on the default platform Alertmanager instance and can better separate user-defined alerts from default platform alerts. In these cases, you can optionally enable a separate instance of Alertmanager to send alerts for user-defined projects only.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
							</li><li class="listitem">
								You have enabled monitoring for user-defined projects.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Edit the <code class="literal">user-workload-monitoring-config</code> <code class="literal">ConfigMap</code> object:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</pre></li><li class="listitem"><p class="simpara">
								Add <code class="literal">enabled: true</code> and <code class="literal">enableAlertmanagerConfig: true</code> in the <code class="literal">alertmanager</code> section under <code class="literal">data/config.yaml</code>:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    alertmanager:
      enabled: true <span id="CO37-1"><!--Empty--></span><span class="callout">1</span>
      enableAlertmanagerConfig: true <span id="CO37-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO37-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Set the <code class="literal">enabled</code> value to <code class="literal">true</code> to enable a dedicated instance of the Alertmanager for user-defined projects in a cluster. Set the value to <code class="literal">false</code> or omit the key entirely to disable the Alertmanager for user-defined projects. If you set this value to <code class="literal">false</code> or if the key is omitted, user-defined alerts are routed to the default platform Alertmanager instance.
									</div></dd><dt><a href="#CO37-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Set the <code class="literal">enableAlertmanagerConfig</code> value to <code class="literal">true</code> to enable users to define their own alert routing configurations with <code class="literal">AlertmanagerConfig</code> objects.
									</div></dd></dl></div></li><li class="listitem">
								Save the file to apply the changes. The dedicated instance of Alertmanager for user-defined projects starts automatically.
							</li></ol></div><div class="itemizedlist"><p class="title"><strong>Verification</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Verify that the <code class="literal">user-workload</code> Alertmanager instance has started:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring get alertmanager</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
</p><pre class="programlisting language-terminal">NAME            VERSION   REPLICAS   AGE
user-workload   0.24.0    2          100s</pre>
<p></p></div></li></ul></div></section><section class="section" id="granting-users-permission-to-configure-alert-routing-for-user-defined-projects_preparing-to-configure-the-monitoring-stack-uwm"><div class="titlepage"><div><div><h5 class="title">4.1.3.3. Granting users permission to configure alert routing for user-defined projects</h5></div></div></div><p>
						You can grant users permission to configure alert routing for user-defined projects.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
							</li><li class="listitem">
								You have enabled monitoring for user-defined projects.
							</li><li class="listitem">
								The user account that you are assigning the role to already exists.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								Assign the <code class="literal">alert-routing-edit</code> cluster role to a user in the user-defined project:
							</p><pre class="programlisting language-terminal">$ oc -n &lt;namespace&gt; adm policy add-role-to-user alert-routing-edit &lt;user&gt; <span id="CO38-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO38-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										For <code class="literal">&lt;namespace&gt;</code>, substitute the namespace for the user-defined project, such as <code class="literal">ns1</code>. For <code class="literal">&lt;user&gt;</code>, substitute the username for the account to which you want to assign the role.
									</div></dd></dl></div></li></ul></div><div class="_additional-resources _additional-resources"><p class="title"><strong>Additional resources</strong></p><p>
<a class="link" href="#configuring-alert-notifications_configuring-alerts-and-notifications-uwm" title="4.5.4. Configuring alert notifications">Configuring alert notifications</a>
</p></div></section></section><section class="section" id="granting-users-permission-to-monitor-user-defined-projects_preparing-to-configure-the-monitoring-stack-uwm"><div class="titlepage"><div><div><h4 class="title">4.1.4. Granting users permissions for monitoring for user-defined projects</h4></div></div></div><p>
					As a cluster administrator, you can monitor all core OpenShift Container Platform and user-defined projects.
				</p><p>
					You can also grant developers and other users different permissions:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Monitoring user-defined projects
						</li><li class="listitem">
							Configuring the components that monitor user-defined projects
						</li><li class="listitem">
							Configuring alert routing for user-defined projects
						</li><li class="listitem">
							Managing alerts and silences for user-defined projects
						</li></ul></div><p>
					You can grant the permissions by assigning one of the following monitoring roles or cluster roles:
				</p><rh-table id="idm140059301334512"><table class="lt-4-cols lt-7-rows"><caption>Table 4.2. Monitoring roles</caption><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059301328752" scope="col" valign="top">Role name</th><th align="left" id="idm140059297539696" scope="col" valign="top">Description</th><th align="left" id="idm140059297538608" scope="col" valign="top">Project</th></tr></thead><tbody><tr><td align="left" headers="idm140059301328752" valign="top"> <p>
<code class="literal">user-workload-monitoring-config-edit</code>
</p>
</td><td align="left" headers="idm140059297539696" valign="top"> <p>
									Users with this role can edit the <code class="literal">user-workload-monitoring-config</code> <code class="literal">ConfigMap</code> object to configure Prometheus, Prometheus Operator, Alertmanager, and Thanos Ruler for user-defined workload monitoring.
								</p>
</td><td align="left" headers="idm140059297538608" valign="top"> <p>
<code class="literal">openshift-user-workload-monitoring</code>
</p>
</td></tr><tr><td align="left" headers="idm140059301328752" valign="top"> <p>
<code class="literal">monitoring-alertmanager-api-reader</code>
</p>
</td><td align="left" headers="idm140059297539696" valign="top"> <p>
									Users with this role have read access to the user-defined Alertmanager API for all projects, if the user-defined Alertmanager is enabled.
								</p>
</td><td align="left" headers="idm140059297538608" valign="top"> <p>
<code class="literal">openshift-user-workload-monitoring</code>
</p>
</td></tr><tr><td align="left" headers="idm140059301328752" valign="top"> <p>
<code class="literal">monitoring-alertmanager-api-writer</code>
</p>
</td><td align="left" headers="idm140059297539696" valign="top"> <p>
									Users with this role have read and write access to the user-defined Alertmanager API for all projects, if the user-defined Alertmanager is enabled.
								</p>
</td><td align="left" headers="idm140059297538608" valign="top"> <p>
<code class="literal">openshift-user-workload-monitoring</code>
</p>
</td></tr></tbody></table></rh-table><rh-table id="idm140059285503920"><table class="lt-4-cols lt-7-rows"><caption>Table 4.3. Monitoring cluster roles</caption><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059284861200" scope="col" valign="top">Cluster role name</th><th align="left" id="idm140059284860112" scope="col" valign="top">Description</th><th align="left" id="idm140059284859024" scope="col" valign="top">Project</th></tr></thead><tbody><tr><td align="left" headers="idm140059284861200" valign="top"> <p>
<code class="literal">monitoring-rules-view</code>
</p>
</td><td align="left" headers="idm140059284860112" valign="top"> <p>
									Users with this cluster role have read access to <code class="literal">PrometheusRule</code> custom resources (CRs) for user-defined projects. They can also view the alerts and silences in the <span class="strong strong"><strong>Developer</strong></span> perspective of the OpenShift Container Platform web console.
								</p>
</td><td align="left" headers="idm140059284859024" valign="top"> <p>
									Can be bound with <code class="literal">RoleBinding</code> to any user project.
								</p>
</td></tr><tr><td align="left" headers="idm140059284861200" valign="top"> <p>
<code class="literal">monitoring-rules-edit</code>
</p>
</td><td align="left" headers="idm140059284860112" valign="top"> <p>
									Users with this cluster role can create, modify, and delete <code class="literal">PrometheusRule</code> CRs for user-defined projects. They can also manage alerts and silences in the <span class="strong strong"><strong>Developer</strong></span> perspective of the OpenShift Container Platform web console.
								</p>
</td><td align="left" headers="idm140059284859024" valign="top"> <p>
									Can be bound with <code class="literal">RoleBinding</code> to any user project.
								</p>
</td></tr><tr><td align="left" headers="idm140059284861200" valign="top"> <p>
<code class="literal">monitoring-edit</code>
</p>
</td><td align="left" headers="idm140059284860112" valign="top"> <p>
									Users with this cluster role have the same privileges as users with the <code class="literal">monitoring-rules-edit</code> cluster role. Additionally, users can create, read, modify, and delete <code class="literal">ServiceMonitor</code> and <code class="literal">PodMonitor</code> resources to scrape metrics from services and pods.
								</p>
</td><td align="left" headers="idm140059284859024" valign="top"> <p>
									Can be bound with <code class="literal">RoleBinding</code> to any user project.
								</p>
</td></tr><tr><td align="left" headers="idm140059284861200" valign="top"> <p>
<code class="literal">alert-routing-edit</code>
</p>
</td><td align="left" headers="idm140059284860112" valign="top"> <p>
									Users with this cluster role can create, update, and delete <code class="literal">AlertmanagerConfig</code> CRs for user-defined projects.
								</p>
</td><td align="left" headers="idm140059284859024" valign="top"> <p>
									Can be bound with <code class="literal">RoleBinding</code> to any user project.
								</p>
</td></tr><tr><td align="left" headers="idm140059284861200" valign="top"> <p>
<code class="literal">pod-metrics-reader</code>
</p>
</td><td align="left" headers="idm140059284860112" valign="top"> <p>
									Users with this cluster role can access Thanos API endpoints for user-defined projects.
								</p>
</td><td align="left" headers="idm140059284859024" valign="top"> <p>
									Can be bound with <code class="literal">RoleBinding</code> to any user project.
								</p>
</td></tr></tbody></table></rh-table><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#cmo-services-resources_accessing-monitoring-apis-by-using-the-cli" title="5.3.5.2. CMO services resources">CMO services resources</a>
</li><li class="listitem">
<a class="link" href="#granting-users-permission-to-configure-monitoring-for-user-defined-projects_preparing-to-configure-the-monitoring-stack-uwm" title="4.1.2.2. Granting users permission to configure monitoring for user-defined projects">Granting users permission to configure monitoring for user-defined projects</a>
</li><li class="listitem">
<a class="link" href="#granting-users-permission-to-configure-alert-routing-for-user-defined-projects_preparing-to-configure-the-monitoring-stack-uwm" title="4.1.3.3. Granting users permission to configure alert routing for user-defined projects">Granting users permission to configure alert routing for user-defined projects</a>
</li></ul></div><section class="section" id="granting-user-permissions-using-the-web-console_preparing-to-configure-the-monitoring-stack-uwm"><div class="titlepage"><div><div><h5 class="title">4.1.4.1. Granting user permissions by using the web console</h5></div></div></div><p>
						You can grant users permissions for the <code class="literal">openshift-monitoring</code> project or their own projects, by using the OpenShift Container Platform web console.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
							</li><li class="listitem">
								The user account that you are assigning the role to already exists.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								In the <span class="strong strong"><strong>Administrator</strong></span> perspective of the OpenShift Container Platform web console, go to <span class="strong strong"><strong>User Management</strong></span> → <span class="strong strong"><strong>RoleBindings</strong></span> → <span class="strong strong"><strong>Create binding</strong></span>.
							</li><li class="listitem">
								In the <span class="strong strong"><strong>Binding Type</strong></span> section, select the <span class="strong strong"><strong>Namespace Role Binding</strong></span> type.
							</li><li class="listitem">
								In the <span class="strong strong"><strong>Name</strong></span> field, enter a name for the role binding.
							</li><li class="listitem"><p class="simpara">
								In the <span class="strong strong"><strong>Namespace</strong></span> field, select the project where you want to grant the access.
							</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
									The monitoring role or cluster role permissions that you grant to a user by using this procedure apply only to the project that you select in the <span class="strong strong"><strong>Namespace</strong></span> field.
								</p></div></rh-alert></li><li class="listitem">
								Select a monitoring role or cluster role from the <span class="strong strong"><strong>Role Name</strong></span> list.
							</li><li class="listitem">
								In the <span class="strong strong"><strong>Subject</strong></span> section, select <span class="strong strong"><strong>User</strong></span>.
							</li><li class="listitem">
								In the <span class="strong strong"><strong>Subject Name</strong></span> field, enter the name of the user.
							</li><li class="listitem">
								Select <span class="strong strong"><strong>Create</strong></span> to apply the role binding.
							</li></ol></div></section><section class="section" id="granting-user-permissions-using-the-cli_preparing-to-configure-the-monitoring-stack-uwm"><div class="titlepage"><div><div><h5 class="title">4.1.4.2. Granting user permissions by using the CLI</h5></div></div></div><p>
						You can grant users permissions to monitor their own projects, by using the OpenShift CLI (<code class="literal">oc</code>).
					</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
							Whichever role or cluster role you choose, you must bind it against a specific project as a cluster administrator.
						</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
							</li><li class="listitem">
								The user account that you are assigning the role to already exists.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								To assign a monitoring role to a user for a project, enter the following command:
							</p><pre class="programlisting language-terminal">$ oc adm policy add-role-to-user &lt;role&gt; &lt;user&gt; -n &lt;namespace&gt; --role-namespace &lt;namespace&gt; <span id="CO39-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO39-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Substitute <code class="literal">&lt;role&gt;</code> with the wanted monitoring role, <code class="literal">&lt;user&gt;</code> with the user to whom you want to assign the role, and <code class="literal">&lt;namespace&gt;</code> with the project where you want to grant the access.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								To assign a monitoring cluster role to a user for a project, enter the following command:
							</p><pre class="programlisting language-terminal">$ oc adm policy add-cluster-role-to-user &lt;cluster-role&gt; &lt;user&gt; -n &lt;namespace&gt; <span id="CO40-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO40-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Substitute <code class="literal">&lt;cluster-role&gt;</code> with the wanted monitoring cluster role, <code class="literal">&lt;user&gt;</code> with the user to whom you want to assign the cluster role, and <code class="literal">&lt;namespace&gt;</code> with the project where you want to grant the access.
									</div></dd></dl></div></li></ul></div></section></section><section class="section" id="excluding-a-user-defined-project-from-monitoring_preparing-to-configure-the-monitoring-stack-uwm"><div class="titlepage"><div><div><h4 class="title">4.1.5. Excluding a user-defined project from monitoring</h4></div></div></div><p>
					Individual user-defined projects can be excluded from user workload monitoring. To do so, add the <code class="literal">openshift.io/user-monitoring</code> label to the project’s namespace with a value of <code class="literal">false</code>.
				</p><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Add the label to the project namespace:
						</p><pre class="programlisting language-terminal">$ oc label namespace my-project 'openshift.io/user-monitoring=false'</pre></li><li class="listitem"><p class="simpara">
							To re-enable monitoring, remove the label from the namespace:
						</p><pre class="programlisting language-terminal">$ oc label namespace my-project 'openshift.io/user-monitoring-'</pre><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
								If there were any active monitoring targets for the project, it may take a few minutes for Prometheus to stop scraping them after adding the label.
							</p></div></rh-alert></li></ol></div></section><section class="section" id="disabling-monitoring-for-user-defined-projects_preparing-to-configure-the-monitoring-stack-uwm"><div class="titlepage"><div><div><h4 class="title">4.1.6. Disabling monitoring for user-defined projects</h4></div></div></div><p>
					After enabling monitoring for user-defined projects, you can disable it again by setting <code class="literal">enableUserWorkload: false</code> in the cluster monitoring <code class="literal">ConfigMap</code> object.
				</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						Alternatively, you can remove <code class="literal">enableUserWorkload: true</code> to disable monitoring for user-defined projects.
					</p></div></rh-alert><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">cluster-monitoring-config</code> <code class="literal">ConfigMap</code> object:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</pre><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
									Set <code class="literal">enableUserWorkload:</code> to <code class="literal">false</code> under <code class="literal">data/config.yaml</code>:
								</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    enableUserWorkload: false</pre></li></ol></div></li><li class="listitem">
							Save the file to apply the changes. Monitoring for user-defined projects is then disabled automatically.
						</li><li class="listitem"><p class="simpara">
							Check that the <code class="literal">prometheus-operator</code>, <code class="literal">prometheus-user-workload</code> and <code class="literal">thanos-ruler-user-workload</code> pods are terminated in the <code class="literal">openshift-user-workload-monitoring</code> project. This might take a short while:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring get pod</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
</p><pre class="programlisting language-terminal">No resources found in openshift-user-workload-monitoring project.</pre>
<p></p></div></li></ol></div><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						The <code class="literal">user-workload-monitoring-config</code> <code class="literal">ConfigMap</code> object in the <code class="literal">openshift-user-workload-monitoring</code> project is not automatically deleted when monitoring for user-defined projects is disabled. This is to preserve any custom configurations that you may have created in the <code class="literal">ConfigMap</code> object.
					</p></div></rh-alert></section></section><section class="section" id="configuring-performance-and-scalability-uwm"><div class="titlepage"><div><div><h3 class="title">4.2. Configuring performance and scalability for user workload monitoring</h3></div></div></div><p>
				You can configure the monitoring stack to optimize the performance and scale of your clusters. The following documentation provides information about how to distribute the monitoring components and control the impact of the monitoring stack on CPU and memory resources.
			</p><section class="section" id="controlling-placement-and-distribution-of-monitoing-components_configuring-performance-and-scalability-uwm"><div class="titlepage"><div><div><h4 class="title">4.2.1. Controlling the placement and distribution of monitoring components</h4></div></div></div><p>
					You can move the monitoring stack components to specific nodes:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Use the <code class="literal">nodeSelector</code> constraint with labeled nodes to move any of the monitoring stack components to specific nodes.
						</li><li class="listitem">
							Assign tolerations to enable moving components to tainted nodes.
						</li></ul></div><p>
					By doing so, you control the placement and distribution of the monitoring components across a cluster.
				</p><p>
					By controlling placement and distribution of monitoring components, you can optimize system resource use, improve performance, and separate workloads based on specific requirements or policies.
				</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#using-node-selectors-to-move-monitoring-components_key-concepts" title="1.3.1.1. Using node selectors to move monitoring components">Using node selectors to move monitoring components</a>
</li></ul></div><section class="section" id="moving-monitoring-components-to-different-nodes_configuring-performance-and-scalability-uwm"><div class="titlepage"><div><div><h5 class="title">4.2.1.1. Moving monitoring components to different nodes</h5></div></div></div><p>
						You can move any of the components that monitor workloads for user-defined projects to specific worker nodes.
					</p><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
							It is not permitted to move components to control plane or infrastructure nodes.
						</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role or as a user with the <code class="literal">user-workload-monitoring-config-edit</code> role in the <code class="literal">openshift-user-workload-monitoring</code> project.
							</li><li class="listitem">
								A cluster administrator has enabled monitoring for user-defined projects.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								If you have not done so yet, add a label to the nodes on which you want to run the monitoring components:
							</p><pre class="programlisting language-terminal">$ oc label nodes &lt;node_name&gt; &lt;node_label&gt; <span id="CO41-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO41-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Replace <code class="literal">&lt;node_name&gt;</code> with the name of the node where you want to add the label. Replace <code class="literal">&lt;node_label&gt;</code> with the name of the wanted label.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Edit the <code class="literal">user-workload-monitoring-config</code> <code class="literal">ConfigMap</code> object in the <code class="literal">openshift-user-workload-monitoring</code> project:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</pre></li><li class="listitem"><p class="simpara">
								Specify the node labels for the <code class="literal">nodeSelector</code> constraint for the component under <code class="literal">data/config.yaml</code>:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    # ...
    &lt;component&gt;: <span id="CO42-1"><!--Empty--></span><span class="callout">1</span>
      nodeSelector:
        &lt;node_label_1&gt; <span id="CO42-2"><!--Empty--></span><span class="callout">2</span>
        &lt;node_label_2&gt; <span id="CO42-3"><!--Empty--></span><span class="callout">3</span>
    # ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO42-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Substitute <code class="literal">&lt;component&gt;</code> with the appropriate monitoring stack component name.
									</div></dd><dt><a href="#CO42-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Substitute <code class="literal">&lt;node_label_1&gt;</code> with the label you added to the node.
									</div></dd><dt><a href="#CO42-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Optional: Specify additional labels. If you specify additional labels, the pods for the component are only scheduled on the nodes that contain all of the specified labels.
									</div></dd></dl></div><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
									If monitoring components remain in a <code class="literal">Pending</code> state after configuring the <code class="literal">nodeSelector</code> constraint, check the pod events for errors relating to taints and tolerations.
								</p></div></rh-alert></li><li class="listitem">
								Save the file to apply the changes. The components specified in the new configuration are automatically moved to the new nodes, and the pods affected by the new configuration are redeployed.
							</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#enabling-monitoring-for-user-defined-projects-uwm_preparing-to-configure-the-monitoring-stack-uwm" title="4.1.2. Enabling monitoring for user-defined projects">Enabling monitoring for user-defined projects</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/nodes/#nodes-nodes-working-updating_nodes-nodes-working">Understanding how to update labels on nodes</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/nodes/#nodes-scheduler-node-selectors">Placing pods on specific nodes using node selectors</a>
</li><li class="listitem">
<a class="link" href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector">nodeSelector</a> (Kubernetes documentation)
							</li></ul></div></section><section class="section" id="assigning-tolerations-to-monitoring-components_configuring-performance-and-scalability-uwm"><div class="titlepage"><div><div><h5 class="title">4.2.1.2. Assigning tolerations to monitoring components</h5></div></div></div><p>
						You can assign tolerations to the components that monitor user-defined projects, to enable moving them to tainted worker nodes. Scheduling is not permitted on control plane or infrastructure nodes.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role, or as a user with the <code class="literal">user-workload-monitoring-config-edit</code> role in the <code class="literal">openshift-user-workload-monitoring</code> project.
							</li><li class="listitem">
								A cluster administrator has enabled monitoring for user-defined projects.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Edit the <code class="literal">user-workload-monitoring-config</code> config map in the <code class="literal">openshift-user-workload-monitoring</code> project:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</pre></li><li class="listitem"><p class="simpara">
								Specify <code class="literal">tolerations</code> for the component:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    &lt;component&gt;:
      tolerations:
        &lt;toleration_specification&gt;</pre><p class="simpara">
								Substitute <code class="literal">&lt;component&gt;</code> and <code class="literal">&lt;toleration_specification&gt;</code> accordingly.
							</p><p class="simpara">
								For example, <code class="literal">oc adm taint nodes node1 key1=value1:NoSchedule</code> adds a taint to <code class="literal">node1</code> with the key <code class="literal">key1</code> and the value <code class="literal">value1</code>. This prevents monitoring components from deploying pods on <code class="literal">node1</code> unless a toleration is configured for that taint. The following example configures the <code class="literal">thanosRuler</code> component to tolerate the example taint:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    thanosRuler:
      tolerations:
      - key: "key1"
        operator: "Equal"
        value: "value1"
        effect: "NoSchedule"</pre></li><li class="listitem">
								Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.
							</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#enabling-monitoring-for-user-defined-projects-uwm_preparing-to-configure-the-monitoring-stack-uwm" title="4.1.2. Enabling monitoring for user-defined projects">Enabling monitoring for user-defined projects</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/nodes/#nodes-scheduler-taints-tolerations">Controlling pod placement using node taints</a>
</li><li class="listitem">
<a class="link" href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/">Taints and Tolerations</a> (Kubernetes documentation)
							</li></ul></div></section></section><section class="section" id="managing-cpu-and-memory-resources-for-monitoring-components_configuring-performance-and-scalability-uwm"><div class="titlepage"><div><div><h4 class="title">4.2.2. Managing CPU and memory resources for monitoring components</h4></div></div></div><p>
					You can ensure that the containers that run monitoring components have enough CPU and memory resources by specifying values for resource limits and requests for those components.
				</p><p>
					You can configure these limits and requests for monitoring components that monitor user-defined projects in the <code class="literal">openshift-user-workload-monitoring</code> namespace.
				</p><section class="section" id="specifying-limits-and-resource-requests-for-monitoring-components_configuring-performance-and-scalability-uwm"><div class="titlepage"><div><div><h5 class="title">4.2.2.1. Specifying limits and requests</h5></div></div></div><p>
						To configure CPU and memory resources, specify values for resource limits and requests in the <code class="literal">user-workload-monitoring-config</code> <code class="literal">ConfigMap</code> object in the <code class="literal">openshift-user-workload-monitoring</code> namespace.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role, or as a user with the <code class="literal">user-workload-monitoring-config-edit</code> role in the <code class="literal">openshift-user-workload-monitoring</code> project.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Edit the <code class="literal">user-workload-monitoring-config</code> config map in the <code class="literal">openshift-user-workload-monitoring</code> project:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</pre></li><li class="listitem"><p class="simpara">
								Add values to define resource limits and requests for each component you want to configure.
							</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
									Ensure that the value set for a limit is always higher than the value set for a request. Otherwise, an error will occur, and the container will not run.
								</p></div></rh-alert><div class="formalpara"><p class="title"><strong>Example of setting resource limits and requests</strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    alertmanager:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
    prometheus:
      resources:
        limits:
          cpu: 500m
          memory: 3Gi
        requests:
          cpu: 200m
          memory: 500Mi
    thanosRuler:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi</pre>
<p></p></div></li><li class="listitem">
								Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.
							</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#about-specifying-limits-and-requests-for-monitoring-components_key-concepts" title="1.3.1.3. About specifying limits and requests for monitoring components">About specifying limits and requests for monitoring components</a>
</li><li class="listitem">
<a class="link" href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits">Kubernetes requests and limits documentation</a> (Kubernetes documentation)
							</li></ul></div></section></section><section class="section" id="controlling-the-impact-of-unbound-attributes-in-user-defined-projects_configuring-performance-and-scalability-uwm"><div class="titlepage"><div><div><h4 class="title">4.2.3. Controlling the impact of unbound metrics attributes in user-defined projects</h4></div></div></div><p>
					Cluster administrators can use the following measures to control the impact of unbound metrics attributes in user-defined projects:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Limit the number of samples that can be accepted per target scrape in user-defined projects
						</li><li class="listitem">
							Limit the number of scraped labels, the length of label names, and the length of label values
						</li><li class="listitem">
							Configure the intervals between consecutive scrapes and between Prometheus rule evaluations
						</li><li class="listitem">
							Create alerts that fire when a scrape sample threshold is reached or when the target cannot be scraped
						</li></ul></div><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						Limiting scrape samples can help prevent the issues caused by adding many unbound attributes to labels. Developers can also prevent the underlying cause by limiting the number of unbound attributes that they define for metrics. Using attributes that are bound to a limited set of possible values reduces the number of potential key-value pair combinations.
					</p></div></rh-alert><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#controlling-the-impact-of-unbound-attributes-in-user-defined-projects_key-concepts" title="1.3.3.1. Controlling the impact of unbound metrics attributes in user-defined projects">Controlling the impact of unbound metrics attributes in user-defined projects</a>
</li><li class="listitem">
<a class="link" href="#enabling-monitoring-for-user-defined-projects-uwm_preparing-to-configure-the-monitoring-stack-uwm" title="4.1.2. Enabling monitoring for user-defined projects">Enabling monitoring for user-defined projects</a>
</li><li class="listitem">
<a class="link" href="#determining-why-prometheus-is-consuming-disk-space_troubleshooting-monitoring-issues" title="7.2. Determining why Prometheus is consuming a lot of disk space">Determining why Prometheus is consuming a lot of disk space</a>
</li></ul></div><section class="section" id="setting-scrape-and-evaluation-intervals-limits-for-user-defined-projects_configuring-performance-and-scalability-uwm"><div class="titlepage"><div><div><h5 class="title">4.2.3.1. Setting scrape intervals, evaluation intervals, and enforced limits for user-defined projects</h5></div></div></div><p>
						You can set the following scrape and label limits for user-defined projects:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Limit the number of samples that can be accepted per target scrape
							</li><li class="listitem">
								Limit the number of scraped labels
							</li><li class="listitem">
								Limit the length of label names and label values
							</li></ul></div><p>
						You can also set an interval between consecutive scrapes and between Prometheus rule evaluations.
					</p><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
							If you set sample or label limits, no further sample data is ingested for that target scrape after the limit is reached.
						</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role, or as a user with the <code class="literal">user-workload-monitoring-config-edit</code> role in the <code class="literal">openshift-user-workload-monitoring</code> project.
							</li><li class="listitem">
								A cluster administrator has enabled monitoring for user-defined projects.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Edit the <code class="literal">user-workload-monitoring-config</code> <code class="literal">ConfigMap</code> object in the <code class="literal">openshift-user-workload-monitoring</code> project:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</pre></li><li class="listitem"><p class="simpara">
								Add the enforced limit and time interval configurations to <code class="literal">data/config.yaml</code>:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      enforcedSampleLimit: 50000 <span id="CO43-1"><!--Empty--></span><span class="callout">1</span>
      enforcedLabelLimit: 500 <span id="CO43-2"><!--Empty--></span><span class="callout">2</span>
      enforcedLabelNameLengthLimit: 50 <span id="CO43-3"><!--Empty--></span><span class="callout">3</span>
      enforcedLabelValueLengthLimit: 600 <span id="CO43-4"><!--Empty--></span><span class="callout">4</span>
      scrapeInterval: 1m30s <span id="CO43-5"><!--Empty--></span><span class="callout">5</span>
      evaluationInterval: 1m15s <span id="CO43-6"><!--Empty--></span><span class="callout">6</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO43-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										A value is required if this parameter is specified. This <code class="literal">enforcedSampleLimit</code> example limits the number of samples that can be accepted per target scrape in user-defined projects to 50,000.
									</div></dd><dt><a href="#CO43-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specifies the maximum number of labels per scrape. The default value is <code class="literal">0</code>, which specifies no limit.
									</div></dd><dt><a href="#CO43-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Specifies the maximum character length for a label name. The default value is <code class="literal">0</code>, which specifies no limit.
									</div></dd><dt><a href="#CO43-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										Specifies the maximum character length for a label value. The default value is <code class="literal">0</code>, which specifies no limit.
									</div></dd><dt><a href="#CO43-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										Specifies the interval between consecutive scrapes. The interval must be set between 5 seconds and 5 minutes. The default value is <code class="literal">30s</code>.
									</div></dd><dt><a href="#CO43-6"><span class="callout">6</span></a> </dt><dd><div class="para">
										Specifies the interval between Prometheus rule evaluations. The interval must be set between 5 seconds and 5 minutes. The default value for Prometheus is <code class="literal">30s</code>.
									</div></dd></dl></div><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
									You can also configure the <code class="literal">evaluationInterval</code> property for Thanos Ruler through the <code class="literal">data/config.yaml/thanosRuler</code> field. The default value for Thanos Ruler is <code class="literal">15s</code>.
								</p></div></rh-alert></li><li class="listitem">
								Save the file to apply the changes. The limits are applied automatically.
							</li></ol></div></section><section class="section" id="creating-scrape-sample-alerts_configuring-performance-and-scalability-uwm"><div class="titlepage"><div><div><h5 class="title">4.2.3.2. Creating scrape sample alerts</h5></div></div></div><p>
						You can create alerts that notify you when:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								The target cannot be scraped or is not available for the specified <code class="literal">for</code> duration
							</li><li class="listitem">
								A scrape sample threshold is reached or is exceeded for the specified <code class="literal">for</code> duration
							</li></ul></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role, or as a user with the <code class="literal">user-workload-monitoring-config-edit</code> role in the <code class="literal">openshift-user-workload-monitoring</code> project.
							</li><li class="listitem">
								A cluster administrator has enabled monitoring for user-defined projects.
							</li><li class="listitem">
								You have limited the number of samples that can be accepted per target scrape in user-defined projects, by using <code class="literal">enforcedSampleLimit</code>.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Create a YAML file with alerts that inform you when the targets are down and when the enforced sample limit is approaching. The file in this example is called <code class="literal">monitoring-stack-alerts.yaml</code>:
							</p><pre class="programlisting language-yaml">apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: k8s
    role: alert-rules
  name: monitoring-stack-alerts <span id="CO44-1"><!--Empty--></span><span class="callout">1</span>
  namespace: ns1 <span id="CO44-2"><!--Empty--></span><span class="callout">2</span>
spec:
  groups:
  - name: general.rules
    rules:
    - alert: TargetDown <span id="CO44-3"><!--Empty--></span><span class="callout">3</span>
      annotations:
        message: '{{ printf "%.4g" $value }}% of the {{ $labels.job }}/{{ $labels.service
          }} targets in {{ $labels.namespace }} namespace are down.' <span id="CO44-4"><!--Empty--></span><span class="callout">4</span>
      expr: 100 * (count(up == 0) BY (job, namespace, service) / count(up) BY (job,
        namespace, service)) &gt; 10
      for: 10m <span id="CO44-5"><!--Empty--></span><span class="callout">5</span>
      labels:
        severity: warning <span id="CO44-6"><!--Empty--></span><span class="callout">6</span>
    - alert: ApproachingEnforcedSamplesLimit <span id="CO44-7"><!--Empty--></span><span class="callout">7</span>
      annotations:
        message: '{{ $labels.container }} container of the {{ $labels.pod }} pod in the {{ $labels.namespace }} namespace consumes {{ $value | humanizePercentage }} of the samples limit budget.' <span id="CO44-8"><!--Empty--></span><span class="callout">8</span>
      expr: (scrape_samples_post_metric_relabeling / (scrape_sample_limit &gt; 0)) &gt; 0.9 <span id="CO44-9"><!--Empty--></span><span class="callout">9</span>
      for: 10m <span id="CO44-10"><!--Empty--></span><span class="callout">10</span>
      labels:
        severity: warning <span id="CO44-11"><!--Empty--></span><span class="callout">11</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO44-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Defines the name of the alerting rule.
									</div></dd><dt><a href="#CO44-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specifies the user-defined project where the alerting rule is deployed.
									</div></dd><dt><a href="#CO44-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The <code class="literal">TargetDown</code> alert fires if the target cannot be scraped and is not available for the <code class="literal">for</code> duration.
									</div></dd><dt><a href="#CO44-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										The message that is displayed when the <code class="literal">TargetDown</code> alert fires.
									</div></dd><dt><a href="#CO44-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										The conditions for the <code class="literal">TargetDown</code> alert must be true for this duration before the alert is fired.
									</div></dd><dt><a href="#CO44-6"><span class="callout">6</span></a> </dt><dd><div class="para">
										Defines the severity for the <code class="literal">TargetDown</code> alert.
									</div></dd><dt><a href="#CO44-7"><span class="callout">7</span></a> </dt><dd><div class="para">
										The <code class="literal">ApproachingEnforcedSamplesLimit</code> alert fires when the defined scrape sample threshold is exceeded and lasts for the specified <code class="literal">for</code> duration.
									</div></dd><dt><a href="#CO44-8"><span class="callout">8</span></a> </dt><dd><div class="para">
										The message that is displayed when the <code class="literal">ApproachingEnforcedSamplesLimit</code> alert fires.
									</div></dd><dt><a href="#CO44-9"><span class="callout">9</span></a> </dt><dd><div class="para">
										The threshold for the <code class="literal">ApproachingEnforcedSamplesLimit</code> alert. In this example, the alert fires when the number of ingested samples exceeds 90% of the configured limit.
									</div></dd><dt><a href="#CO44-10"><span class="callout">10</span></a> </dt><dd><div class="para">
										The conditions for the <code class="literal">ApproachingEnforcedSamplesLimit</code> alert must be true for this duration before the alert is fired.
									</div></dd><dt><a href="#CO44-11"><span class="callout">11</span></a> </dt><dd><div class="para">
										Defines the severity for the <code class="literal">ApproachingEnforcedSamplesLimit</code> alert.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Apply the configuration to the user-defined project:
							</p><pre class="programlisting language-terminal">$ oc apply -f monitoring-stack-alerts.yaml</pre></li><li class="listitem"><p class="simpara">
								Additionally, you can check if a target has hit the configured limit:
							</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
										In the <span class="strong strong"><strong>Administrator</strong></span> perspective of the web console, go to <span class="strong strong"><strong>Observe</strong></span> → <span class="strong strong"><strong>Targets</strong></span> and select an endpoint with a <code class="literal">Down</code> status that you want to check.
									</p><p class="simpara">
										The <span class="strong strong"><strong>Scrape failed: sample limit exceeded</strong></span> message is displayed if the endpoint failed because of an exceeded sample limit.
									</p></li></ol></div></li></ol></div></section></section><section class="section" id="configuring-pod-topology-spread-constraints_configuring-performance-and-scalability-uwm"><div class="titlepage"><div><div><h4 class="title">4.2.4. Configuring pod topology spread constraints</h4></div></div></div><p>
					You can configure pod topology spread constraints for all the pods for user-defined monitoring to control how pod replicas are scheduled to nodes across zones. This ensures that the pods are highly available and run more efficiently, because workloads are spread across nodes in different data centers or hierarchical infrastructure zones.
				</p><p>
					You can configure pod topology spread constraints for monitoring pods by using the <code class="literal">user-workload-monitoring-config</code> config map.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role or as a user with the <code class="literal">user-workload-monitoring-config-edit</code> role in the <code class="literal">openshift-user-workload-monitoring</code> project.
						</li><li class="listitem">
							A cluster administrator has enabled monitoring for user-defined projects.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">user-workload-monitoring-config</code> config map in the <code class="literal">openshift-user-workload-monitoring</code> project:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</pre></li><li class="listitem"><p class="simpara">
							Add the following settings under the <code class="literal">data/config.yaml</code> field to configure pod topology spread constraints:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    &lt;component&gt;: <span id="CO45-1"><!--Empty--></span><span class="callout">1</span>
      topologySpreadConstraints:
      - maxSkew: &lt;n&gt; <span id="CO45-2"><!--Empty--></span><span class="callout">2</span>
        topologyKey: &lt;key&gt; <span id="CO45-3"><!--Empty--></span><span class="callout">3</span>
        whenUnsatisfiable: &lt;value&gt; <span id="CO45-4"><!--Empty--></span><span class="callout">4</span>
        labelSelector: <span id="CO45-5"><!--Empty--></span><span class="callout">5</span>
          &lt;match_option&gt;</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO45-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify a name of the component for which you want to set up pod topology spread constraints.
								</div></dd><dt><a href="#CO45-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify a numeric value for <code class="literal">maxSkew</code>, which defines the degree to which pods are allowed to be unevenly distributed.
								</div></dd><dt><a href="#CO45-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									Specify a key of node labels for <code class="literal">topologyKey</code>. Nodes that have a label with this key and identical values are considered to be in the same topology. The scheduler tries to put a balanced number of pods into each domain.
								</div></dd><dt><a href="#CO45-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									Specify a value for <code class="literal">whenUnsatisfiable</code>. Available options are <code class="literal">DoNotSchedule</code> and <code class="literal">ScheduleAnyway</code>. Specify <code class="literal">DoNotSchedule</code> if you want the <code class="literal">maxSkew</code> value to define the maximum difference allowed between the number of matching pods in the target topology and the global minimum. Specify <code class="literal">ScheduleAnyway</code> if you want the scheduler to still schedule the pod but to give higher priority to nodes that might reduce the skew.
								</div></dd><dt><a href="#CO45-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									Specify <code class="literal">labelSelector</code> to find matching pods. Pods that match this label selector are counted to determine the number of pods in their corresponding topology domain.
								</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example configuration for Thanos Ruler</strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    thanosRuler:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: monitoring
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: thanos-ruler</pre>
<p></p></div></li><li class="listitem">
							Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#using-pod-topology-spread-constraints-for-monitoring_key-concepts" title="1.3.1.2. About pod topology spread constraints for monitoring">About pod topology spread constraints for monitoring</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/nodes/#nodes-scheduler-pod-topology-spread-constraints-about">Controlling pod placement by using pod topology spread constraints</a>
</li><li class="listitem">
<a class="link" href="https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/">Pod Topology Spread Constraints</a> (Kubernetes documentation)
						</li></ul></div></section></section><section class="section" id="storing-and-recording-data-uwm"><div class="titlepage"><div><div><h3 class="title">4.3. Storing and recording data for user workload monitoring</h3></div></div></div><p>
				Store and record your metrics and alerting data, configure logs to specify which activities are recorded, control how long Prometheus retains stored data, and set the maximum amount of disk space for the data. These actions help you protect your data and use them for troubleshooting.
			</p><section class="section" id="configuring-persistent-storage_storing-and-recording-data-uwm"><div class="titlepage"><div><div><h4 class="title">4.3.1. Configuring persistent storage</h4></div></div></div><p>
					Run cluster monitoring with persistent storage to gain the following benefits:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Protect your metrics and alerting data from data loss by storing them in a persistent volume (PV). As a result, they can survive pods being restarted or recreated.
						</li><li class="listitem">
							Avoid getting duplicate notifications and losing silences for alerts when the Alertmanager pods are restarted.
						</li></ul></div><p>
					For production environments, it is highly recommended to configure persistent storage.
				</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
						In multi-node clusters, you must configure persistent storage for Prometheus, Alertmanager, and Thanos Ruler to ensure high availability.
					</p></div></rh-alert><section class="section" id="persistent-storage-prerequisites_storing-and-recording-data-uwm"><div class="titlepage"><div><div><h5 class="title">4.3.1.1. Persistent storage prerequisites</h5></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Dedicate sufficient persistent storage to ensure that the disk does not become full.
							</li><li class="listitem"><p class="simpara">
								Use <code class="literal">Filesystem</code> as the storage type value for the <code class="literal">volumeMode</code> parameter when you configure the persistent volume.
							</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
											Do not use a raw block volume, which is described with <code class="literal">volumeMode: Block</code> in the <code class="literal">PersistentVolume</code> resource. Prometheus cannot use raw block volumes.
										</li><li class="listitem">
											Prometheus does not support file systems that are not POSIX compliant. For example, some NFS file system implementations are not POSIX compliant. If you want to use an NFS file system for storage, verify with the vendor that their NFS implementation is fully POSIX compliant.
										</li></ul></div></div></rh-alert></li></ul></div></section><section class="section" id="configuring-a-persistent-volume-claim_storing-and-recording-data-uwm"><div class="titlepage"><div><div><h5 class="title">4.3.1.2. Configuring a persistent volume claim</h5></div></div></div><p>
						To use a persistent volume (PV) for monitoring components, you must configure a persistent volume claim (PVC).
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role, or as a user with the <code class="literal">user-workload-monitoring-config-edit</code> role in the <code class="literal">openshift-user-workload-monitoring</code> project.
							</li><li class="listitem">
								A cluster administrator has enabled monitoring for user-defined projects.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Edit the <code class="literal">user-workload-monitoring-config</code> config map in the <code class="literal">openshift-user-workload-monitoring</code> project:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</pre></li><li class="listitem"><p class="simpara">
								Add your PVC configuration for the component under <code class="literal">data/config.yaml</code>:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    &lt;component&gt;: <span id="CO46-1"><!--Empty--></span><span class="callout">1</span>
      volumeClaimTemplate:
        spec:
          storageClassName: &lt;storage_class&gt; <span id="CO46-2"><!--Empty--></span><span class="callout">2</span>
          resources:
            requests:
              storage: &lt;amount_of_storage&gt; <span id="CO46-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO46-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specify the monitoring component for which you want to configure the PVC.
									</div></dd><dt><a href="#CO46-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specify an existing storage class. If a storage class is not specified, the default storage class is used.
									</div></dd><dt><a href="#CO46-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Specify the amount of required storage.
									</div></dd></dl></div><p class="simpara">
								The following example configures a PVC that claims persistent storage for Thanos Ruler:
							</p><div class="formalpara"><p class="title"><strong>Example PVC configuration</strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    thanosRuler:
      volumeClaimTemplate:
        spec:
          storageClassName: my-storage-class
          resources:
            requests:
              storage: 10Gi</pre>
<p></p></div><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
									Storage requirements for the <code class="literal">thanosRuler</code> component depend on the number of rules that are evaluated and how many samples each rule generates.
								</p></div></rh-alert></li><li class="listitem"><p class="simpara">
								Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed and the new storage configuration is applied.
							</p><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
									When you update the config map with a PVC configuration, the affected <code class="literal">StatefulSet</code> object is recreated, resulting in a temporary service outage.
								</p></div></rh-alert></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/storage/#understanding-persistent-storage">Understanding persistent storage</a>
</li><li class="listitem">
<a class="link" href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">PersistentVolumeClaims</a> (Kubernetes documentation)
							</li></ul></div></section><section class="section" id="resizing-a-persistent-volume_storing-and-recording-data-uwm"><div class="titlepage"><div><div><h5 class="title">4.3.1.3. Resizing a persistent volume</h5></div></div></div><p>
						You can resize a persistent volume (PV) for the instances of Prometheus, Thanos Ruler, and Alertmanager. You need to manually expand a persistent volume claim (PVC), and then update the config map in which the component is configured.
					</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
							You can only expand the size of the PVC. Shrinking the storage size is not possible.
						</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role, or as a user with the <code class="literal">user-workload-monitoring-config-edit</code> role in the <code class="literal">openshift-user-workload-monitoring</code> project.
							</li><li class="listitem">
								A cluster administrator has enabled monitoring for user-defined projects.
							</li><li class="listitem">
								You have configured at least one PVC for components that monitor user-defined projects.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Manually expand a PVC with the updated storage request. For more information, see "Expanding persistent volume claims (PVCs) with a file system" in <span class="emphasis"><em>Expanding persistent volumes</em></span>.
							</li><li class="listitem"><p class="simpara">
								Edit the <code class="literal">user-workload-monitoring-config</code> config map in the <code class="literal">openshift-user-workload-monitoring</code> project:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</pre></li><li class="listitem"><p class="simpara">
								Add a new storage size for the PVC configuration for the component under <code class="literal">data/config.yaml</code>:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    &lt;component&gt;: <span id="CO47-1"><!--Empty--></span><span class="callout">1</span>
      volumeClaimTemplate:
        spec:
          resources:
            requests:
              storage: &lt;amount_of_storage&gt; <span id="CO47-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO47-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The component for which you want to change the storage size.
									</div></dd><dt><a href="#CO47-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specify the new size for the storage volume. It must be greater than the previous value.
									</div></dd></dl></div><p class="simpara">
								The following example sets the new PVC request to 20 gigabytes for Thanos Ruler:
							</p><div class="formalpara"><p class="title"><strong>Example storage configuration for <code class="literal">thanosRuler</code></strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    thanosRuler:
      volumeClaimTemplate:
        spec:
          resources:
            requests:
              storage: 20Gi</pre>
<p></p></div><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
									Storage requirements for the <code class="literal">thanosRuler</code> component depend on the number of rules that are evaluated and how many samples each rule generates.
								</p></div></rh-alert></li><li class="listitem"><p class="simpara">
								Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.
							</p><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
									When you update the config map with a new storage size, the affected <code class="literal">StatefulSet</code> object is recreated, resulting in a temporary service outage.
								</p></div></rh-alert></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/scalability_and_performance/#prometheus-database-storage-requirements_recommended-infrastructure-practices">Prometheus database storage requirements</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/storage/#expanding-pvc-filesystem_expanding-persistent-volumes">Expanding persistent volume claims (PVCs) with a file system</a>
</li></ul></div></section></section><section class="section" id="modifying-retention-time-and-size-for-prometheus-metrics-data_storing-and-recording-data-uwm"><div class="titlepage"><div><div><h4 class="title">4.3.2. Modifying retention time and size for Prometheus metrics data</h4></div></div></div><p>
					By default, Prometheus retains metrics data for 24 hours for monitoring for user-defined projects. You can modify the retention time for the Prometheus instance to change when the data is deleted. You can also set the maximum amount of disk space the retained metrics data uses.
				</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						Data compaction occurs every two hours. Therefore, a persistent volume (PV) might fill up before compaction, potentially exceeding the <code class="literal">retentionSize</code> limit. In such cases, the <code class="literal">KubePersistentVolumeFillingUp</code> alert fires until the space on a PV is lower than the <code class="literal">retentionSize</code> limit.
					</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role, or as a user with the <code class="literal">user-workload-monitoring-config-edit</code> role in the <code class="literal">openshift-user-workload-monitoring</code> project.
						</li><li class="listitem">
							A cluster administrator has enabled monitoring for user-defined projects.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">user-workload-monitoring-config</code> config map in the <code class="literal">openshift-user-workload-monitoring</code> project:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</pre></li><li class="listitem"><p class="simpara">
							Add the retention time and size configuration under <code class="literal">data/config.yaml</code>:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      retention: &lt;time_specification&gt; <span id="CO48-1"><!--Empty--></span><span class="callout">1</span>
      retentionSize: &lt;size_specification&gt; <span id="CO48-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO48-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The retention time: a number directly followed by <code class="literal">ms</code> (milliseconds), <code class="literal">s</code> (seconds), <code class="literal">m</code> (minutes), <code class="literal">h</code> (hours), <code class="literal">d</code> (days), <code class="literal">w</code> (weeks), or <code class="literal">y</code> (years). You can also combine time values for specific times, such as <code class="literal">1h30m15s</code>.
								</div></dd><dt><a href="#CO48-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The retention size: a number directly followed by <code class="literal">B</code> (bytes), <code class="literal">KB</code> (kilobytes), <code class="literal">MB</code> (megabytes), <code class="literal">GB</code> (gigabytes), <code class="literal">TB</code> (terabytes), <code class="literal">PB</code> (petabytes), and <code class="literal">EB</code> (exabytes).
								</div></dd></dl></div><p class="simpara">
							The following example sets the retention time to 24 hours and the retention size to 10 gigabytes for the Prometheus instance:
						</p><div class="formalpara"><p class="title"><strong>Example of setting retention time for Prometheus</strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      retention: 24h
      retentionSize: 10GB</pre>
<p></p></div></li><li class="listitem">
							Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.
						</li></ol></div><section class="section" id="modifying-the-retention-time-for-thanos-ruler-metrics-data_storing-and-recording-data-uwm"><div class="titlepage"><div><div><h5 class="title">4.3.2.1. Modifying the retention time for Thanos Ruler metrics data</h5></div></div></div><p>
						By default, for user-defined projects, Thanos Ruler automatically retains metrics data for 24 hours. You can modify the retention time to change how long this data is retained by specifying a time value in the <code class="literal">user-workload-monitoring-config</code> config map in the <code class="literal">openshift-user-workload-monitoring</code> namespace.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role or as a user with the <code class="literal">user-workload-monitoring-config-edit</code> role in the <code class="literal">openshift-user-workload-monitoring</code> project.
							</li><li class="listitem">
								A cluster administrator has enabled monitoring for user-defined projects.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Edit the <code class="literal">user-workload-monitoring-config</code> <code class="literal">ConfigMap</code> object in the <code class="literal">openshift-user-workload-monitoring</code> project:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</pre></li><li class="listitem"><p class="simpara">
								Add the retention time configuration under <code class="literal">data/config.yaml</code>:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    thanosRuler:
      retention: &lt;time_specification&gt; <span id="CO49-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO49-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specify the retention time in the following format: a number directly followed by <code class="literal">ms</code> (milliseconds), <code class="literal">s</code> (seconds), <code class="literal">m</code> (minutes), <code class="literal">h</code> (hours), <code class="literal">d</code> (days), <code class="literal">w</code> (weeks), or <code class="literal">y</code> (years). You can also combine time values for specific times, such as <code class="literal">1h30m15s</code>. The default is <code class="literal">24h</code>.
									</div></dd></dl></div><p class="simpara">
								The following example sets the retention time to 10 days for Thanos Ruler data:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    thanosRuler:
      retention: 10d</pre></li><li class="listitem">
								Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.
							</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#retention-time-and-size-for-prometheus-metrics-data_key-concepts" title="1.3.2.1. Retention time and size for Prometheus metrics">Retention time and size for Prometheus metrics</a>
</li><li class="listitem">
<a class="link" href="#enabling-monitoring-for-user-defined-projects-uwm_preparing-to-configure-the-monitoring-stack-uwm" title="4.1.2. Enabling monitoring for user-defined projects">Enabling monitoring for user-defined projects</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/scalability_and_performance/#prometheus-database-storage-requirements_cluster-monitoring-operator">Prometheus database storage requirements</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/scalability_and_performance/#optimizing-storage">Recommended configurable storage technology</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/storage/#understanding-persistent-storage">Understanding persistent storage</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/scalability_and_performance/#optimizing-storage">Optimizing storage</a>
</li></ul></div></section></section><section class="section" id="setting-log-levels-for-monitoring-components_storing-and-recording-data-uwm"><div class="titlepage"><div><div><h4 class="title">4.3.3. Setting log levels for monitoring components</h4></div></div></div><p>
					You can configure the log level for Alertmanager, Prometheus Operator, Prometheus, and Thanos Ruler.
				</p><p>
					The following log levels can be applied to the relevant component in the <code class="literal">user-workload-monitoring-config</code> <code class="literal">ConfigMap</code> object:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<code class="literal">debug</code>. Log debug, informational, warning, and error messages.
						</li><li class="listitem">
<code class="literal">info</code>. Log informational, warning, and error messages.
						</li><li class="listitem">
<code class="literal">warn</code>. Log warning and error messages only.
						</li><li class="listitem">
<code class="literal">error</code>. Log error messages only.
						</li></ul></div><p>
					The default log level is <code class="literal">info</code>.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role or as a user with the <code class="literal">user-workload-monitoring-config-edit</code> role in the <code class="literal">openshift-user-workload-monitoring</code> project.
						</li><li class="listitem">
							A cluster administrator has enabled monitoring for user-defined projects.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">user-workload-monitoring-config</code> config map in the <code class="literal">openshift-user-workload-monitoring</code> project:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</pre></li><li class="listitem"><p class="simpara">
							Add <code class="literal">logLevel: &lt;log_level&gt;</code> for a component under <code class="literal">data/config.yaml</code>:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    &lt;component&gt;: <span id="CO50-1"><!--Empty--></span><span class="callout">1</span>
      logLevel: &lt;log_level&gt; <span id="CO50-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO50-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The monitoring stack component for which you are setting a log level. Available component values are <code class="literal">prometheus</code>, <code class="literal">alertmanager</code>, <code class="literal">prometheusOperator</code>, and <code class="literal">thanosRuler</code>.
								</div></dd><dt><a href="#CO50-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The log level to set for the component. The available values are <code class="literal">error</code>, <code class="literal">warn</code>, <code class="literal">info</code>, and <code class="literal">debug</code>. The default value is <code class="literal">info</code>.
								</div></dd></dl></div></li><li class="listitem">
							Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.
						</li><li class="listitem"><p class="simpara">
							Confirm that the log level has been applied by reviewing the deployment or pod configuration in the related project. The following example checks the log level for the <code class="literal">prometheus-operator</code> deployment:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring get deploy prometheus-operator -o yaml | grep "log-level"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
</p><pre class="programlisting language-terminal">        - --log-level=debug</pre>
<p></p></div></li><li class="listitem"><p class="simpara">
							Check that the pods for the component are running. The following example lists the status of pods:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring get pods</pre><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
								If an unrecognized <code class="literal">logLevel</code> value is included in the <code class="literal">ConfigMap</code> object, the pods for the component might not restart successfully.
							</p></div></rh-alert></li></ol></div></section><section class="section" id="setting-query-log-file-for-prometheus_storing-and-recording-data-uwm"><div class="titlepage"><div><div><h4 class="title">4.3.4. Enabling the query log file for Prometheus</h4></div></div></div><p>
					You can configure Prometheus to write all queries that have been run by the engine to a log file.
				</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
						Because log rotation is not supported, only enable this feature temporarily when you need to troubleshoot an issue. After you finish troubleshooting, disable query logging by reverting the changes you made to the <code class="literal">ConfigMap</code> object to enable the feature.
					</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role or as a user with the <code class="literal">user-workload-monitoring-config-edit</code> role in the <code class="literal">openshift-user-workload-monitoring</code> project.
						</li><li class="listitem">
							A cluster administrator has enabled monitoring for user-defined projects.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">user-workload-monitoring-config</code> config map in the <code class="literal">openshift-user-workload-monitoring</code> project:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</pre></li><li class="listitem"><p class="simpara">
							Add the <code class="literal">queryLogFile</code> parameter for Prometheus under <code class="literal">data/config.yaml</code>:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      queryLogFile: &lt;path&gt; <span id="CO51-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO51-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Add the full path to the file in which queries will be logged.
								</div></dd></dl></div></li><li class="listitem">
							Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.
						</li><li class="listitem"><p class="simpara">
							Verify that the pods for the component are running. The following sample command lists the status of pods:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring get pods</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
</p><pre class="programlisting language-terminal">...
prometheus-operator-776fcbbd56-2nbfm   2/2     Running   0          132m
prometheus-user-workload-0             5/5     Running   1          132m
prometheus-user-workload-1             5/5     Running   1          132m
thanos-ruler-user-workload-0           3/3     Running   0          132m
thanos-ruler-user-workload-1           3/3     Running   0          132m
...</pre>
<p></p></div></li><li class="listitem"><p class="simpara">
							Read the query log:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring exec prometheus-user-workload-0 -- cat &lt;path&gt;</pre><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
								Revert the setting in the config map after you have examined the logged query information.
							</p></div></rh-alert></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#enabling-monitoring-for-user-defined-projects-uwm_preparing-to-configure-the-monitoring-stack-uwm" title="4.1.2. Enabling monitoring for user-defined projects">Enabling monitoring for user-defined projects</a>
</li></ul></div></section></section><section class="section" id="configuring-metrics-uwm"><div class="titlepage"><div><div><h3 class="title">4.4. Configuring metrics for user workload monitoring</h3></div></div></div><p>
				Configure the collection of metrics to monitor how cluster components and your own workloads are performing.
			</p><p>
				You can send ingested metrics to remote systems for long-term storage and add cluster ID labels to the metrics to identify the data coming from different clusters.
			</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#understanding-metrics_key-concepts" title="1.3.3. Understanding metrics">Understanding metrics</a>
</li></ul></div><section class="section" id="configuring-remote-write-storage_configuring-metrics-uwm"><div class="titlepage"><div><div><h4 class="title">4.4.1. Configuring remote write storage</h4></div></div></div><p>
					You can configure remote write storage to enable Prometheus to send ingested metrics to remote systems for long-term storage. Doing so has no impact on how or for how long Prometheus stores metrics.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role or as a user with the <code class="literal">user-workload-monitoring-config-edit</code> role in the <code class="literal">openshift-user-workload-monitoring</code> project.
						</li><li class="listitem">
							A cluster administrator has enabled monitoring for user-defined projects.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem"><p class="simpara">
							You have set up a remote write compatible endpoint (such as Thanos) and know the endpoint URL. See the <a class="link" href="https://prometheus.io/docs/operating/integrations/#remote-endpoints-and-storage">Prometheus remote endpoints and storage documentation</a> for information about endpoints that are compatible with the remote write feature.
						</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
								Red Hat only provides information for configuring remote write senders and does not offer guidance on configuring receiver endpoints. Customers are responsible for setting up their own endpoints that are remote-write compatible. Issues with endpoint receiver configurations are not included in Red Hat production support.
							</p></div></rh-alert></li><li class="listitem"><p class="simpara">
							You have set up authentication credentials in a <code class="literal">Secret</code> object for the remote write endpoint. You must create the secret in the <code class="literal">openshift-user-workload-monitoring</code> namespace.
						</p><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><p>
								To reduce security risks, use HTTPS and authentication to send metrics to an endpoint.
							</p></div></rh-alert></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">user-workload-monitoring-config</code> config map in the <code class="literal">openshift-user-workload-monitoring</code> project:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</pre></li><li class="listitem"><p class="simpara">
							Add a <code class="literal">remoteWrite:</code> section under <code class="literal">data/config.yaml/prometheus</code>, as shown in the following example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com" <span id="CO52-1"><!--Empty--></span><span class="callout">1</span>
        &lt;endpoint_authentication_credentials&gt; <span id="CO52-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO52-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The URL of the remote write endpoint.
								</div></dd><dt><a href="#CO52-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The authentication method and credentials for the endpoint. Currently supported authentication methods are AWS Signature Version 4, authentication using HTTP in an <code class="literal">Authorization</code> request header, Basic authentication, OAuth 2.0, and TLS client. See <span class="emphasis"><em>Supported remote write authentication settings</em></span> for sample configurations of supported authentication methods.
								</div></dd></dl></div></li><li class="listitem"><p class="simpara">
							Add write relabel configuration values after the authentication credentials:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        &lt;endpoint_authentication_credentials&gt;
        writeRelabelConfigs:
        - &lt;your_write_relabel_configs&gt; <span id="CO53-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO53-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Add configuration for metrics that you want to send to the remote endpoint.
								</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example of forwarding a single metric called <code class="literal">my_metric</code></strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        writeRelabelConfigs:
        - sourceLabels: [__name__]
          regex: 'my_metric'
          action: keep</pre>
<p></p></div><div class="formalpara"><p class="title"><strong>Example of forwarding metrics called <code class="literal">my_metric_1</code> and <code class="literal">my_metric_2</code> in <code class="literal">my_namespace</code> namespace</strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        writeRelabelConfigs:
        - sourceLabels: [__name__,namespace]
          regex: '(my_metric_1|my_metric_2);my_namespace'
          action: keep</pre>
<p></p></div></li><li class="listitem">
							Save the file to apply the changes. The new configuration is applied automatically.
						</li></ol></div><section class="section" id="supported-remote-write-authentication-settings_configuring-metrics-uwm"><div class="titlepage"><div><div><h5 class="title">4.4.1.1. Supported remote write authentication settings</h5></div></div></div><p>
						You can use different methods to authenticate with a remote write endpoint. Currently supported authentication methods are AWS Signature Version 4, basic authentication, authorization, OAuth 2.0, and TLS client. The following table provides details about supported authentication methods for use with remote write.
					</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059285393600" scope="col" valign="top">Authentication method</th><th align="left" id="idm140059285392512" scope="col" valign="top">Config map field</th><th align="left" id="idm140059285391424" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059285393600" valign="top"> <p>
										AWS Signature Version 4
									</p>
</td><td align="left" headers="idm140059285392512" valign="top"> <p>
<code class="literal">sigv4</code>
</p>
</td><td align="left" headers="idm140059285391424" valign="top"> <p>
										This method uses AWS Signature Version 4 authentication to sign requests. You cannot use this method simultaneously with authorization, OAuth 2.0, or Basic authentication.
									</p>
</td></tr><tr><td align="left" headers="idm140059285393600" valign="top"> <p>
										Basic authentication
									</p>
</td><td align="left" headers="idm140059285392512" valign="top"> <p>
<code class="literal">basicAuth</code>
</p>
</td><td align="left" headers="idm140059285391424" valign="top"> <p>
										Basic authentication sets the authorization header on every remote write request with the configured username and password.
									</p>
</td></tr><tr><td align="left" headers="idm140059285393600" valign="top"> <p>
										authorization
									</p>
</td><td align="left" headers="idm140059285392512" valign="top"> <p>
<code class="literal">authorization</code>
</p>
</td><td align="left" headers="idm140059285391424" valign="top"> <p>
										Authorization sets the <code class="literal">Authorization</code> header on every remote write request using the configured token.
									</p>
</td></tr><tr><td align="left" headers="idm140059285393600" valign="top"> <p>
										OAuth 2.0
									</p>
</td><td align="left" headers="idm140059285392512" valign="top"> <p>
<code class="literal">oauth2</code>
</p>
</td><td align="left" headers="idm140059285391424" valign="top"> <p>
										An OAuth 2.0 configuration uses the client credentials grant type. Prometheus fetches an access token from <code class="literal">tokenUrl</code> with the specified client ID and client secret to access the remote write endpoint. You cannot use this method simultaneously with authorization, AWS Signature Version 4, or Basic authentication.
									</p>
</td></tr><tr><td align="left" headers="idm140059285393600" valign="top"> <p>
										TLS client
									</p>
</td><td align="left" headers="idm140059285392512" valign="top"> <p>
<code class="literal">tlsConfig</code>
</p>
</td><td align="left" headers="idm140059285391424" valign="top"> <p>
										A TLS client configuration specifies the CA certificate, the client certificate, and the client key file information used to authenticate with the remote write endpoint server using TLS. The sample configuration assumes that you have already created a CA certificate file, a client certificate file, and a client key file.
									</p>
</td></tr></tbody></table></rh-table></section><section class="section" id="example-remote-write-authentication-settings_configuring-metrics-uwm"><div class="titlepage"><div><div><h5 class="title">4.4.1.2. Example remote write authentication settings</h5></div></div></div><p>
						The following samples show different authentication settings you can use to connect to a remote write endpoint. Each sample also shows how to configure a corresponding <code class="literal">Secret</code> object that contains authentication credentials and other relevant settings. Each sample configures authentication for use with monitoring for user-defined projects in the <code class="literal">openshift-user-workload-monitoring</code> namespace.
					</p><section class="section" id="remote-write-sample-yaml-aws-sigv4_configuring-metrics-uwm"><div class="titlepage"><div><div><h5 class="title">4.4.1.2.1. Sample YAML for AWS Signature Version 4 authentication</h5></div></div></div><p>
							The following shows the settings for a <code class="literal">sigv4</code> secret named <code class="literal">sigv4-credentials</code> in the <code class="literal">openshift-user-workload-monitoring</code> namespace.
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: sigv4-credentials
  namespace: openshift-user-workload-monitoring
stringData:
  accessKey: &lt;AWS_access_key&gt; <span id="CO54-1"><!--Empty--></span><span class="callout">1</span>
  secretKey: &lt;AWS_secret_key&gt; <span id="CO54-2"><!--Empty--></span><span class="callout">2</span>
type: Opaque</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO54-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The AWS API access key.
								</div></dd><dt><a href="#CO54-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The AWS API secret key.
								</div></dd></dl></div><p>
							The following shows sample AWS Signature Version 4 remote write authentication settings that use a <code class="literal">Secret</code> object named <code class="literal">sigv4-credentials</code> in the <code class="literal">openshift-user-workload-monitoring</code> namespace:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      remoteWrite:
      - url: "https://authorization.example.com/api/write"
        sigv4:
          region: &lt;AWS_region&gt; <span id="CO55-1"><!--Empty--></span><span class="callout">1</span>
          accessKey:
            name: sigv4-credentials <span id="CO55-2"><!--Empty--></span><span class="callout">2</span>
            key: accessKey <span id="CO55-3"><!--Empty--></span><span class="callout">3</span>
          secretKey:
            name: sigv4-credentials <span id="CO55-4"><!--Empty--></span><span class="callout">4</span>
            key: secretKey <span id="CO55-5"><!--Empty--></span><span class="callout">5</span>
          profile: &lt;AWS_profile_name&gt; <span id="CO55-6"><!--Empty--></span><span class="callout">6</span>
          roleArn: &lt;AWS_role_arn&gt; <span id="CO55-7"><!--Empty--></span><span class="callout">7</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO55-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The AWS region.
								</div></dd><dt><a href="#CO55-2"><span class="callout">2</span></a> <a href="#CO55-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									The name of the <code class="literal">Secret</code> object containing the AWS API access credentials.
								</div></dd><dt><a href="#CO55-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The key that contains the AWS API access key in the specified <code class="literal">Secret</code> object.
								</div></dd><dt><a href="#CO55-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									The key that contains the AWS API secret key in the specified <code class="literal">Secret</code> object.
								</div></dd><dt><a href="#CO55-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									The name of the AWS profile that is being used to authenticate.
								</div></dd><dt><a href="#CO55-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									The unique identifier for the Amazon Resource Name (ARN) assigned to your role.
								</div></dd></dl></div></section><section class="section" id="remote-write-sample-yaml-basic-auth_configuring-metrics-uwm"><div class="titlepage"><div><div><h5 class="title">4.4.1.2.2. Sample YAML for Basic authentication</h5></div></div></div><p>
							The following shows sample Basic authentication settings for a <code class="literal">Secret</code> object named <code class="literal">rw-basic-auth</code> in the <code class="literal">openshift-user-workload-monitoring</code> namespace:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: rw-basic-auth
  namespace: openshift-user-workload-monitoring
stringData:
  user: &lt;basic_username&gt; <span id="CO56-1"><!--Empty--></span><span class="callout">1</span>
  password: &lt;basic_password&gt; <span id="CO56-2"><!--Empty--></span><span class="callout">2</span>
type: Opaque</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO56-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The username.
								</div></dd><dt><a href="#CO56-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The password.
								</div></dd></dl></div><p>
							The following sample shows a <code class="literal">basicAuth</code> remote write configuration that uses a <code class="literal">Secret</code> object named <code class="literal">rw-basic-auth</code> in the <code class="literal">openshift-user-workload-monitoring</code> namespace. It assumes that you have already set up authentication credentials for the endpoint.
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      remoteWrite:
      - url: "https://basicauth.example.com/api/write"
        basicAuth:
          username:
            name: rw-basic-auth <span id="CO57-1"><!--Empty--></span><span class="callout">1</span>
            key: user <span id="CO57-2"><!--Empty--></span><span class="callout">2</span>
          password:
            name: rw-basic-auth <span id="CO57-3"><!--Empty--></span><span class="callout">3</span>
            key: password <span id="CO57-4"><!--Empty--></span><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO57-1"><span class="callout">1</span></a> <a href="#CO57-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The name of the <code class="literal">Secret</code> object that contains the authentication credentials.
								</div></dd><dt><a href="#CO57-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The key that contains the username in the specified <code class="literal">Secret</code> object.
								</div></dd><dt><a href="#CO57-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									The key that contains the password in the specified <code class="literal">Secret</code> object.
								</div></dd></dl></div></section><section class="section" id="remote-write-sample-yaml-bearer-token_configuring-metrics-uwm"><div class="titlepage"><div><div><h5 class="title">4.4.1.2.3. Sample YAML for authentication with a bearer token using a <code class="literal">Secret</code> Object</h5></div></div></div><p>
							The following shows bearer token settings for a <code class="literal">Secret</code> object named <code class="literal">rw-bearer-auth</code> in the <code class="literal">openshift-user-workload-monitoring</code> namespace:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: rw-bearer-auth
  namespace: openshift-user-workload-monitoring
stringData:
  token: &lt;authentication_token&gt; <span id="CO58-1"><!--Empty--></span><span class="callout">1</span>
type: Opaque</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO58-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The authentication token.
								</div></dd></dl></div><p>
							The following shows sample bearer token config map settings that use a <code class="literal">Secret</code> object named <code class="literal">rw-bearer-auth</code> in the <code class="literal">openshift-user-workload-monitoring</code> namespace:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    enableUserWorkload: true
    prometheus:
      remoteWrite:
      - url: "https://authorization.example.com/api/write"
        authorization:
          type: Bearer <span id="CO59-1"><!--Empty--></span><span class="callout">1</span>
          credentials:
            name: rw-bearer-auth <span id="CO59-2"><!--Empty--></span><span class="callout">2</span>
            key: token <span id="CO59-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO59-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The authentication type of the request. The default value is <code class="literal">Bearer</code>.
								</div></dd><dt><a href="#CO59-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The name of the <code class="literal">Secret</code> object that contains the authentication credentials.
								</div></dd><dt><a href="#CO59-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The key that contains the authentication token in the specified <code class="literal">Secret</code> object.
								</div></dd></dl></div></section><section class="section" id="remote-write-sample-yaml-oauth-20_configuring-metrics-uwm"><div class="titlepage"><div><div><h5 class="title">4.4.1.2.4. Sample YAML for OAuth 2.0 authentication</h5></div></div></div><p>
							The following shows sample OAuth 2.0 settings for a <code class="literal">Secret</code> object named <code class="literal">oauth2-credentials</code> in the <code class="literal">openshift-user-workload-monitoring</code> namespace:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: oauth2-credentials
  namespace: openshift-user-workload-monitoring
stringData:
  id: &lt;oauth2_id&gt; <span id="CO60-1"><!--Empty--></span><span class="callout">1</span>
  secret: &lt;oauth2_secret&gt; <span id="CO60-2"><!--Empty--></span><span class="callout">2</span>
type: Opaque</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO60-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The Oauth 2.0 ID.
								</div></dd><dt><a href="#CO60-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The OAuth 2.0 secret.
								</div></dd></dl></div><p>
							The following shows an <code class="literal">oauth2</code> remote write authentication sample configuration that uses a <code class="literal">Secret</code> object named <code class="literal">oauth2-credentials</code> in the <code class="literal">openshift-user-workload-monitoring</code> namespace:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      remoteWrite:
      - url: "https://test.example.com/api/write"
        oauth2:
          clientId:
            secret:
              name: oauth2-credentials <span id="CO61-1"><!--Empty--></span><span class="callout">1</span>
              key: id <span id="CO61-2"><!--Empty--></span><span class="callout">2</span>
          clientSecret:
            name: oauth2-credentials <span id="CO61-3"><!--Empty--></span><span class="callout">3</span>
            key: secret <span id="CO61-4"><!--Empty--></span><span class="callout">4</span>
          tokenUrl: https://example.com/oauth2/token <span id="CO61-5"><!--Empty--></span><span class="callout">5</span>
          scopes: <span id="CO61-6"><!--Empty--></span><span class="callout">6</span>
          - &lt;scope_1&gt;
          - &lt;scope_2&gt;
          endpointParams: <span id="CO61-7"><!--Empty--></span><span class="callout">7</span>
            param1: &lt;parameter_1&gt;
            param2: &lt;parameter_2&gt;</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO61-1"><span class="callout">1</span></a> <a href="#CO61-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The name of the corresponding <code class="literal">Secret</code> object. Note that <code class="literal">ClientId</code> can alternatively refer to a <code class="literal">ConfigMap</code> object, although <code class="literal">clientSecret</code> must refer to a <code class="literal">Secret</code> object.
								</div></dd><dt><a href="#CO61-2"><span class="callout">2</span></a> <a href="#CO61-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									The key that contains the OAuth 2.0 credentials in the specified <code class="literal">Secret</code> object.
								</div></dd><dt><a href="#CO61-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									The URL used to fetch a token with the specified <code class="literal">clientId</code> and <code class="literal">clientSecret</code>.
								</div></dd><dt><a href="#CO61-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									The OAuth 2.0 scopes for the authorization request. These scopes limit what data the tokens can access.
								</div></dd><dt><a href="#CO61-7"><span class="callout">7</span></a> </dt><dd><div class="para">
									The OAuth 2.0 authorization request parameters required for the authorization server.
								</div></dd></dl></div></section><section class="section" id="remote-write-sample-yaml-tls_configuring-metrics-uwm"><div class="titlepage"><div><div><h5 class="title">4.4.1.2.5. Sample YAML for TLS client authentication</h5></div></div></div><p>
							The following shows sample TLS client settings for a <code class="literal">tls</code> <code class="literal">Secret</code> object named <code class="literal">mtls-bundle</code> in the <code class="literal">openshift-user-workload-monitoring</code> namespace.
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: mtls-bundle
  namespace: openshift-user-workload-monitoring
data:
  ca.crt: &lt;ca_cert&gt; <span id="CO62-1"><!--Empty--></span><span class="callout">1</span>
  client.crt: &lt;client_cert&gt; <span id="CO62-2"><!--Empty--></span><span class="callout">2</span>
  client.key: &lt;client_key&gt; <span id="CO62-3"><!--Empty--></span><span class="callout">3</span>
type: tls</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO62-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The CA certificate in the Prometheus container with which to validate the server certificate.
								</div></dd><dt><a href="#CO62-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The client certificate for authentication with the server.
								</div></dd><dt><a href="#CO62-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The client key.
								</div></dd></dl></div><p>
							The following sample shows a <code class="literal">tlsConfig</code> remote write authentication configuration that uses a TLS <code class="literal">Secret</code> object named <code class="literal">mtls-bundle</code>.
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        tlsConfig:
          ca:
            secret:
              name: mtls-bundle <span id="CO63-1"><!--Empty--></span><span class="callout">1</span>
              key: ca.crt <span id="CO63-2"><!--Empty--></span><span class="callout">2</span>
          cert:
            secret:
              name: mtls-bundle <span id="CO63-3"><!--Empty--></span><span class="callout">3</span>
              key: client.crt <span id="CO63-4"><!--Empty--></span><span class="callout">4</span>
          keySecret:
            name: mtls-bundle <span id="CO63-5"><!--Empty--></span><span class="callout">5</span>
            key: client.key <span id="CO63-6"><!--Empty--></span><span class="callout">6</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO63-1"><span class="callout">1</span></a> <a href="#CO63-3"><span class="callout">3</span></a> <a href="#CO63-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									The name of the corresponding <code class="literal">Secret</code> object that contains the TLS authentication credentials. Note that <code class="literal">ca</code> and <code class="literal">cert</code> can alternatively refer to a <code class="literal">ConfigMap</code> object, though <code class="literal">keySecret</code> must refer to a <code class="literal">Secret</code> object.
								</div></dd><dt><a href="#CO63-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The key in the specified <code class="literal">Secret</code> object that contains the CA certificate for the endpoint.
								</div></dd><dt><a href="#CO63-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									The key in the specified <code class="literal">Secret</code> object that contains the client certificate for the endpoint.
								</div></dd><dt><a href="#CO63-6"><span class="callout">6</span></a> </dt><dd><div class="para">
									The key in the specified <code class="literal">Secret</code> object that contains the client key secret.
								</div></dd></dl></div></section></section><section class="section" id="example-remote-write-queue-configuration_configuring-metrics-uwm"><div class="titlepage"><div><div><h5 class="title">4.4.1.3. Example remote write queue configuration</h5></div></div></div><p>
						You can use the <code class="literal">queueConfig</code> object for remote write to tune the remote write queue parameters. The following example shows the queue parameters with their default values for monitoring for user-defined projects in the <code class="literal">openshift-user-workload-monitoring</code> namespace.
					</p><div class="formalpara"><p class="title"><strong>Example configuration of remote write parameters with default values</strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        &lt;endpoint_authentication_credentials&gt;
        queueConfig:
          capacity: 10000 <span id="CO64-1"><!--Empty--></span><span class="callout">1</span>
          minShards: 1 <span id="CO64-2"><!--Empty--></span><span class="callout">2</span>
          maxShards: 50 <span id="CO64-3"><!--Empty--></span><span class="callout">3</span>
          maxSamplesPerSend: 2000 <span id="CO64-4"><!--Empty--></span><span class="callout">4</span>
          batchSendDeadline: 5s <span id="CO64-5"><!--Empty--></span><span class="callout">5</span>
          minBackoff: 30ms <span id="CO64-6"><!--Empty--></span><span class="callout">6</span>
          maxBackoff: 5s <span id="CO64-7"><!--Empty--></span><span class="callout">7</span>
          retryOnRateLimit: false <span id="CO64-8"><!--Empty--></span><span class="callout">8</span>
          sampleAgeLimit: 0s <span id="CO64-9"><!--Empty--></span><span class="callout">9</span></pre>
<p></p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO64-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								The number of samples to buffer per shard before they are dropped from the queue.
							</div></dd><dt><a href="#CO64-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								The minimum number of shards.
							</div></dd><dt><a href="#CO64-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								The maximum number of shards.
							</div></dd><dt><a href="#CO64-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								The maximum number of samples per send.
							</div></dd><dt><a href="#CO64-5"><span class="callout">5</span></a> </dt><dd><div class="para">
								The maximum time for a sample to wait in buffer.
							</div></dd><dt><a href="#CO64-6"><span class="callout">6</span></a> </dt><dd><div class="para">
								The initial time to wait before retrying a failed request. The time gets doubled for every retry up to the <code class="literal">maxbackoff</code> time.
							</div></dd><dt><a href="#CO64-7"><span class="callout">7</span></a> </dt><dd><div class="para">
								The maximum time to wait before retrying a failed request.
							</div></dd><dt><a href="#CO64-8"><span class="callout">8</span></a> </dt><dd><div class="para">
								Set this parameter to <code class="literal">true</code> to retry a request after receiving a 429 status code from the remote write storage.
							</div></dd><dt><a href="#CO64-9"><span class="callout">9</span></a> </dt><dd><div class="para">
								The samples that are older than the <code class="literal">sampleAgeLimit</code> limit are dropped from the queue. If the value is undefined or set to <code class="literal">0s</code>, the parameter is ignored.
							</div></dd></dl></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/monitoring_apis/#spec-remotewrite-2">Prometheus REST API reference for remote write</a>
</li><li class="listitem">
<a class="link" href="https://prometheus.io/docs/operating/integrations/#remote-endpoints-and-storage">Setting up remote write compatible endpoints</a> (Prometheus documentation)
							</li><li class="listitem">
<a class="link" href="https://prometheus.io/docs/practices/remote_write/#remote-write-tuning">Tuning remote write settings</a> (Prometheus documentation)
							</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/nodes/#nodes-pods-secrets-about_nodes-pods-secrets">Understanding secrets</a>
</li></ul></div></section></section><section class="section" id="creating-cluster-id-labels-for-metrics_configuring-metrics-uwm"><div class="titlepage"><div><div><h4 class="title">4.4.2. Creating cluster ID labels for metrics</h4></div></div></div><p>
					You can create cluster ID labels for metrics by adding the <code class="literal">write_relabel</code> settings for remote write storage in the <code class="literal">user-workload-monitoring-config</code> config map in the <code class="literal">openshift-user-workload-monitoring</code> namespace.
				</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						When Prometheus scrapes user workload targets that expose a <code class="literal">namespace</code> label, the system stores this label as <code class="literal">exported_namespace</code>. This behavior ensures that the final namespace label value is equal to the namespace of the target pod. You cannot override this default configuration by setting the value of the <code class="literal">honorLabels</code> field to <code class="literal">true</code> for <code class="literal">PodMonitor</code> or <code class="literal">ServiceMonitor</code> objects.
					</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role, or as a user with the <code class="literal">user-workload-monitoring-config-edit</code> role in the <code class="literal">openshift-user-workload-monitoring</code> project.
						</li><li class="listitem">
							A cluster administrator has enabled monitoring for user-defined projects.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have configured remote write storage.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">user-workload-monitoring-config</code> config map in the <code class="literal">openshift-user-workload-monitoring</code> project:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</pre></li><li class="listitem"><p class="simpara">
							In the <code class="literal">writeRelabelConfigs:</code> section under <code class="literal">data/config.yaml/prometheus/remoteWrite</code>, add cluster ID relabel configuration values:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        &lt;endpoint_authentication_credentials&gt;
        writeRelabelConfigs: <span id="CO65-1"><!--Empty--></span><span class="callout">1</span>
          - &lt;relabel_config&gt; <span id="CO65-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO65-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Add a list of write relabel configurations for metrics that you want to send to the remote endpoint.
								</div></dd><dt><a href="#CO65-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Substitute the label configuration for the metrics sent to the remote write endpoint.
								</div></dd></dl></div><p class="simpara">
							The following sample shows how to forward a metric with the cluster ID label <code class="literal">cluster_id</code>:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        writeRelabelConfigs:
        - sourceLabels:
          - __tmp_openshift_cluster_id__ <span id="CO66-1"><!--Empty--></span><span class="callout">1</span>
          targetLabel: cluster_id <span id="CO66-2"><!--Empty--></span><span class="callout">2</span>
          action: replace <span id="CO66-3"><!--Empty--></span><span class="callout">3</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO66-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The system initially applies a temporary cluster ID source label named <code class="literal">__tmp_openshift_cluster_id__</code>. This temporary label gets replaced by the cluster ID label name that you specify.
								</div></dd><dt><a href="#CO66-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify the name of the cluster ID label for metrics sent to remote write storage. If you use a label name that already exists for a metric, that value is overwritten with the name of this cluster ID label. For the label name, do not use <code class="literal">__tmp_openshift_cluster_id__</code>. The final relabeling step removes labels that use this name.
								</div></dd><dt><a href="#CO66-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The <code class="literal">replace</code> write relabel action replaces the temporary label with the target label for outgoing metrics. This action is the default and is applied if no action is specified.
								</div></dd></dl></div></li><li class="listitem">
							Save the file to apply the changes. The new configuration is applied automatically.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#adding-cluster-id-labels-to-metrics_key-concepts" title="1.3.3.2. Adding cluster ID labels to metrics">Adding cluster ID labels to metrics</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/support/#support-get-cluster-id_gathering-cluster-data">Obtaining your cluster ID</a>
</li></ul></div></section><section class="section" id="setting-up-metrics-collection-for-user-defined-projects_configuring-metrics-uwm"><div class="titlepage"><div><div><h4 class="title">4.4.3. Setting up metrics collection for user-defined projects</h4></div></div></div><p>
					You can create a <code class="literal">ServiceMonitor</code> resource to scrape metrics from a service endpoint in a user-defined project. This assumes that your application uses a Prometheus client library to expose metrics to the <code class="literal">/metrics</code> canonical name.
				</p><p>
					This section describes how to deploy a sample service in a user-defined project and then create a <code class="literal">ServiceMonitor</code> resource that defines how that service should be monitored.
				</p><section class="section" id="deploying-a-sample-service_configuring-metrics-uwm"><div class="titlepage"><div><div><h5 class="title">4.4.3.1. Deploying a sample service</h5></div></div></div><p>
						To test monitoring of a service in a user-defined project, you can deploy a sample service.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role or as a user with administrative permissions for the namespace.
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Create a YAML file for the service configuration. In this example, it is called <code class="literal">prometheus-example-app.yaml</code>.
							</li><li class="listitem"><p class="simpara">
								Add the following deployment and service configuration details to the file:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Namespace
metadata:
  name: ns1
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: prometheus-example-app
  name: prometheus-example-app
  namespace: ns1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus-example-app
  template:
    metadata:
      labels:
        app: prometheus-example-app
    spec:
      containers:
      - image: ghcr.io/rhobs/prometheus-example-app:0.4.2
        imagePullPolicy: IfNotPresent
        name: prometheus-example-app
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: prometheus-example-app
  name: prometheus-example-app
  namespace: ns1
spec:
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
    name: web
  selector:
    app: prometheus-example-app
  type: ClusterIP</pre><p class="simpara">
								This configuration deploys a service named <code class="literal">prometheus-example-app</code> in the user-defined <code class="literal">ns1</code> project. This service exposes the custom <code class="literal">version</code> metric.
							</p></li><li class="listitem"><p class="simpara">
								Apply the configuration to the cluster:
							</p><pre class="programlisting language-terminal">$ oc apply -f prometheus-example-app.yaml</pre><p class="simpara">
								It takes some time to deploy the service.
							</p></li><li class="listitem"><p class="simpara">
								You can check that the pod is running:
							</p><pre class="programlisting language-terminal">$ oc -n ns1 get pod</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
</p><pre class="programlisting language-terminal">NAME                                      READY     STATUS    RESTARTS   AGE
prometheus-example-app-7857545cb7-sbgwq   1/1       Running   0          81m</pre>
<p></p></div></li></ol></div></section><section class="section" id="specifying-how-a-service-is-monitored_configuring-metrics-uwm"><div class="titlepage"><div><div><h5 class="title">4.4.3.2. Specifying how a service is monitored</h5></div></div></div><p>
						To use the metrics exposed by your service, you must configure OpenShift Container Platform monitoring to scrape metrics from the <code class="literal">/metrics</code> endpoint. You can do this using a <code class="literal">ServiceMonitor</code> custom resource definition (CRD) that specifies how a service should be monitored, or a <code class="literal">PodMonitor</code> CRD that specifies how a pod should be monitored. The former requires a <code class="literal">Service</code> object, while the latter does not, allowing Prometheus to directly scrape metrics from the metrics endpoint exposed by a pod.
					</p><p>
						This procedure shows you how to create a <code class="literal">ServiceMonitor</code> resource for a service in a user-defined project.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role or the <code class="literal">monitoring-edit</code> cluster role.
							</li><li class="listitem">
								You have enabled monitoring for user-defined projects.
							</li><li class="listitem"><p class="simpara">
								For this example, you have deployed the <code class="literal">prometheus-example-app</code> sample service in the <code class="literal">ns1</code> project.
							</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
									The <code class="literal">prometheus-example-app</code> sample service does not support TLS authentication.
								</p></div></rh-alert></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Create a new YAML configuration file named <code class="literal">example-app-service-monitor.yaml</code>.
							</li><li class="listitem"><p class="simpara">
								Add a <code class="literal">ServiceMonitor</code> resource to the YAML file. The following example creates a service monitor named <code class="literal">prometheus-example-monitor</code> to scrape metrics exposed by the <code class="literal">prometheus-example-app</code> service in the <code class="literal">ns1</code> namespace:
							</p><pre class="programlisting language-yaml">apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: prometheus-example-monitor
  namespace: ns1 <span id="CO67-1"><!--Empty--></span><span class="callout">1</span>
spec:
  endpoints:
  - interval: 30s
    port: web <span id="CO67-2"><!--Empty--></span><span class="callout">2</span>
    scheme: http
  selector: <span id="CO67-3"><!--Empty--></span><span class="callout">3</span>
    matchLabels:
      app: prometheus-example-app</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO67-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specify a user-defined namespace where your service runs.
									</div></dd><dt><a href="#CO67-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specify endpoint ports to be scraped by Prometheus.
									</div></dd><dt><a href="#CO67-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Configure a selector to match your service based on its metadata labels.
									</div></dd></dl></div><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
									A <code class="literal">ServiceMonitor</code> resource in a user-defined namespace can only discover services in the same namespace. That is, the <code class="literal">namespaceSelector</code> field of the <code class="literal">ServiceMonitor</code> resource is always ignored.
								</p></div></rh-alert></li><li class="listitem"><p class="simpara">
								Apply the configuration to the cluster:
							</p><pre class="programlisting language-terminal">$ oc apply -f example-app-service-monitor.yaml</pre><p class="simpara">
								It takes some time to deploy the <code class="literal">ServiceMonitor</code> resource.
							</p></li><li class="listitem"><p class="simpara">
								Verify that the <code class="literal">ServiceMonitor</code> resource is running:
							</p><pre class="programlisting language-terminal">$ oc -n &lt;namespace&gt; get servicemonitor</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
</p><pre class="programlisting language-terminal">NAME                         AGE
prometheus-example-monitor   81m</pre>
<p></p></div></li></ol></div></section><section class="section" id="example-service-endpoint-authentication-settings_configuring-metrics-uwm"><div class="titlepage"><div><div><h5 class="title">4.4.3.3. Example service endpoint authentication settings</h5></div></div></div><p>
						You can configure authentication for service endpoints for user-defined project monitoring by using <code class="literal">ServiceMonitor</code> and <code class="literal">PodMonitor</code> custom resource definitions (CRDs).
					</p><p>
						The following samples show different authentication settings for a <code class="literal">ServiceMonitor</code> resource. Each sample shows how to configure a corresponding <code class="literal">Secret</code> object that contains authentication credentials and other relevant settings.
					</p><section class="section" id="sample-yaml-authentication-with-a-bearer-token"><div class="titlepage"><div><div><h5 class="title">4.4.3.3.1. Sample YAML authentication with a bearer token</h5></div></div></div><p>
							The following sample shows bearer token settings for a <code class="literal">Secret</code> object named <code class="literal">example-bearer-auth</code> in the <code class="literal">ns1</code> namespace:
						</p><div class="formalpara"><p class="title"><strong>Example bearer token secret</strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: example-bearer-auth
  namespace: ns1
stringData:
  token: &lt;authentication_token&gt; <span id="CO68-1"><!--Empty--></span><span class="callout">1</span></pre>
<p></p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO68-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify an authentication token.
								</div></dd></dl></div><p>
							The following sample shows bearer token authentication settings for a <code class="literal">ServiceMonitor</code> CRD. The example uses a <code class="literal">Secret</code> object named <code class="literal">example-bearer-auth</code>:
						</p><div class="formalpara" id="sample-yaml-bearer-token_configuring-metrics-uwm"><p class="title"><strong>Example bearer token authentication settings</strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: prometheus-example-monitor
  namespace: ns1
spec:
  endpoints:
  - authorization:
      credentials:
        key: token <span id="CO69-1"><!--Empty--></span><span class="callout">1</span>
        name: example-bearer-auth <span id="CO69-2"><!--Empty--></span><span class="callout">2</span>
    port: web
  selector:
    matchLabels:
      app: prometheus-example-app</pre>
<p></p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO69-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The key that contains the authentication token in the specified <code class="literal">Secret</code> object.
								</div></dd><dt><a href="#CO69-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									The name of the <code class="literal">Secret</code> object that contains the authentication credentials.
								</div></dd></dl></div><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
								Do not use <code class="literal">bearerTokenFile</code> to configure bearer token. If you use the <code class="literal">bearerTokenFile</code> configuration, the <code class="literal">ServiceMonitor</code> resource is rejected.
							</p></div></rh-alert></section><section class="section" id="sample-yaml-basic-auth_configuring-metrics-uwm"><div class="titlepage"><div><div><h5 class="title">4.4.3.3.2. Sample YAML for Basic authentication</h5></div></div></div><p>
							The following sample shows Basic authentication settings for a <code class="literal">Secret</code> object named <code class="literal">example-basic-auth</code> in the <code class="literal">ns1</code> namespace:
						</p><div class="formalpara"><p class="title"><strong>Example Basic authentication secret</strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: example-basic-auth
  namespace: ns1
stringData:
  user: &lt;basic_username&gt; <span id="CO70-1"><!--Empty--></span><span class="callout">1</span>
  password: &lt;basic_password&gt;  <span id="CO70-2"><!--Empty--></span><span class="callout">2</span></pre>
<p></p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO70-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify a username for authentication.
								</div></dd><dt><a href="#CO70-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify a password for authentication.
								</div></dd></dl></div><p>
							The following sample shows Basic authentication settings for a <code class="literal">ServiceMonitor</code> CRD. The example uses a <code class="literal">Secret</code> object named <code class="literal">example-basic-auth</code>:
						</p><div class="formalpara"><p class="title"><strong>Example Basic authentication settings</strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: prometheus-example-monitor
  namespace: ns1
spec:
  endpoints:
  - basicAuth:
      username:
        key: user <span id="CO71-1"><!--Empty--></span><span class="callout">1</span>
        name: example-basic-auth <span id="CO71-2"><!--Empty--></span><span class="callout">2</span>
      password:
        key: password <span id="CO71-3"><!--Empty--></span><span class="callout">3</span>
        name: example-basic-auth <span id="CO71-4"><!--Empty--></span><span class="callout">4</span>
    port: web
  selector:
    matchLabels:
      app: prometheus-example-app</pre>
<p></p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO71-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The key that contains the username in the specified <code class="literal">Secret</code> object.
								</div></dd><dt><a href="#CO71-2"><span class="callout">2</span></a> <a href="#CO71-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									The name of the <code class="literal">Secret</code> object that contains the Basic authentication.
								</div></dd><dt><a href="#CO71-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The key that contains the password in the specified <code class="literal">Secret</code> object.
								</div></dd></dl></div></section><section class="section" id="sample-yaml-oauth-20_configuring-metrics-uwm"><div class="titlepage"><div><div><h5 class="title">4.4.3.3.3. Sample YAML authentication with OAuth 2.0</h5></div></div></div><p>
							The following sample shows OAuth 2.0 settings for a <code class="literal">Secret</code> object named <code class="literal">example-oauth2</code> in the <code class="literal">ns1</code> namespace:
						</p><div class="formalpara"><p class="title"><strong>Example OAuth 2.0 secret</strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: example-oauth2
  namespace: ns1
stringData:
  id: &lt;oauth2_id&gt; <span id="CO72-1"><!--Empty--></span><span class="callout">1</span>
  secret: &lt;oauth2_secret&gt; <span id="CO72-2"><!--Empty--></span><span class="callout">2</span></pre>
<p></p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO72-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Specify an Oauth 2.0 ID.
								</div></dd><dt><a href="#CO72-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Specify an Oauth 2.0 secret.
								</div></dd></dl></div><p>
							The following sample shows OAuth 2.0 authentication settings for a <code class="literal">ServiceMonitor</code> CRD. The example uses a <code class="literal">Secret</code> object named <code class="literal">example-oauth2</code>:
						</p><div class="formalpara"><p class="title"><strong>Example OAuth 2.0 authentication settings</strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: prometheus-example-monitor
  namespace: ns1
spec:
  endpoints:
  - oauth2:
      clientId:
        secret:
          key: id <span id="CO73-1"><!--Empty--></span><span class="callout">1</span>
          name: example-oauth2 <span id="CO73-2"><!--Empty--></span><span class="callout">2</span>
      clientSecret:
        key: secret <span id="CO73-3"><!--Empty--></span><span class="callout">3</span>
        name: example-oauth2 <span id="CO73-4"><!--Empty--></span><span class="callout">4</span>
      tokenUrl: https://example.com/oauth2/token <span id="CO73-5"><!--Empty--></span><span class="callout">5</span>
    port: web
  selector:
    matchLabels:
      app: prometheus-example-app</pre>
<p></p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO73-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									The key that contains the OAuth 2.0 ID in the specified <code class="literal">Secret</code> object.
								</div></dd><dt><a href="#CO73-2"><span class="callout">2</span></a> <a href="#CO73-4"><span class="callout">4</span></a> </dt><dd><div class="para">
									The name of the <code class="literal">Secret</code> object that contains the OAuth 2.0 credentials.
								</div></dd><dt><a href="#CO73-3"><span class="callout">3</span></a> </dt><dd><div class="para">
									The key that contains the OAuth 2.0 secret in the specified <code class="literal">Secret</code> object.
								</div></dd><dt><a href="#CO73-5"><span class="callout">5</span></a> </dt><dd><div class="para">
									The URL used to fetch a token with the specified <code class="literal">clientId</code> and <code class="literal">clientSecret</code>.
								</div></dd></dl></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#enabling-monitoring-for-user-defined-projects-uwm_preparing-to-configure-the-monitoring-stack-uwm" title="4.1.2. Enabling monitoring for user-defined projects">Enabling monitoring for user-defined projects</a>
</li><li class="listitem">
<a class="link" href="https://access.redhat.com/articles/6675491">Scrape Prometheus metrics using TLS in ServiceMonitor configuration</a> (Red Hat Customer Portal article)
								</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/monitoring_apis/#podmonitor-monitoring-coreos-com-v1">PodMonitor API</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/monitoring_apis/#servicemonitor-monitoring-coreos-com-v1">ServiceMonitor API</a>
</li></ul></div></section></section></section></section><section class="section" id="configuring-alerts-and-notifications-uwm"><div class="titlepage"><div><div><h3 class="title">4.5. Configuring alerts and notifications for user workload monitoring</h3></div></div></div><p>
				You can configure a local or external Alertmanager instance to route alerts from Prometheus to endpoint receivers. You can also attach custom labels to all time series and alerts to add useful metadata information.
			</p><section class="section" id="monitoring-configuring-external-alertmanagers_configuring-alerts-and-notifications-uwm"><div class="titlepage"><div><div><h4 class="title">4.5.1. Configuring external Alertmanager instances</h4></div></div></div><p>
					The OpenShift Container Platform monitoring stack includes a local Alertmanager instance that routes alerts from Prometheus.
				</p><p>
					You can add external Alertmanager instances to route alerts for user-defined projects.
				</p><p>
					If you add the same external Alertmanager configuration for multiple clusters and disable the local instance for each cluster, you can then manage alert routing for multiple clusters by using a single external Alertmanager instance.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role or as a user with the <code class="literal">user-workload-monitoring-config-edit</code> role in the <code class="literal">openshift-user-workload-monitoring</code> project.
						</li><li class="listitem">
							A cluster administrator has enabled monitoring for user-defined projects.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">user-workload-monitoring-config</code> config map in the <code class="literal">openshift-user-workload-monitoring</code> project:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</pre></li><li class="listitem"><p class="simpara">
							Add an <code class="literal">additionalAlertmanagerConfigs</code> section with configuration details under <code class="literal">data/config.yaml/&lt;component&gt;</code>:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    &lt;component&gt;: <span id="CO74-1"><!--Empty--></span><span class="callout">1</span>
      additionalAlertmanagerConfigs:
      - &lt;alertmanager_specification&gt; <span id="CO74-2"><!--Empty--></span><span class="callout">2</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO74-2"><span class="callout">2</span></a> </dt><dd><div class="para">
									Substitute <code class="literal">&lt;alertmanager_specification&gt;</code> with authentication and other configuration details for additional Alertmanager instances. Currently supported authentication methods are bearer token (<code class="literal">bearerToken</code>) and client TLS (<code class="literal">tlsConfig</code>).
								</div></dd><dt><a href="#CO74-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Substitute <code class="literal">&lt;component&gt;</code> for one of two supported external Alertmanager components: <code class="literal">prometheus</code> or <code class="literal">thanosRuler</code>.
								</div></dd></dl></div><p class="simpara">
							The following sample config map configures an additional Alertmanager for Thanos Ruler by using a bearer token with client TLS authentication:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    thanosRuler:
      additionalAlertmanagerConfigs:
      - scheme: https
        pathPrefix: /
        timeout: "30s"
        apiVersion: v1
        bearerToken:
          name: alertmanager-bearer-token
          key: token
        tlsConfig:
          key:
            name: alertmanager-tls
            key: tls.key
          cert:
            name: alertmanager-tls
            key: tls.crt
          ca:
            name: alertmanager-tls
            key: tls.ca
        staticConfigs:
        - external-alertmanager1-remote.com
        - external-alertmanager1-remote2.com</pre></li><li class="listitem">
							Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.
						</li></ol></div></section><section class="section" id="monitoring-configuring-secrets-for-alertmanager_configuring-alerts-and-notifications-uwm"><div class="titlepage"><div><div><h4 class="title">4.5.2. Configuring secrets for Alertmanager</h4></div></div></div><p>
					The OpenShift Container Platform monitoring stack includes Alertmanager, which routes alerts from Prometheus to endpoint receivers. If you need to authenticate with a receiver so that Alertmanager can send alerts to it, you can configure Alertmanager to use a secret that contains authentication credentials for the receiver.
				</p><p>
					For example, you can configure Alertmanager to use a secret to authenticate with an endpoint receiver that requires a certificate issued by a private Certificate Authority (CA). You can also configure Alertmanager to use a secret to authenticate with a receiver that requires a password file for Basic HTTP authentication. In either case, authentication details are contained in the <code class="literal">Secret</code> object rather than in the <code class="literal">ConfigMap</code> object.
				</p><section class="section" id="monitoring-adding-a-secret-to-the-alertmanager-configuration_configuring-alerts-and-notifications-uwm"><div class="titlepage"><div><div><h5 class="title">4.5.2.1. Adding a secret to the Alertmanager configuration</h5></div></div></div><p>
						You can add secrets to the Alertmanager configuration by editing the <code class="literal">user-workload-monitoring-config</code> config map in the <code class="literal">openshift-user-workload-monitoring</code> project.
					</p><p>
						After you add a secret to the config map, the secret is mounted as a volume at <code class="literal">/etc/alertmanager/secrets/&lt;secret_name&gt;</code> within the <code class="literal">alertmanager</code> container for the Alertmanager pods.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role or as a user with the <code class="literal">user-workload-monitoring-config-edit</code> role in the <code class="literal">openshift-user-workload-monitoring</code> project.
							</li><li class="listitem">
								A cluster administrator has enabled monitoring for user-defined projects.
							</li><li class="listitem">
								You have created the secret to be configured in Alertmanager in the <code class="literal">openshift-user-workload-monitoring</code> project.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Edit the <code class="literal">user-workload-monitoring-config</code> config map in the <code class="literal">openshift-user-workload-monitoring</code> project:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</pre></li><li class="listitem"><p class="simpara">
								Add a <code class="literal">secrets:</code> section under <code class="literal">data/config.yaml/alertmanager</code> with the following configuration:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    alertmanager:
      secrets: <span id="CO75-1"><!--Empty--></span><span class="callout">1</span>
      - &lt;secret_name_1&gt; <span id="CO75-2"><!--Empty--></span><span class="callout">2</span>
      - &lt;secret_name_2&gt;</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO75-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										This section contains the secrets to be mounted into Alertmanager. The secrets must be located within the same namespace as the Alertmanager object.
									</div></dd><dt><a href="#CO75-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The name of the <code class="literal">Secret</code> object that contains authentication credentials for the receiver. If you add multiple secrets, place each one on a new line.
									</div></dd></dl></div><p class="simpara">
								The following sample config map settings configure Alertmanager to use two <code class="literal">Secret</code> objects named <code class="literal">test-secret-basic-auth</code> and <code class="literal">test-secret-api-token</code>:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    alertmanager:
      secrets:
      - test-secret-basic-auth
      - test-secret-api-token</pre></li><li class="listitem">
								Save the file to apply the changes. The new configuration is applied automatically.
							</li></ol></div></section></section><section class="section" id="attaching-additional-labels-to-your-time-series-and-alerts_configuring-alerts-and-notifications-uwm"><div class="titlepage"><div><div><h4 class="title">4.5.3. Attaching additional labels to your time series and alerts</h4></div></div></div><p>
					You can attach custom labels to all time series and alerts leaving Prometheus by using the external labels feature of Prometheus.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role or as a user with the <code class="literal">user-workload-monitoring-config-edit</code> role in the <code class="literal">openshift-user-workload-monitoring</code> project.
						</li><li class="listitem">
							A cluster administrator has enabled monitoring for user-defined projects.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Edit the <code class="literal">user-workload-monitoring-config</code> config map in the <code class="literal">openshift-user-workload-monitoring</code> project:
						</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</pre></li><li class="listitem"><p class="simpara">
							Define labels you want to add for every metric under <code class="literal">data/config.yaml</code>:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      externalLabels:
        &lt;key&gt;: &lt;value&gt; <span id="CO76-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO76-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Substitute <code class="literal">&lt;key&gt;: &lt;value&gt;</code> with key-value pairs where <code class="literal">&lt;key&gt;</code> is a unique name for the new label and <code class="literal">&lt;value&gt;</code> is its value.
								</div></dd></dl></div><rh-alert class="admonition warning" state="danger"><div class="admonition_header" slot="header">Warning</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										Do not use <code class="literal">prometheus</code> or <code class="literal">prometheus_replica</code> as key names, because they are reserved and will be overwritten.
									</li><li class="listitem">
										Do not use <code class="literal">cluster</code> or <code class="literal">managed_cluster</code> as key names. Using them can cause issues where you are unable to see data in the developer dashboards.
									</li></ul></div></div></rh-alert><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
								In the <code class="literal">openshift-user-workload-monitoring</code> project, Prometheus handles metrics and Thanos Ruler handles alerting and recording rules. Setting <code class="literal">externalLabels</code> for <code class="literal">prometheus</code> in the <code class="literal">user-workload-monitoring-config</code> <code class="literal">ConfigMap</code> object will only configure external labels for metrics and not for any rules.
							</p></div></rh-alert><p class="simpara">
							For example, to add metadata about the region and environment to all time series and alerts, use the following example:
						</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      externalLabels:
        region: eu
        environment: prod</pre></li><li class="listitem">
							Save the file to apply the changes. The pods affected by the new configuration are automatically redeployed.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#enabling-monitoring-for-user-defined-projects-uwm_preparing-to-configure-the-monitoring-stack-uwm" title="4.1.2. Enabling monitoring for user-defined projects">Enabling monitoring for user-defined projects</a>
</li></ul></div></section><section class="section" id="configuring-alert-notifications_configuring-alerts-and-notifications-uwm"><div class="titlepage"><div><div><h4 class="title">4.5.4. Configuring alert notifications</h4></div></div></div><p>
					In OpenShift Container Platform, an administrator can enable alert routing for user-defined projects with one of the following methods:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Use the default platform Alertmanager instance.
						</li><li class="listitem">
							Use a separate Alertmanager instance only for user-defined projects.
						</li></ul></div><p>
					Developers and other users with the <code class="literal">alert-routing-edit</code> cluster role can configure custom alert notifications for their user-defined projects by configuring alert receivers.
				</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						Review the following limitations of alert routing for user-defined projects:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								User-defined alert routing is scoped to the namespace in which the resource is defined. For example, a routing configuration in namespace <code class="literal">ns1</code> only applies to <code class="literal">PrometheusRules</code> resources in the same namespace.
							</li><li class="listitem">
								When a namespace is excluded from user-defined monitoring, <code class="literal">AlertmanagerConfig</code> resources in the namespace cease to be part of the Alertmanager configuration.
							</li></ul></div></div></rh-alert><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#understanding-alert-routing-for-user-defined-projects_key-concepts" title="1.3.6. Understanding alert routing for user-defined projects">Understanding alert routing for user-defined projects</a>
</li><li class="listitem">
<a class="link" href="#sending-notifications-to-external-systems_key-concepts" title="1.3.7. Sending notifications to external systems">Sending notifications to external systems</a>
</li><li class="listitem">
<a class="link" href="https://www.pagerduty.com/">PagerDuty</a> (PagerDuty official site)
						</li><li class="listitem">
<a class="link" href="https://www.pagerduty.com/docs/guides/prometheus-integration-guide/">Prometheus Integration Guide</a> (PagerDuty official site)
						</li><li class="listitem">
<a class="link" href="#support-version-matrix-for-monitoring-components_maintenance-and-support-for-monitoring" title="2.1.3. Support version matrix for monitoring components">Support version matrix for monitoring components</a>
</li><li class="listitem">
<a class="link" href="#enabling-alert-routing-for-user-defined-projects_preparing-to-configure-the-monitoring-stack-uwm" title="4.1.3. Enabling alert routing for user-defined projects">Enabling alert routing for user-defined projects</a>
</li></ul></div><section class="section" id="configuring-alert-routing-for-user-defined-projects_configuring-alerts-and-notifications-uwm"><div class="titlepage"><div><div><h5 class="title">4.5.4.1. Configuring alert routing for user-defined projects</h5></div></div></div><p>
						If you are a non-administrator user who has been given the <code class="literal">alert-routing-edit</code> cluster role, you can create or edit alert routing for user-defined projects.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								A cluster administrator has enabled monitoring for user-defined projects.
							</li><li class="listitem">
								A cluster administrator has enabled alert routing for user-defined projects.
							</li><li class="listitem">
								You are logged in as a user that has the <code class="literal">alert-routing-edit</code> cluster role for the project for which you want to create alert routing.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Create a YAML file for alert routing. The example in this procedure uses a file called <code class="literal">example-app-alert-routing.yaml</code>.
							</li><li class="listitem"><p class="simpara">
								Add an <code class="literal">AlertmanagerConfig</code> YAML definition to the file. For example:
							</p><pre class="programlisting language-yaml">apiVersion: monitoring.coreos.com/v1beta1
kind: AlertmanagerConfig
metadata:
  name: example-routing
  namespace: ns1
spec:
  route:
    receiver: default
    groupBy: [job]
  receivers:
  - name: default
    webhookConfigs:
    - url: https://example.org/post</pre></li><li class="listitem">
								Save the file.
							</li><li class="listitem"><p class="simpara">
								Apply the resource to the cluster:
							</p><pre class="programlisting language-terminal">$ oc apply -f example-app-alert-routing.yaml</pre><p class="simpara">
								The configuration is automatically applied to the Alertmanager pods.
							</p></li></ol></div></section><section class="section" id="configuring-alert-routing-user-defined-alerts-secret_configuring-alerts-and-notifications-uwm"><div class="titlepage"><div><div><h5 class="title">4.5.4.2. Configuring alert routing for user-defined projects with the Alertmanager secret</h5></div></div></div><p>
						If you have enabled a separate instance of Alertmanager that is dedicated to user-defined alert routing, you can customize where and how the instance sends notifications by editing the <code class="literal">alertmanager-user-workload</code> secret in the <code class="literal">openshift-user-workload-monitoring</code> namespace.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							All features of a supported version of upstream Alertmanager are also supported in an OpenShift Container Platform Alertmanager configuration. To check all the configuration options of a supported version of upstream Alertmanager, see <a class="link" href="https://prometheus.io/docs/alerting/0.27/configuration/">Alertmanager configuration</a> (Prometheus documentation).
						</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
							</li><li class="listitem">
								You have enabled a separate instance of Alertmanager for user-defined alert routing.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Print the currently active Alertmanager configuration into the file <code class="literal">alertmanager.yaml</code>:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring get secret alertmanager-user-workload --template='{{ index .data "alertmanager.yaml" }}' | base64 --decode &gt; alertmanager.yaml</pre></li><li class="listitem"><p class="simpara">
								Edit the configuration in <code class="literal">alertmanager.yaml</code>:
							</p><pre class="programlisting language-yaml">global:
  http_config:
    proxy_from_environment: true <span id="CO77-1"><!--Empty--></span><span class="callout">1</span>
route:
  receiver: Default
  group_by:
  - name: Default
  routes:
  - matchers:
    - "service = prometheus-example-monitor" <span id="CO77-2"><!--Empty--></span><span class="callout">2</span>
    receiver: &lt;receiver&gt; <span id="CO77-3"><!--Empty--></span><span class="callout">3</span>
receivers:
- name: Default
- name: &lt;receiver&gt;
  &lt;receiver_configuration&gt; <span id="CO77-4"><!--Empty--></span><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO77-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										If you configured an HTTP cluster-wide proxy, set the <code class="literal">proxy_from_environment</code> parameter to <code class="literal">true</code> to enable proxying for all alert receivers.
									</div></dd><dt><a href="#CO77-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Specify labels to match your alerts. This example targets all alerts that have the <code class="literal">service="prometheus-example-monitor"</code> label.
									</div></dd><dt><a href="#CO77-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										Specify the name of the receiver to use for the alerts group.
									</div></dd><dt><a href="#CO77-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										Specify the receiver configuration.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Apply the new configuration in the file:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring create secret generic alertmanager-user-workload --from-file=alertmanager.yaml --dry-run=client -o=yaml |  oc -n openshift-user-workload-monitoring replace secret --filename=-</pre></li></ol></div></section><section class="section" id="configuring-different-alert-receivers-for-default-platform-alerts-and-user-defined-alerts_configuring-alerts-and-notifications-uwm"><div class="titlepage"><div><div><h5 class="title">4.5.4.3. Configuring different alert receivers for default platform alerts and user-defined alerts</h5></div></div></div><p>
						You can configure different alert receivers for default platform alerts and user-defined alerts to ensure the following results:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								All default platform alerts are sent to a receiver owned by the team in charge of these alerts.
							</li><li class="listitem">
								All user-defined alerts are sent to another receiver so that the team can focus only on platform alerts.
							</li></ul></div><p>
						You can achieve this by using the <code class="literal">openshift_io_alert_source="platform"</code> label that is added by the Cluster Monitoring Operator to all platform alerts:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Use the <code class="literal">openshift_io_alert_source="platform"</code> matcher to match default platform alerts.
							</li><li class="listitem">
								Use the <code class="literal">openshift_io_alert_source!="platform"</code> or <code class="literal">'openshift_io_alert_source=""'</code> matcher to match user-defined alerts.
							</li></ul></div><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							This configuration does not apply if you have enabled a separate instance of Alertmanager dedicated to user-defined alerts.
						</p></div></rh-alert></section></section></section></section><section class="chapter" id="accessing-metrics"><div class="titlepage"><div><div><h2 class="title">Chapter 5. Accessing metrics</h2></div></div></div><section class="section" id="accessing-metrics-as-an-administrator"><div class="titlepage"><div><div><h3 class="title">5.1. Accessing metrics as an administrator</h3></div></div></div><p>
				You can access metrics to monitor the performance of cluster components and your workloads.
			</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#understanding-metrics_key-concepts" title="1.3.3. Understanding metrics">Understanding metrics</a>
</li></ul></div><section class="section" id="viewing-a-list-of-available-metrics_accessing-metrics-as-an-administrator"><div class="titlepage"><div><div><h4 class="title">5.1.1. Viewing a list of available metrics</h4></div></div></div><p>
					As a cluster administrator or as a user with view permissions for all projects, you can view a list of metrics available in a cluster and output the list in JSON format.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You are a cluster administrator, or you have access to the cluster as a user with the <code class="literal">cluster-monitoring-view</code> cluster role.
						</li><li class="listitem">
							You have installed the OpenShift Container Platform CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have obtained the OpenShift Container Platform API route for Thanos Querier.
						</li><li class="listitem"><p class="simpara">
							You are able to get a bearer token by using the <code class="literal">oc whoami -t</code> command.
						</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
								You can only use bearer token authentication to access the Thanos Querier API route.
							</p></div></rh-alert></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							If you have not obtained the OpenShift Container Platform API route for Thanos Querier, run the following command:
						</p><pre class="programlisting language-terminal">$ oc get routes -n openshift-monitoring thanos-querier -o jsonpath='{.status.ingress[0].host}'</pre></li><li class="listitem"><p class="simpara">
							Retrieve a list of metrics in JSON format from the Thanos Querier API route by running the following command. This command uses <code class="literal">oc</code> to authenticate with a bearer token.
						</p><pre class="programlisting language-terminal">$ curl -k -H "Authorization: Bearer $(oc whoami -t)" https://&lt;thanos_querier_route&gt;/api/v1/metadata <span id="CO78-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO78-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace <code class="literal">&lt;thanos_querier_route&gt;</code> with the OpenShift Container Platform API route for Thanos Querier.
								</div></dd></dl></div></li></ol></div></section><section class="section" id="querying-metrics-for-all-projects-with-mon-dashboard_accessing-metrics-as-an-administrator"><div class="titlepage"><div><div><h4 class="title">5.1.2. Querying metrics for all projects with the OpenShift Container Platform web console</h4></div></div></div><p>
					You can use the OpenShift Container Platform metrics query browser to run Prometheus Query Language (PromQL) queries to examine metrics visualized on a plot. This functionality provides information about the state of a cluster and any user-defined workloads that you are monitoring.
				</p><p>
					As a cluster administrator or as a user with view permissions for all projects, you can access metrics for all default OpenShift Container Platform and user-defined projects in the Metrics UI.
				</p><p>
					The Metrics UI includes predefined queries, for example, CPU, memory, bandwidth, or network packet for all projects. You can also run custom Prometheus Query Language (PromQL) queries.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role or with view permissions for all projects.
						</li><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the <span class="strong strong"><strong>Administrator</strong></span> perspective of the OpenShift Container Platform web console, click <span class="strong strong"><strong>Observe</strong></span> and go to the <span class="strong strong"><strong>Metrics</strong></span> tab.
						</li><li class="listitem"><p class="simpara">
							To add one or more queries, perform any of the following actions:
						</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 50%; "/><!--Empty--><col class="col_2" style="width: 50%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059288479024" scope="col" valign="top">Option</th><th align="left" id="idm140059288477936" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059288479024" valign="top"> <p>
											Select an existing query.
										</p>
</td><td align="left" headers="idm140059288477936" valign="top"> <p>
											From the <span class="strong strong"><strong>Select query</strong></span> drop-down list, select an existing query.
										</p>
</td></tr><tr><td align="left" headers="idm140059288479024" valign="top"> <p>
											Create a custom query.
										</p>
</td><td align="left" headers="idm140059288477936" valign="top"> <p>
											Add your Prometheus Query Language (PromQL) query to the <span class="strong strong"><strong>Expression</strong></span> field.
										</p>
<p>
											As you type a PromQL expression, autocomplete suggestions appear in a drop-down list. These suggestions include functions, metrics, labels, and time tokens. Use the keyboard arrows to select one of these suggested items and then press Enter to add the item to your expression. Move your mouse pointer over a suggested item to view a brief description of that item.
										</p>
</td></tr><tr><td align="left" headers="idm140059288479024" valign="top"> <p>
											Add multiple queries.
										</p>
</td><td align="left" headers="idm140059288477936" valign="top"> <p>
											Click <span class="strong strong"><strong>Add query</strong></span>.
										</p>
</td></tr><tr><td align="left" headers="idm140059288479024" valign="top"> <p>
											Duplicate an existing query.
										</p>
</td><td align="left" headers="idm140059288477936" valign="top"> <p>
											Click the options menu 
											<span class="inlinemediaobject"><img alt="kebab" src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.18-Monitoring-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png"/></span>
											 next to the query, then choose <span class="strong strong"><strong>Duplicate query</strong></span>.
										</p>
</td></tr><tr><td align="left" headers="idm140059288479024" valign="top"> <p>
											Disable a query from being run.
										</p>
</td><td align="left" headers="idm140059288477936" valign="top"> <p>
											Click the options menu 
											<span class="inlinemediaobject"><img alt="kebab" src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.18-Monitoring-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png"/></span>
											 next to the query and choose <span class="strong strong"><strong>Disable query</strong></span>.
										</p>
</td></tr></tbody></table></rh-table></li><li class="listitem"><p class="simpara">
							To run queries that you created, click <span class="strong strong"><strong>Run queries</strong></span>. The metrics from the queries are visualized on the plot. If a query is invalid, the UI shows an error message.
						</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										When drawing time series graphs, queries that operate on large amounts of data might time out or overload the browser. To avoid this, click <span class="strong strong"><strong>Hide graph</strong></span> and calibrate your query by using only the metrics table. Then, after finding a feasible query, enable the plot to draw the graphs.
									</li><li class="listitem">
										By default, the query table shows an expanded view that lists every metric and its current value. Click the <span class="strong strong"><strong>˅</strong></span> down arrowhead to minimize the expanded view for a query.
									</li></ul></div></div></rh-alert></li><li class="listitem">
							Optional: Save the page URL to use this set of queries again in the future.
						</li><li class="listitem"><p class="simpara">
							Explore the visualized metrics. Initially, all metrics from all enabled queries are shown on the plot. Select which metrics are shown by performing any of the following actions:
						</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 50%; "/><!--Empty--><col class="col_2" style="width: 50%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059287316560" scope="col" valign="top">Option</th><th align="left" id="idm140059287315472" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059287316560" valign="top"> <p>
											Hide all metrics from a query.
										</p>
</td><td align="left" headers="idm140059287315472" valign="top"> <p>
											Click the options menu 
											<span class="inlinemediaobject"><img alt="kebab" src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.18-Monitoring-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png"/></span>
											 for the query and click <span class="strong strong"><strong>Hide all series</strong></span>.
										</p>
</td></tr><tr><td align="left" headers="idm140059287316560" valign="top"> <p>
											Hide a specific metric.
										</p>
</td><td align="left" headers="idm140059287315472" valign="top"> <p>
											Go to the query table and click the colored square near the metric name.
										</p>
</td></tr><tr><td align="left" headers="idm140059287316560" valign="top"> <p>
											Zoom into the plot and change the time range.
										</p>
</td><td align="left" headers="idm140059287315472" valign="top"> <p>
											Perform one of the following actions:
										</p>
<div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
													Visually select the time range by clicking and dragging on the plot horizontally.
												</li><li class="listitem">
													Use the menu to select the time range.
												</li></ul></div>
</td></tr><tr><td align="left" headers="idm140059287316560" valign="top"> <p>
											Reset the time range.
										</p>
</td><td align="left" headers="idm140059287315472" valign="top"> <p>
											Click <span class="strong strong"><strong>Reset zoom</strong></span>.
										</p>
</td></tr><tr><td align="left" headers="idm140059287316560" valign="top"> <p>
											Display outputs for all queries at a specific point in time.
										</p>
</td><td align="left" headers="idm140059287315472" valign="top"> <p>
											Hover over the plot at the point you are interested in. The query outputs appear in a pop-up box.
										</p>
</td></tr><tr><td align="left" headers="idm140059287316560" valign="top"> <p>
											Hide the plot.
										</p>
</td><td align="left" headers="idm140059287315472" valign="top"> <p>
											Click <span class="strong strong"><strong>Hide graph</strong></span>.
										</p>
</td></tr></tbody></table></rh-table></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="https://prometheus.io/docs/prometheus/latest/querying/basics/">Querying Prometheus</a> (Prometheus documentation)
						</li></ul></div></section><section class="section" id="getting-detailed-information-about-a-target_accessing-metrics-as-an-administrator"><div class="titlepage"><div><div><h4 class="title">5.1.3. Getting detailed information about a metrics target</h4></div></div></div><p>
					You can use the OpenShift Container Platform web console to view, search, and filter the endpoints that are currently targeted for scraping, which helps you to identify and troubleshoot problems. For example, you can view the current status of targeted endpoints to see when OpenShift Container Platform monitoring is not able to scrape metrics from a targeted component.
				</p><p>
					The <span class="strong strong"><strong>Metrics targets</strong></span> page shows targets for default OpenShift Container Platform projects and for user-defined projects.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as an administrator for the project for which you want to view metrics targets.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							In the <span class="strong strong"><strong>Administrator</strong></span> perspective of the OpenShift Container Platform web console, go to <span class="strong strong"><strong>Observe</strong></span> → <span class="strong strong"><strong>Targets</strong></span>. The <span class="strong strong"><strong>Metrics targets</strong></span> page opens with a list of all service endpoint targets that are being scraped for metrics.
						</p><p class="simpara">
							This page shows details about targets for default OpenShift Container Platform and user-defined projects. This page lists the following information for each target:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Service endpoint URL being scraped
								</li><li class="listitem">
									The <code class="literal">ServiceMonitor</code> resource being monitored
								</li><li class="listitem">
									The <span class="strong strong"><strong>up</strong></span> or <span class="strong strong"><strong>down</strong></span> status of the target
								</li><li class="listitem">
									Namespace
								</li><li class="listitem">
									Last scrape time
								</li><li class="listitem">
									Duration of the last scrape
								</li></ul></div></li><li class="listitem"><p class="simpara">
							Optional: To find a specific target, perform any of the following actions:
						</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 50%; "/><!--Empty--><col class="col_2" style="width: 50%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059285060048" scope="col" valign="top">Option</th><th align="left" id="idm140059285058960" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059285060048" valign="top"> <p>
											Filter the targets by status and source.
										</p>
</td><td align="left" headers="idm140059285058960" valign="top"> <p>
											Choose filters in the <span class="strong strong"><strong>Filter</strong></span> list.
										</p>
<p>
											The following filtering options are available:
										</p>
<div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
<span class="strong strong"><strong>Status</strong></span> filters:
												</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
<span class="strong strong"><strong>Up</strong></span>. The target is currently up and being actively scraped for metrics.
														</li><li class="listitem">
<span class="strong strong"><strong>Down</strong></span>. The target is currently down and not being scraped for metrics.
														</li></ul></div></li><li class="listitem"><p class="simpara">
<span class="strong strong"><strong>Source</strong></span> filters:
												</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
<span class="strong strong"><strong>Platform</strong></span>. Platform-level targets relate only to default Red Hat OpenShift Service on AWS projects. These projects provide core Red Hat OpenShift Service on AWS functionality.
														</li><li class="listitem">
<span class="strong strong"><strong>User</strong></span>. User targets relate to user-defined projects. These projects are user-created and can be customized.
														</li></ul></div></li></ul></div>
</td></tr><tr><td align="left" headers="idm140059285060048" valign="top"> <p>
											Search for a target by name or label.
										</p>
</td><td align="left" headers="idm140059285058960" valign="top"> <p>
											Enter a search term in the <span class="strong strong"><strong>Text</strong></span> or <span class="strong strong"><strong>Label</strong></span> field next to the search box.
										</p>
</td></tr><tr><td align="left" headers="idm140059285060048" valign="top"> <p>
											Sort the targets.
										</p>
</td><td align="left" headers="idm140059285058960" valign="top"> <p>
											Click one or more of the <span class="strong strong"><strong>Endpoint Status</strong></span>, <span class="strong strong"><strong>Namespace</strong></span>, <span class="strong strong"><strong>Last Scrape</strong></span>, and <span class="strong strong"><strong>Scrape Duration</strong></span> column headers.
										</p>
</td></tr></tbody></table></rh-table></li><li class="listitem"><p class="simpara">
							Click the URL in the <span class="strong strong"><strong>Endpoint</strong></span> column for a target to go to its <span class="strong strong"><strong>Target details</strong></span> page. This page provides information about the target, including the following information:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									The endpoint URL being scraped for metrics
								</li><li class="listitem">
									The current <span class="strong strong"><strong>Up</strong></span> or <span class="strong strong"><strong>Down</strong></span> status of the target
								</li><li class="listitem">
									A link to the namespace
								</li><li class="listitem">
									A link to the <code class="literal">ServiceMonitor</code> resource details
								</li><li class="listitem">
									Labels attached to the target
								</li><li class="listitem">
									The most recent time that the target was scraped for metrics
								</li></ul></div></li></ol></div></section><section class="section" id="reviewing-monitoring-dashboards-admin_accessing-metrics-as-an-administrator"><div class="titlepage"><div><div><h4 class="title">5.1.4. Reviewing monitoring dashboards as a cluster administrator</h4></div></div></div><p>
					In the <span class="strong strong"><strong>Administrator</strong></span> perspective, you can view dashboards relating to core OpenShift Container Platform cluster components.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the <span class="strong strong"><strong>Administrator</strong></span> perspective of the OpenShift Container Platform web console, go to <span class="strong strong"><strong>Observe</strong></span> → <span class="strong strong"><strong>Dashboards</strong></span>.
						</li><li class="listitem">
							Choose a dashboard in the <span class="strong strong"><strong>Dashboard</strong></span> list. Some dashboards, such as <span class="strong strong"><strong>etcd</strong></span> and <span class="strong strong"><strong>Prometheus</strong></span> dashboards, produce additional sub-menus when selected.
						</li><li class="listitem"><p class="simpara">
							Optional: Select a time range for the graphs in the <span class="strong strong"><strong>Time Range</strong></span> list.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Select a pre-defined time period.
								</li><li class="listitem"><p class="simpara">
									Set a custom time range by clicking <span class="strong strong"><strong>Custom time range</strong></span> in the <span class="strong strong"><strong>Time Range</strong></span> list.
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
											Input or select the <span class="strong strong"><strong>From</strong></span> and <span class="strong strong"><strong>To</strong></span> dates and times.
										</li><li class="listitem">
											Click <span class="strong strong"><strong>Save</strong></span> to save the custom time range.
										</li></ol></div></li></ul></div></li><li class="listitem">
							Optional: Select a <span class="strong strong"><strong>Refresh Interval</strong></span>.
						</li><li class="listitem">
							Hover over each of the graphs within a dashboard to display detailed information about specific items.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#about-monitoring-dashboards_key-concepts" title="1.3.4. About monitoring dashboards">About monitoring dashboards</a>
</li></ul></div></section></section><section class="section" id="accessing-metrics-as-a-developer"><div class="titlepage"><div><div><h3 class="title">5.2. Accessing metrics as a developer</h3></div></div></div><p>
				You can access metrics to monitor the performance of your cluster workloads.
			</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#understanding-metrics_key-concepts" title="1.3.3. Understanding metrics">Understanding metrics</a>
</li></ul></div><section class="section" id="viewing-a-list-of-available-metrics_accessing-metrics-as-a-developer"><div class="titlepage"><div><div><h4 class="title">5.2.1. Viewing a list of available metrics</h4></div></div></div><p>
					As a cluster administrator or as a user with view permissions for all projects, you can view a list of metrics available in a cluster and output the list in JSON format.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You are a cluster administrator, or you have access to the cluster as a user with the <code class="literal">cluster-monitoring-view</code> cluster role.
						</li><li class="listitem">
							You have installed the OpenShift Container Platform CLI (<code class="literal">oc</code>).
						</li><li class="listitem">
							You have obtained the OpenShift Container Platform API route for Thanos Querier.
						</li><li class="listitem"><p class="simpara">
							You are able to get a bearer token by using the <code class="literal">oc whoami -t</code> command.
						</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
								You can only use bearer token authentication to access the Thanos Querier API route.
							</p></div></rh-alert></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							If you have not obtained the OpenShift Container Platform API route for Thanos Querier, run the following command:
						</p><pre class="programlisting language-terminal">$ oc get routes -n openshift-monitoring thanos-querier -o jsonpath='{.status.ingress[0].host}'</pre></li><li class="listitem"><p class="simpara">
							Retrieve a list of metrics in JSON format from the Thanos Querier API route by running the following command. This command uses <code class="literal">oc</code> to authenticate with a bearer token.
						</p><pre class="programlisting language-terminal">$ curl -k -H "Authorization: Bearer $(oc whoami -t)" https://&lt;thanos_querier_route&gt;/api/v1/metadata <span id="CO79-1"><!--Empty--></span><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO79-1"><span class="callout">1</span></a> </dt><dd><div class="para">
									Replace <code class="literal">&lt;thanos_querier_route&gt;</code> with the OpenShift Container Platform API route for Thanos Querier.
								</div></dd></dl></div></li></ol></div></section><section class="section" id="querying-metrics-for-user-defined-projects-with-mon-dashboard_accessing-metrics-as-a-developer"><div class="titlepage"><div><div><h4 class="title">5.2.2. Querying metrics for user-defined projects with the OpenShift Container Platform web console</h4></div></div></div><p>
					You can use the OpenShift Container Platform metrics query browser to run Prometheus Query Language (PromQL) queries to examine metrics visualized on a plot. This functionality provides information about any user-defined workloads that you are monitoring.
				</p><p>
					As a developer, you must specify a project name when querying metrics. You must have the required privileges to view metrics for the selected project.
				</p><p>
					The Metrics UI includes predefined queries, for example, CPU, memory, bandwidth, or network packet. These queries are restricted to the selected project. You can also run custom Prometheus Query Language (PromQL) queries for the project.
				</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						Developers can only use the <span class="strong strong"><strong>Developer</strong></span> perspective and not the <span class="strong strong"><strong>Administrator</strong></span> perspective. As a developer, you can only query metrics for one project at a time.
					</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a developer or as a user with view permissions for the project that you are viewing metrics for.
						</li><li class="listitem">
							You have enabled monitoring for user-defined projects.
						</li><li class="listitem">
							You have deployed a service in a user-defined project.
						</li><li class="listitem">
							You have created a <code class="literal">ServiceMonitor</code> custom resource definition (CRD) for the service to define how the service is monitored.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the <span class="strong strong"><strong>Developer</strong></span> perspective of the OpenShift Container Platform web console, click <span class="strong strong"><strong>Observe</strong></span> and go to the <span class="strong strong"><strong>Metrics</strong></span> tab.
						</li><li class="listitem">
							Select the project that you want to view metrics for from the <span class="strong strong"><strong>Project:</strong></span> list.
						</li><li class="listitem"><p class="simpara">
							To add one or more queries, perform any of the following actions:
						</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 50%; "/><!--Empty--><col class="col_2" style="width: 50%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059285696960" scope="col" valign="top">Option</th><th align="left" id="idm140059285695872" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059285696960" valign="top"> <p>
											Select an existing query.
										</p>
</td><td align="left" headers="idm140059285695872" valign="top"> <p>
											From the <span class="strong strong"><strong>Select query</strong></span> drop-down list, select an existing query.
										</p>
</td></tr><tr><td align="left" headers="idm140059285696960" valign="top"> <p>
											Create a custom query.
										</p>
</td><td align="left" headers="idm140059285695872" valign="top"> <p>
											Add your Prometheus Query Language (PromQL) query to the <span class="strong strong"><strong>Expression</strong></span> field.
										</p>
<p>
											As you type a PromQL expression, autocomplete suggestions appear in a drop-down list. These suggestions include functions, metrics, labels, and time tokens. Use the keyboard arrows to select one of these suggested items and then press Enter to add the item to your expression. Move your mouse pointer over a suggested item to view a brief description of that item.
										</p>
</td></tr><tr><td align="left" headers="idm140059285696960" valign="top"> <p>
											Add multiple queries.
										</p>
</td><td align="left" headers="idm140059285695872" valign="top"> <p>
											Click <span class="strong strong"><strong>Add query</strong></span>.
										</p>
</td></tr><tr><td align="left" headers="idm140059285696960" valign="top"> <p>
											Duplicate an existing query.
										</p>
</td><td align="left" headers="idm140059285695872" valign="top"> <p>
											Click the options menu 
											<span class="inlinemediaobject"><img alt="kebab" src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.18-Monitoring-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png"/></span>
											 next to the query, then choose <span class="strong strong"><strong>Duplicate query</strong></span>.
										</p>
</td></tr><tr><td align="left" headers="idm140059285696960" valign="top"> <p>
											Disable a query from being run.
										</p>
</td><td align="left" headers="idm140059285695872" valign="top"> <p>
											Click the options menu 
											<span class="inlinemediaobject"><img alt="kebab" src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.18-Monitoring-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png"/></span>
											 next to the query and choose <span class="strong strong"><strong>Disable query</strong></span>.
										</p>
</td></tr></tbody></table></rh-table></li><li class="listitem"><p class="simpara">
							To run queries that you created, click <span class="strong strong"><strong>Run queries</strong></span>. The metrics from the queries are visualized on the plot. If a query is invalid, the UI shows an error message.
						</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										When drawing time series graphs, queries that operate on large amounts of data might time out or overload the browser. To avoid this, click <span class="strong strong"><strong>Hide graph</strong></span> and calibrate your query by using only the metrics table. Then, after finding a feasible query, enable the plot to draw the graphs.
									</li><li class="listitem">
										By default, the query table shows an expanded view that lists every metric and its current value. Click the <span class="strong strong"><strong>˅</strong></span> down arrowhead to minimize the expanded view for a query.
									</li></ul></div></div></rh-alert></li><li class="listitem">
							Optional: Save the page URL to use this set of queries again in the future.
						</li><li class="listitem"><p class="simpara">
							Explore the visualized metrics. Initially, all metrics from all enabled queries are shown on the plot. Select which metrics are shown by performing any of the following actions:
						</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 50%; "/><!--Empty--><col class="col_2" style="width: 50%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059289053328" scope="col" valign="top">Option</th><th align="left" id="idm140059289052240" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059289053328" valign="top"> <p>
											Hide all metrics from a query.
										</p>
</td><td align="left" headers="idm140059289052240" valign="top"> <p>
											Click the options menu 
											<span class="inlinemediaobject"><img alt="kebab" src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.18-Monitoring-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png"/></span>
											 for the query and click <span class="strong strong"><strong>Hide all series</strong></span>.
										</p>
</td></tr><tr><td align="left" headers="idm140059289053328" valign="top"> <p>
											Hide a specific metric.
										</p>
</td><td align="left" headers="idm140059289052240" valign="top"> <p>
											Go to the query table and click the colored square near the metric name.
										</p>
</td></tr><tr><td align="left" headers="idm140059289053328" valign="top"> <p>
											Zoom into the plot and change the time range.
										</p>
</td><td align="left" headers="idm140059289052240" valign="top"> <p>
											Perform one of the following actions:
										</p>
<div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
													Visually select the time range by clicking and dragging on the plot horizontally.
												</li><li class="listitem">
													Use the menu to select the time range.
												</li></ul></div>
</td></tr><tr><td align="left" headers="idm140059289053328" valign="top"> <p>
											Reset the time range.
										</p>
</td><td align="left" headers="idm140059289052240" valign="top"> <p>
											Click <span class="strong strong"><strong>Reset zoom</strong></span>.
										</p>
</td></tr><tr><td align="left" headers="idm140059289053328" valign="top"> <p>
											Display outputs for all queries at a specific point in time.
										</p>
</td><td align="left" headers="idm140059289052240" valign="top"> <p>
											Hover over the plot at the point you are interested in. The query outputs appear in a pop-up box.
										</p>
</td></tr><tr><td align="left" headers="idm140059289053328" valign="top"> <p>
											Hide the plot.
										</p>
</td><td align="left" headers="idm140059289052240" valign="top"> <p>
											Click <span class="strong strong"><strong>Hide graph</strong></span>.
										</p>
</td></tr></tbody></table></rh-table></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="https://prometheus.io/docs/prometheus/latest/querying/basics/">Querying Prometheus</a> (Prometheus documentation)
						</li></ul></div></section><section class="section" id="reviewing-monitoring-dashboards-developer_accessing-metrics-as-a-developer"><div class="titlepage"><div><div><h4 class="title">5.2.3. Reviewing monitoring dashboards as a developer</h4></div></div></div><p>
					In the <span class="strong strong"><strong>Developer</strong></span> perspective, you can view dashboards relating to a selected project.
				</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						In the <span class="strong strong"><strong>Developer</strong></span> perspective, you can view dashboards for only one project at a time.
					</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a developer or as a user.
						</li><li class="listitem">
							You have view permissions for the project that you are viewing the dashboard for.
						</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
							In the Developer perspective in the OpenShift Container Platform web console, click <span class="strong strong"><strong>Observe</strong></span> and go to the <span class="strong strong"><strong>Dashboards</strong></span> tab.
						</li><li class="listitem">
							Select a project from the <span class="strong strong"><strong>Project:</strong></span> drop-down list.
						</li><li class="listitem"><p class="simpara">
							Select a dashboard from the <span class="strong strong"><strong>Dashboard</strong></span> drop-down list to see the filtered metrics.
						</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
								All dashboards produce additional sub-menus when selected, except <span class="strong strong"><strong>Kubernetes / Compute Resources / Namespace (Pods)</strong></span>.
							</p></div></rh-alert></li><li class="listitem"><p class="simpara">
							Optional: Select a time range for the graphs in the <span class="strong strong"><strong>Time Range</strong></span> list.
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Select a pre-defined time period.
								</li><li class="listitem"><p class="simpara">
									Set a custom time range by clicking <span class="strong strong"><strong>Custom time range</strong></span> in the <span class="strong strong"><strong>Time Range</strong></span> list.
								</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
											Input or select the <span class="strong strong"><strong>From</strong></span> and <span class="strong strong"><strong>To</strong></span> dates and times.
										</li><li class="listitem">
											Click <span class="strong strong"><strong>Save</strong></span> to save the custom time range.
										</li></ol></div></li></ul></div></li><li class="listitem">
							Optional: Select a <span class="strong strong"><strong>Refresh Interval</strong></span>.
						</li><li class="listitem">
							Hover over each of the graphs within a dashboard to display detailed information about specific items.
						</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#about-monitoring-dashboards_key-concepts" title="1.3.4. About monitoring dashboards">About monitoring dashboards</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/building_applications/#monitoring-project-and-application-metrics-using-developer-perspective">Monitoring project and application metrics using the Developer perspective</a>
</li></ul></div></section></section><section class="section" id="accessing-monitoring-apis-by-using-the-cli"><div class="titlepage"><div><div><h3 class="title">5.3. Accessing monitoring APIs by using the CLI</h3></div></div></div><p>
				In OpenShift Container Platform, you can access web service APIs for some monitoring components from the command-line interface (CLI).
			</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
					In certain situations, accessing API endpoints can degrade the performance and scalability of your cluster, especially if you use endpoints to retrieve, send, or query large amounts of metrics data.
				</p><p>
					To avoid these issues, consider the following recommendations:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Avoid querying endpoints frequently. Limit queries to a maximum of one every 30 seconds.
						</li><li class="listitem">
							Do not retrieve all metrics data through the <code class="literal">/federate</code> endpoint for Prometheus. Query the endpoint only when you want to retrieve a limited, aggregated data set. For example, retrieving fewer than 1,000 samples for each request helps minimize the risk of performance degradation.
						</li></ul></div></div></rh-alert><section class="section" id="about-accessing-monitoring-web-service-apis_accessing-monitoring-apis-by-using-the-cli"><div class="titlepage"><div><div><h4 class="title">5.3.1. About accessing monitoring web service APIs</h4></div></div></div><p>
					You can directly access web service API endpoints from the command line for the following monitoring stack components:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Prometheus
						</li><li class="listitem">
							Alertmanager
						</li><li class="listitem">
							Thanos Ruler
						</li><li class="listitem">
							Thanos Querier
						</li></ul></div><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
						To access Thanos Ruler and Thanos Querier service APIs, the requesting account must have <code class="literal">get</code> permission on the namespaces resource, which can be granted by binding the <code class="literal">cluster-monitoring-view</code> cluster role to the account.
					</p></div></rh-alert><p>
					When you access web service API endpoints for monitoring components, be aware of the following limitations:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							You can only use bearer token authentication to access API endpoints.
						</li><li class="listitem">
							You can only access endpoints in the <code class="literal">/api</code> path for a route. If you try to access an API endpoint in a web browser, an <code class="literal">Application is not available</code> error occurs. To access monitoring features in a web browser, use the OpenShift Container Platform web console to review monitoring dashboards.
						</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#reviewing-monitoring-dashboards-admin_accessing-metrics-as-an-administrator" title="5.1.4. Reviewing monitoring dashboards as a cluster administrator">Reviewing monitoring dashboards as a cluster administrator</a>
</li><li class="listitem">
<a class="link" href="#reviewing-monitoring-dashboards-developer_accessing-metrics-as-a-developer" title="5.2.3. Reviewing monitoring dashboards as a developer">Reviewing monitoring dashboards as a developer</a>
</li></ul></div></section><section class="section" id="accessing-a-monitoring-web-service-api_accessing-monitoring-apis-by-using-the-cli"><div class="titlepage"><div><div><h4 class="title">5.3.2. Accessing a monitoring web service API</h4></div></div></div><p>
					The following example shows how to query the service API receivers for the Alertmanager service used in core platform monitoring. You can use a similar method to access the <code class="literal">prometheus-k8s</code> service for core platform Prometheus and the <code class="literal">thanos-ruler</code> service for Thanos Ruler.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You are logged in to an account that is bound against the <code class="literal">monitoring-alertmanager-edit</code> role in the <code class="literal">openshift-monitoring</code> namespace.
						</li><li class="listitem"><p class="simpara">
							You are logged in to an account that has permission to get the Alertmanager API route.
						</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
								If your account does not have permission to get the Alertmanager API route, a cluster administrator can provide the URL for the route.
							</p></div></rh-alert></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Extract an authentication token by running the following command:
						</p><pre class="programlisting language-terminal">$ TOKEN=$(oc whoami -t)</pre></li><li class="listitem"><p class="simpara">
							Extract the <code class="literal">alertmanager-main</code> API route URL by running the following command:
						</p><pre class="programlisting language-terminal">$ HOST=$(oc -n openshift-monitoring get route alertmanager-main -ojsonpath='{.status.ingress[].host}')</pre></li><li class="listitem"><p class="simpara">
							Query the service API receivers for Alertmanager by running the following command:
						</p><pre class="programlisting language-terminal">$ curl -H "Authorization: Bearer $TOKEN" -k "https://$HOST/api/v2/receivers"</pre></li></ol></div></section><section class="section" id="monitoring-querying-metrics-by-using-the-federation-endpoint-for-prometheus_accessing-monitoring-apis-by-using-the-cli"><div class="titlepage"><div><div><h4 class="title">5.3.3. Querying metrics by using the federation endpoint for Prometheus</h4></div></div></div><p>
					You can use the federation endpoint for Prometheus to scrape platform and user-defined metrics from a network location outside the cluster. To do so, access the Prometheus <code class="literal">/federate</code> endpoint for the cluster via an OpenShift Container Platform route.
				</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
						A delay in retrieving metrics data occurs when you use federation. This delay can affect the accuracy and timeliness of the scraped metrics.
					</p><p>
						Using the federation endpoint can also degrade the performance and scalability of your cluster, especially if you use the federation endpoint to retrieve large amounts of metrics data. To avoid these issues, follow these recommendations:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Do not try to retrieve all metrics data via the federation endpoint for Prometheus. Query it only when you want to retrieve a limited, aggregated data set. For example, retrieving fewer than 1,000 samples for each request helps minimize the risk of performance degradation.
							</li><li class="listitem">
								Avoid frequent querying of the federation endpoint for Prometheus. Limit queries to a maximum of one every 30 seconds.
							</li></ul></div><p>
						If you need to forward large amounts of data outside the cluster, use remote write instead. For more information, see the <span class="emphasis"><em>Configuring remote write storage</em></span> section.
					</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have installed the OpenShift CLI (<code class="literal">oc</code>).
						</li><li class="listitem"><p class="simpara">
							You have access to the cluster as a user with the <code class="literal">cluster-monitoring-view</code> cluster role or have obtained a bearer token with <code class="literal">get</code> permission on the <code class="literal">namespaces</code> resource.
						</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
								You can only use bearer token authentication to access the Prometheus federation endpoint.
							</p></div></rh-alert></li><li class="listitem"><p class="simpara">
							You are logged in to an account that has permission to get the Prometheus federation route.
						</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
								If your account does not have permission to get the Prometheus federation route, a cluster administrator can provide the URL for the route.
							</p></div></rh-alert></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Retrieve the bearer token by running the following the command:
						</p><pre class="programlisting language-terminal">$ TOKEN=$(oc whoami -t)</pre></li><li class="listitem"><p class="simpara">
							Get the Prometheus federation route URL by running the following command:
						</p><pre class="programlisting language-terminal">$ HOST=$(oc -n openshift-monitoring get route prometheus-k8s-federate -ojsonpath='{.status.ingress[].host}')</pre></li><li class="listitem"><p class="simpara">
							Query metrics from the <code class="literal">/federate</code> route. The following example command queries <code class="literal">up</code> metrics:
						</p><pre class="programlisting language-terminal">$ curl -G -k -H "Authorization: Bearer $TOKEN" https://$HOST/federate --data-urlencode 'match[]=up'</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
</p><pre class="programlisting language-terminal"># TYPE up untyped
up{apiserver="kube-apiserver",endpoint="https",instance="10.0.143.148:6443",job="apiserver",namespace="default",service="kubernetes",prometheus="openshift-monitoring/k8s",prometheus_replica="prometheus-k8s-0"} 1 1657035322214
up{apiserver="kube-apiserver",endpoint="https",instance="10.0.148.166:6443",job="apiserver",namespace="default",service="kubernetes",prometheus="openshift-monitoring/k8s",prometheus_replica="prometheus-k8s-0"} 1 1657035338597
up{apiserver="kube-apiserver",endpoint="https",instance="10.0.173.16:6443",job="apiserver",namespace="default",service="kubernetes",prometheus="openshift-monitoring/k8s",prometheus_replica="prometheus-k8s-0"} 1 1657035343834
...</pre>
<p></p></div></li></ol></div></section><section class="section" id="accessing-metrics-from-outside-cluster_accessing-monitoring-apis-by-using-the-cli"><div class="titlepage"><div><div><h4 class="title">5.3.4. Accessing metrics from outside the cluster for custom applications</h4></div></div></div><p>
					You can query Prometheus metrics from outside the cluster when monitoring your own services with user-defined projects. Access this data from outside the cluster by using the <code class="literal">thanos-querier</code> route.
				</p><p>
					This access only supports using a bearer token for authentication.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have deployed your own service, following the "Enabling monitoring for user-defined projects" procedure.
						</li><li class="listitem">
							You are logged in to an account with the <code class="literal">cluster-monitoring-view</code> cluster role, which provides permission to access the Thanos Querier API.
						</li><li class="listitem"><p class="simpara">
							You are logged in to an account that has permission to get the Thanos Querier API route.
						</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
								If your account does not have permission to get the Thanos Querier API route, a cluster administrator can provide the URL for the route.
							</p></div></rh-alert></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
							Extract an authentication token to connect to Prometheus by running the following command:
						</p><pre class="programlisting language-terminal">$ TOKEN=$(oc whoami -t)</pre></li><li class="listitem"><p class="simpara">
							Extract the <code class="literal">thanos-querier</code> API route URL by running the following command:
						</p><pre class="programlisting language-terminal">$ HOST=$(oc -n openshift-monitoring get route thanos-querier -ojsonpath='{.status.ingress[].host}')</pre></li><li class="listitem"><p class="simpara">
							Set the namespace to the namespace in which your service is running by using the following command:
						</p><pre class="programlisting language-terminal">$ NAMESPACE=ns1</pre></li><li class="listitem"><p class="simpara">
							Query the metrics of your own services in the command line by running the following command:
						</p><pre class="programlisting language-terminal">$ curl -H "Authorization: Bearer $TOKEN" -k "https://$HOST/api/v1/query?" --data-urlencode "query=up{namespace='$NAMESPACE'}"</pre><p class="simpara">
							The output shows the status for each application pod that Prometheus is scraping:
						</p><div class="formalpara"><p class="title"><strong>The formatted example output</strong></p><p>
</p><pre class="programlisting language-terminal">{
  "status": "success",
  "data": {
    "resultType": "vector",
    "result": [
      {
        "metric": {
          "__name__": "up",
          "endpoint": "web",
          "instance": "10.129.0.46:8080",
          "job": "prometheus-example-app",
          "namespace": "ns1",
          "pod": "prometheus-example-app-68d47c4fb6-jztp2",
          "service": "prometheus-example-app"
        },
        "value": [
          1591881154.748,
          "1"
        ]
      }
    ],
  }
}</pre>
<p></p></div><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
										The formatted example output uses a filtering tool, such as <code class="literal">jq</code>, to provide the formatted indented JSON. See the <a class="link" href="https://stedolan.github.io/jq/manual/">jq Manual</a> (jq documentation) for more information about using <code class="literal">jq</code>.
									</li><li class="listitem">
										The command requests an instant query endpoint of the Thanos Querier service, which evaluates selectors at one point in time.
									</li></ul></div></div></rh-alert></li></ol></div></section><section class="section" id="resources-reference-for-the-cluster-monitoring-operator_accessing-monitoring-apis-by-using-the-cli"><div class="titlepage"><div><div><h4 class="title">5.3.5. Resources reference for the Cluster Monitoring Operator</h4></div></div></div><p>
					This document describes the following resources deployed and managed by the Cluster Monitoring Operator (CMO):
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<a class="link" href="#cmo-routes-resources_accessing-monitoring-apis-by-using-the-cli" title="5.3.5.1. CMO routes resources">Routes</a>
</li><li class="listitem">
<a class="link" href="#cmo-services-resources_accessing-monitoring-apis-by-using-the-cli" title="5.3.5.2. CMO services resources">Services</a>
</li></ul></div><p>
					Use this information when you want to configure API endpoint connections to retrieve, send, or query metrics data.
				</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
						In certain situations, accessing endpoints can degrade the performance and scalability of your cluster, especially if you use endpoints to retrieve, send, or query large amounts of metrics data.
					</p><p>
						To avoid these issues, follow these recommendations:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								Avoid querying endpoints frequently. Limit queries to a maximum of one every 30 seconds.
							</li><li class="listitem">
								Do not try to retrieve all metrics data via the <code class="literal">/federate</code> endpoint. Query it only when you want to retrieve a limited, aggregated data set. For example, retrieving fewer than 1,000 samples for each request helps minimize the risk of performance degradation.
							</li></ul></div></div></rh-alert><section class="section" id="cmo-routes-resources_accessing-monitoring-apis-by-using-the-cli"><div class="titlepage"><div><div><h5 class="title">5.3.5.1. CMO routes resources</h5></div></div></div><section class="section" id="openshift-monitoring-alertmanager-main"><div class="titlepage"><div><div><h5 class="title">5.3.5.1.1. openshift-monitoring/alertmanager-main</h5></div></div></div><p>
							Expose the <code class="literal">/api</code> endpoints of the <code class="literal">alertmanager-main</code> service via a router.
						</p></section><section class="section" id="openshift-monitoring-prometheus-k8s"><div class="titlepage"><div><div><h5 class="title">5.3.5.1.2. openshift-monitoring/prometheus-k8s</h5></div></div></div><p>
							Expose the <code class="literal">/api</code> endpoints of the <code class="literal">prometheus-k8s</code> service via a router.
						</p></section><section class="section" id="openshift-monitoring-prometheus-k8s-federate"><div class="titlepage"><div><div><h5 class="title">5.3.5.1.3. openshift-monitoring/prometheus-k8s-federate</h5></div></div></div><p>
							Expose the <code class="literal">/federate</code> endpoint of the <code class="literal">prometheus-k8s</code> service via a router.
						</p></section><section class="section" id="openshift-user-workload-monitoring-federate"><div class="titlepage"><div><div><h5 class="title">5.3.5.1.4. openshift-user-workload-monitoring/federate</h5></div></div></div><p>
							Expose the <code class="literal">/federate</code> endpoint of the <code class="literal">prometheus-user-workload</code> service via a router.
						</p></section><section class="section" id="openshift-monitoring-thanos-querier"><div class="titlepage"><div><div><h5 class="title">5.3.5.1.5. openshift-monitoring/thanos-querier</h5></div></div></div><p>
							Expose the <code class="literal">/api</code> endpoints of the <code class="literal">thanos-querier</code> service via a router.
						</p></section><section class="section" id="openshift-user-workload-monitoring-thanos-ruler"><div class="titlepage"><div><div><h5 class="title">5.3.5.1.6. openshift-user-workload-monitoring/thanos-ruler</h5></div></div></div><p>
							Expose the <code class="literal">/api</code> endpoints of the <code class="literal">thanos-ruler</code> service via a router.
						</p></section></section><section class="section" id="cmo-services-resources_accessing-monitoring-apis-by-using-the-cli"><div class="titlepage"><div><div><h5 class="title">5.3.5.2. CMO services resources</h5></div></div></div><section class="section" id="openshift-monitoring-prometheus-operator-admission-webhook"><div class="titlepage"><div><div><h5 class="title">5.3.5.2.1. openshift-monitoring/prometheus-operator-admission-webhook</h5></div></div></div><p>
							Expose the admission webhook service which validates <code class="literal">PrometheusRules</code> and <code class="literal">AlertmanagerConfig</code> custom resources on port 8443.
						</p></section><section class="section" id="openshift-user-workload-monitoring-alertmanager-user-workload"><div class="titlepage"><div><div><h5 class="title">5.3.5.2.2. openshift-user-workload-monitoring/alertmanager-user-workload</h5></div></div></div><p>
							Expose the user-defined Alertmanager web server within the cluster on the following ports:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Port 9095 provides access to the Alertmanager endpoints. Granting access requires binding a user to the <code class="literal">monitoring-alertmanager-api-reader</code> role (for read-only operations) or <code class="literal">monitoring-alertmanager-api-writer</code> role in the <code class="literal">openshift-user-workload-monitoring</code> project.
								</li><li class="listitem">
									Port 9092 provides access to the Alertmanager endpoints restricted to a given project. Granting access requires binding a user to the <code class="literal">monitoring-rules-edit</code> cluster role or <code class="literal">monitoring-edit</code> cluster role in the project.
								</li><li class="listitem">
									Port 9097 provides access to the <code class="literal">/metrics</code> endpoint only. This port is for internal use, and no other usage is guaranteed.
								</li></ul></div></section><section class="section" id="openshift-monitoring-alertmanager-main-2"><div class="titlepage"><div><div><h5 class="title">5.3.5.2.3. openshift-monitoring/alertmanager-main</h5></div></div></div><p>
							Expose the Alertmanager web server within the cluster on the following ports:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Port 9094 provides access to all the Alertmanager endpoints. Granting access requires binding a user to the <code class="literal">monitoring-alertmanager-view</code> (for read-only operations) or <code class="literal">monitoring-alertmanager-edit</code> role in the <code class="literal">openshift-monitoring</code> project.
								</li><li class="listitem">
									Port 9092 provides access to the Alertmanager endpoints restricted to a given project. Granting access requires binding a user to the <code class="literal">monitoring-rules-edit</code> cluster role or <code class="literal">monitoring-edit</code> cluster role in the project.
								</li><li class="listitem">
									Port 9097 provides access to the <code class="literal">/metrics</code> endpoint only. This port is for internal use, and no other usage is guaranteed.
								</li></ul></div></section><section class="section" id="openshift-monitoring-kube-state-metrics"><div class="titlepage"><div><div><h5 class="title">5.3.5.2.4. openshift-monitoring/kube-state-metrics</h5></div></div></div><p>
							Expose kube-state-metrics <code class="literal">/metrics</code> endpoints within the cluster on the following ports:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Port 8443 provides access to the Kubernetes resource metrics. This port is for internal use, and no other usage is guaranteed.
								</li><li class="listitem">
									Port 9443 provides access to the internal kube-state-metrics metrics. This port is for internal use, and no other usage is guaranteed.
								</li></ul></div></section><section class="section" id="openshift-monitoring-metrics-server"><div class="titlepage"><div><div><h5 class="title">5.3.5.2.5. openshift-monitoring/metrics-server</h5></div></div></div><p>
							Expose the metrics-server web server on port 443. This port is for internal use, and no other usage is guaranteed.
						</p></section><section class="section" id="openshift-monitoring-monitoring-plugin"><div class="titlepage"><div><div><h5 class="title">5.3.5.2.6. openshift-monitoring/monitoring-plugin</h5></div></div></div><p>
							Expose the monitoring plugin service on port 9443. This port is for internal use, and no other usage is guaranteed.
						</p></section><section class="section" id="openshift-monitoring-node-exporter"><div class="titlepage"><div><div><h5 class="title">5.3.5.2.7. openshift-monitoring/node-exporter</h5></div></div></div><p>
							Expose the <code class="literal">/metrics</code> endpoint on port 9100. This port is for internal use, and no other usage is guaranteed.
						</p></section><section class="section" id="openshift-monitoring-openshift-state-metrics"><div class="titlepage"><div><div><h5 class="title">5.3.5.2.8. openshift-monitoring/openshift-state-metrics</h5></div></div></div><p>
							Expose openshift-state-metrics <code class="literal">/metrics</code> endpoints within the cluster on the following ports:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Port 8443 provides access to the OpenShift resource metrics. This port is for internal use, and no other usage is guaranteed.
								</li><li class="listitem">
									Port 9443 provides access to the internal <code class="literal">openshift-state-metrics</code> metrics. This port is for internal use, and no other usage is guaranteed.
								</li></ul></div></section><section class="section" id="openshift-monitoring-prometheus-k8s-2"><div class="titlepage"><div><div><h5 class="title">5.3.5.2.9. openshift-monitoring/prometheus-k8s</h5></div></div></div><p>
							Expose the Prometheus web server within the cluster on the following ports:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Port 9091 provides access to all the Prometheus endpoints. Granting access requires binding a user to the <code class="literal">cluster-monitoring-view</code> cluster role.
								</li><li class="listitem">
									Port 9092 provides access to the <code class="literal">/metrics</code> and <code class="literal">/federate</code> endpoints only. This port is for internal use, and no other usage is guaranteed.
								</li></ul></div></section><section class="section" id="openshift-user-workload-monitoring-prometheus-operator"><div class="titlepage"><div><div><h5 class="title">5.3.5.2.10. openshift-user-workload-monitoring/prometheus-operator</h5></div></div></div><p>
							Expose the <code class="literal">/metrics</code> endpoint on port 8443. This port is for internal use, and no other usage is guaranteed.
						</p></section><section class="section" id="openshift-monitoring-prometheus-operator"><div class="titlepage"><div><div><h5 class="title">5.3.5.2.11. openshift-monitoring/prometheus-operator</h5></div></div></div><p>
							Expose the <code class="literal">/metrics</code> endpoint on port 8443. This port is for internal use, and no other usage is guaranteed.
						</p></section><section class="section" id="openshift-user-workload-monitoring-prometheus-user-workload"><div class="titlepage"><div><div><h5 class="title">5.3.5.2.12. openshift-user-workload-monitoring/prometheus-user-workload</h5></div></div></div><p>
							Expose the Prometheus web server within the cluster on the following ports:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Port 9091 provides access to the <code class="literal">/metrics</code> endpoint only. This port is for internal use, and no other usage is guaranteed.
								</li><li class="listitem">
									Port 9092 provides access to the <code class="literal">/federate</code> endpoint only. Granting access requires binding a user to the <code class="literal">cluster-monitoring-view</code> cluster role.
								</li></ul></div><p>
							This also exposes the <code class="literal">/metrics</code> endpoint of the Thanos sidecar web server on port 10902. This port is for internal use, and no other usage is guaranteed.
						</p></section><section class="section" id="openshift-monitoring-telemeter-client"><div class="titlepage"><div><div><h5 class="title">5.3.5.2.13. openshift-monitoring/telemeter-client</h5></div></div></div><p>
							Expose the <code class="literal">/metrics</code> endpoint on port 8443. This port is for internal use, and no other usage is guaranteed.
						</p></section><section class="section" id="openshift-monitoring-thanos-querier-2"><div class="titlepage"><div><div><h5 class="title">5.3.5.2.14. openshift-monitoring/thanos-querier</h5></div></div></div><p>
							Expose the Thanos Querier web server within the cluster on the following ports:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Port 9091 provides access to all the Thanos Querier endpoints. Granting access requires binding a user to the <code class="literal">cluster-monitoring-view</code> cluster role.
								</li><li class="listitem">
									Port 9092 provides access to the <code class="literal">/api/v1/query</code>, <code class="literal">/api/v1/query_range/</code>, <code class="literal">/api/v1/labels</code>, <code class="literal">/api/v1/label/*/values</code>, and <code class="literal">/api/v1/series</code> endpoints restricted to a given project. Granting access requires binding a user to the <code class="literal">view</code> cluster role in the project.
								</li><li class="listitem">
									Port 9093 provides access to the <code class="literal">/api/v1/alerts</code>, and <code class="literal">/api/v1/rules</code> endpoints restricted to a given project. Granting access requires binding a user to the <code class="literal">monitoring-rules-edit</code>, <code class="literal">monitoring-edit</code>, or <code class="literal">monitoring-rules-view</code> cluster role in the project.
								</li><li class="listitem">
									Port 9094 provides access to the <code class="literal">/metrics</code> endpoint only. This port is for internal use, and no other usage is guaranteed.
								</li></ul></div></section><section class="section" id="openshift-user-workload-monitoring-thanos-ruler-2"><div class="titlepage"><div><div><h5 class="title">5.3.5.2.15. openshift-user-workload-monitoring/thanos-ruler</h5></div></div></div><p>
							Expose the Thanos Ruler web server within the cluster on the following ports:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Port 9091 provides access to all Thanos Ruler endpoints. Granting access requires binding a user to the <code class="literal">cluster-monitoring-view</code> cluster role.
								</li><li class="listitem">
									Port 9092 provides access to the <code class="literal">/metrics</code> endpoint only. This port is for internal use, and no other usage is guaranteed.
								</li></ul></div><p>
							This also exposes the gRPC endpoints on port 10901. This port is for internal use, and no other usage is guaranteed.
						</p></section><section class="section" id="openshift-monitoring-cluster-monitoring-operator"><div class="titlepage"><div><div><h5 class="title">5.3.5.2.16. openshift-monitoring/cluster-monitoring-operator</h5></div></div></div><p>
							Expose the <code class="literal">/metrics</code> and <code class="literal">/validate-webhook</code> endpoints on port 8443. This port is for internal use, and no other usage is guaranteed.
						</p></section></section></section><section class="section _additional-resources" id="additional-resources_accessing-monitoring-apis-by-using-the-cli"><div class="titlepage"><div><div><h4 class="title">5.3.6. Additional resources</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<a class="link" href="#enabling-monitoring-for-user-defined-projects-uwm_preparing-to-configure-the-monitoring-stack-uwm" title="4.1.2. Enabling monitoring for user-defined projects">Enabling monitoring for user-defined projects</a>
</li><li class="listitem">
<a class="link" href="#configuring-remote-write-storage_configuring-metrics" title="3.4.1. Configuring remote write storage">Configuring remote write storage for core platform monitoring</a>
</li><li class="listitem">
<a class="link" href="#configuring-remote-write-storage_configuring-metrics-uwm" title="4.4.1. Configuring remote write storage">Configuring remote write storage for monitoring of user-defined projects</a>
</li><li class="listitem">
<a class="link" href="#accessing-metrics-as-an-administrator" title="5.1. Accessing metrics as an administrator">Accessing metrics as an administrator</a>
</li><li class="listitem">
<a class="link" href="#accessing-metrics-as-a-developer" title="5.2. Accessing metrics as a developer">Accessing metrics as a developer</a>
</li><li class="listitem">
<a class="link" href="#managing-alerts-as-an-administrator" title="6.1. Managing alerts as an Administrator">Managing alerts as an Administrator</a>
</li><li class="listitem">
<a class="link" href="#managing-alerts-as-a-developer" title="6.2. Managing alerts as a Developer">Managing alerts as a Developer</a>
</li></ul></div></section></section></section><section class="chapter" id="managing-alerts"><div class="titlepage"><div><div><h2 class="title">Chapter 6. Managing alerts</h2></div></div></div><section class="section" id="managing-alerts-as-an-administrator"><div class="titlepage"><div><div><h3 class="title">6.1. Managing alerts as an Administrator</h3></div></div></div><p>
				In OpenShift Container Platform, the Alerting UI enables you to manage alerts, silences, and alerting rules.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					The alerts, silences, and alerting rules that are available in the Alerting UI relate to the projects that you have access to. For example, if you are logged in as a user with the <code class="literal">cluster-admin</code> role, you can access all alerts, silences, and alerting rules.
				</p></div></rh-alert><section class="section" id="monitoring-accessing-the-alerting-ui-adm_managing-alerts-as-an-administrator"><div class="titlepage"><div><div><h4 class="title">6.1.1. Accessing the Alerting UI from the Administrator perspective</h4></div></div></div><p>
					The Alerting UI is accessible through the <span class="strong strong"><strong>Administrator</strong></span> perspective of the OpenShift Container Platform web console.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							From the <span class="strong strong"><strong>Administrator</strong></span> perspective, go to <span class="strong strong"><strong>Observe</strong></span> → <span class="strong strong"><strong>Alerting</strong></span>. The three main pages in the Alerting UI in this perspective are the <span class="strong strong"><strong>Alerts</strong></span>, <span class="strong strong"><strong>Silences</strong></span>, and <span class="strong strong"><strong>Alerting rules</strong></span> pages.
						</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#searching-alerts-silences-and-alerting-rules_key-concepts" title="1.3.5.7. Searching and filtering alerts, silences, and alerting rules">Searching and filtering alerts, silences, and alerting rules</a>
</li></ul></div></section><section class="section" id="getting-information-about-alerts-silences-and-alerting-rules-adm_managing-alerts-as-an-administrator"><div class="titlepage"><div><div><h4 class="title">6.1.2. Getting information about alerts, silences, and alerting rules from the Administrator perspective</h4></div></div></div><p>
					The Alerting UI provides detailed information about alerts and their governing alerting rules and silences.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with view permissions for the project that you are viewing alerts for.
						</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To obtain information about alerts:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							From the <span class="strong strong"><strong>Administrator</strong></span> perspective of the OpenShift Container Platform web console, go to the <span class="strong strong"><strong>Observe</strong></span> → <span class="strong strong"><strong>Alerting</strong></span> → <span class="strong strong"><strong>Alerts</strong></span> page.
						</li><li class="listitem">
							Optional: Search for alerts by name by using the <span class="strong strong"><strong>Name</strong></span> field in the search list.
						</li><li class="listitem">
							Optional: Filter alerts by state, severity, and source by selecting filters in the <span class="strong strong"><strong>Filter</strong></span> list.
						</li><li class="listitem">
							Optional: Sort the alerts by clicking one or more of the <span class="strong strong"><strong>Name</strong></span>, <span class="strong strong"><strong>Severity</strong></span>, <span class="strong strong"><strong>State</strong></span>, and <span class="strong strong"><strong>Source</strong></span> column headers.
						</li><li class="listitem"><p class="simpara">
							Click the name of an alert to view its <span class="strong strong"><strong>Alert details</strong></span> page. The page includes a graph that illustrates alert time series data. It also provides the following information about the alert:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									A description of the alert
								</li><li class="listitem">
									Messages associated with the alert
								</li><li class="listitem">
									Labels attached to the alert
								</li><li class="listitem">
									A link to its governing alerting rule
								</li><li class="listitem">
									Silences for the alert, if any exist
								</li></ul></div></li></ol></div><p>
					To obtain information about silences:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							From the <span class="strong strong"><strong>Administrator</strong></span> perspective of the OpenShift Container Platform web console, go to the <span class="strong strong"><strong>Observe</strong></span> → <span class="strong strong"><strong>Alerting</strong></span> → <span class="strong strong"><strong>Silences</strong></span> page.
						</li><li class="listitem">
							Optional: Filter the silences by name using the <span class="strong strong"><strong>Search by name</strong></span> field.
						</li><li class="listitem">
							Optional: Filter silences by state by selecting filters in the <span class="strong strong"><strong>Filter</strong></span> list. By default, <span class="strong strong"><strong>Active</strong></span> and <span class="strong strong"><strong>Pending</strong></span> filters are applied.
						</li><li class="listitem">
							Optional: Sort the silences by clicking one or more of the <span class="strong strong"><strong>Name</strong></span>, <span class="strong strong"><strong>Firing alerts</strong></span>, <span class="strong strong"><strong>State</strong></span>, and <span class="strong strong"><strong>Creator</strong></span> column headers.
						</li><li class="listitem"><p class="simpara">
							Select the name of a silence to view its <span class="strong strong"><strong>Silence details</strong></span> page. The page includes the following details:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Alert specification
								</li><li class="listitem">
									Start time
								</li><li class="listitem">
									End time
								</li><li class="listitem">
									Silence state
								</li><li class="listitem">
									Number and list of firing alerts
								</li></ul></div></li></ol></div><p>
					To obtain information about alerting rules:
				</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							From the <span class="strong strong"><strong>Administrator</strong></span> perspective of the OpenShift Container Platform web console, go to the <span class="strong strong"><strong>Observe</strong></span> → <span class="strong strong"><strong>Alerting</strong></span> → <span class="strong strong"><strong>Alerting rules</strong></span> page.
						</li><li class="listitem">
							Optional: Filter alerting rules by state, severity, and source by selecting filters in the <span class="strong strong"><strong>Filter</strong></span> list.
						</li><li class="listitem">
							Optional: Sort the alerting rules by clicking one or more of the <span class="strong strong"><strong>Name</strong></span>, <span class="strong strong"><strong>Severity</strong></span>, <span class="strong strong"><strong>Alert state</strong></span>, and <span class="strong strong"><strong>Source</strong></span> column headers.
						</li><li class="listitem"><p class="simpara">
							Select the name of an alerting rule to view its <span class="strong strong"><strong>Alerting rule details</strong></span> page. The page provides the following details about the alerting rule:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									Alerting rule name, severity, and description.
								</li><li class="listitem">
									The expression that defines the condition for firing the alert.
								</li><li class="listitem">
									The time for which the condition should be true for an alert to fire.
								</li><li class="listitem">
									A graph for each alert governed by the alerting rule, showing the value with which the alert is firing.
								</li><li class="listitem">
									A table of all alerts governed by the alerting rule.
								</li></ul></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="https://github.com/openshift/runbooks/tree/master/alerts/cluster-monitoring-operator">Cluster Monitoring Operator runbooks</a> (Cluster Monitoring Operator GitHub repository)
						</li></ul></div></section><section class="section" id="managing-silences_managing-alerts-as-an-administrator"><div class="titlepage"><div><div><h4 class="title">6.1.3. Managing silences</h4></div></div></div><p>
					You can create a silence for an alert in the OpenShift Container Platform web console in the <span class="strong strong"><strong>Administrator</strong></span> perspective. After you create silences, you can view, edit, and expire them. You also do not receive notifications about a silenced alert when the alert fires.
				</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						When you create silences, they are replicated across Alertmanager pods. However, if you do not configure persistent storage for Alertmanager, silences might be lost. This can happen, for example, if all Alertmanager pods restart at the same time.
					</p></div></rh-alert><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#managing-silences_key-concepts" title="1.3.5.1. Managing silences">Managing silences</a>
</li><li class="listitem">
<a class="link" href="#configuring-persistent-storage_storing-and-recording-data" title="3.3.1. Configuring persistent storage">Configuring persistent storage</a>
</li></ul></div><section class="section" id="silencing-alerts-adm_managing-alerts-as-an-administrator"><div class="titlepage"><div><div><h5 class="title">6.1.3.1. Silencing alerts from the Administrator perspective</h5></div></div></div><p>
						You can silence a specific alert or silence alerts that match a specification that you define.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							To silence a specific alert:
						</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								From the <span class="strong strong"><strong>Administrator</strong></span> perspective of the OpenShift Container Platform web console, go to <span class="strong strong"><strong>Observe</strong></span> → <span class="strong strong"><strong>Alerting</strong></span> → <span class="strong strong"><strong>Alerts</strong></span>.
							</li><li class="listitem">
								For the alert that you want to silence, click 
								<span class="inlinemediaobject"><img alt="kebab" src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.18-Monitoring-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png"/></span>
								 and select <span class="strong strong"><strong>Silence alert</strong></span> to open the <span class="strong strong"><strong>Silence alert</strong></span> page with a default configuration for the chosen alert.
							</li><li class="listitem"><p class="simpara">
								Optional: Change the default configuration details for the silence.
							</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
									You must add a comment before saving a silence.
								</p></div></rh-alert></li><li class="listitem">
								To save the silence, click <span class="strong strong"><strong>Silence</strong></span>.
							</li></ol></div><p>
						To silence a set of alerts:
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								From the <span class="strong strong"><strong>Administrator</strong></span> perspective of the OpenShift Container Platform web console, go to <span class="strong strong"><strong>Observe</strong></span> → <span class="strong strong"><strong>Alerting</strong></span> → <span class="strong strong"><strong>Silences</strong></span>.
							</li><li class="listitem">
								Click <span class="strong strong"><strong>Create silence</strong></span>.
							</li><li class="listitem"><p class="simpara">
								On the <span class="strong strong"><strong>Create silence</strong></span> page, set the schedule, duration, and label details for an alert.
							</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
									You must add a comment before saving a silence.
								</p></div></rh-alert></li><li class="listitem">
								To create silences for alerts that match the labels that you entered, click <span class="strong strong"><strong>Silence</strong></span>.
							</li></ol></div></section><section class="section" id="editing-silences-adm_managing-alerts-as-an-administrator"><div class="titlepage"><div><div><h5 class="title">6.1.3.2. Editing silences from the Administrator perspective</h5></div></div></div><p>
						You can edit a silence, which expires the existing silence and creates a new one with the changed configuration.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								If you are a cluster administrator, you have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem"><p class="simpara">
								If you are a non-administrator user, you have access to the cluster as a user with the following user roles:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										The <code class="literal">cluster-monitoring-view</code> cluster role, which allows you to access Alertmanager.
									</li><li class="listitem">
										The <code class="literal">monitoring-alertmanager-edit</code> role, which permits you to create and silence alerts in the <span class="strong strong"><strong>Administrator</strong></span> perspective in the web console.
									</li></ul></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								From the <span class="strong strong"><strong>Administrator</strong></span> perspective of the OpenShift Container Platform web console, go to <span class="strong strong"><strong>Observe</strong></span> → <span class="strong strong"><strong>Alerting</strong></span> → <span class="strong strong"><strong>Silences</strong></span>.
							</li><li class="listitem"><p class="simpara">
								For the silence you want to modify, click 
								<span class="inlinemediaobject"><img alt="kebab" src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.18-Monitoring-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png"/></span>
								 and select <span class="strong strong"><strong>Edit silence</strong></span>.
							</p><p class="simpara">
								Alternatively, you can click <span class="strong strong"><strong>Actions</strong></span> and select <span class="strong strong"><strong>Edit silence</strong></span> on the <span class="strong strong"><strong>Silence details</strong></span> page for a silence.
							</p></li><li class="listitem">
								On the <span class="strong strong"><strong>Edit silence</strong></span> page, make changes and click <span class="strong strong"><strong>Silence</strong></span>. Doing so expires the existing silence and creates one with the updated configuration.
							</li></ol></div></section><section class="section" id="expiring-silences-adm_managing-alerts-as-an-administrator"><div class="titlepage"><div><div><h5 class="title">6.1.3.3. Expiring silences from the Administrator perspective</h5></div></div></div><p>
						You can expire a single silence or multiple silences. Expiring a silence deactivates it permanently.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							You cannot delete expired, silenced alerts. Expired silences older than 120 hours are garbage collected.
						</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								If you are a cluster administrator, you have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem"><p class="simpara">
								If you are a non-administrator user, you have access to the cluster as a user with the following user roles:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										The <code class="literal">cluster-monitoring-view</code> cluster role, which allows you to access Alertmanager.
									</li><li class="listitem">
										The <code class="literal">monitoring-alertmanager-edit</code> role, which permits you to create and silence alerts in the <span class="strong strong"><strong>Administrator</strong></span> perspective in the web console.
									</li></ul></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Go to <span class="strong strong"><strong>Observe</strong></span> → <span class="strong strong"><strong>Alerting</strong></span> → <span class="strong strong"><strong>Silences</strong></span>.
							</li><li class="listitem">
								For the silence or silences you want to expire, select the checkbox in the corresponding row.
							</li><li class="listitem"><p class="simpara">
								Click <span class="strong strong"><strong>Expire 1 silence</strong></span> to expire a single selected silence or <span class="strong strong"><strong>Expire <span class="emphasis"><em>&lt;n&gt;</em></span> silences</strong></span> to expire multiple selected silences, where <span class="emphasis"><em>&lt;n&gt;</em></span> is the number of silences you selected.
							</p><p class="simpara">
								Alternatively, to expire a single silence you can click <span class="strong strong"><strong>Actions</strong></span> and select <span class="strong strong"><strong>Expire silence</strong></span> on the <span class="strong strong"><strong>Silence details</strong></span> page for a silence.
							</p></li></ol></div></section></section><section class="section" id="managing-alerting-rules-for-core-platform-monitoring_managing-alerts-as-an-administrator"><div class="titlepage"><div><div><h4 class="title">6.1.4. Managing alerting rules for core platform monitoring</h4></div></div></div><p>
					The OpenShift Container Platform monitoring includes a large set of default alerting rules for platform metrics. As a cluster administrator, you can customize this set of rules in two ways:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Modify the settings for existing platform alerting rules by adjusting thresholds or by adding and modifying labels. For example, you can change the <code class="literal">severity</code> label for an alert from <code class="literal">warning</code> to <code class="literal">critical</code> to help you route and triage issues flagged by an alert.
						</li><li class="listitem">
							Define and add new custom alerting rules by constructing a query expression based on core platform metrics in the <code class="literal">openshift-monitoring</code> project.
						</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#managing-core-platform-alerting-rules_key-concepts" title="1.3.5.2. Managing alerting rules for core platform monitoring">Managing alerting rules for core platform monitoring</a>
</li><li class="listitem">
<a class="link" href="#tips-for-optimizing-alerting-rules-for-core-platform-monitoring_key-concepts" title="1.3.5.3. Tips for optimizing alerting rules for core platform monitoring">Tips for optimizing alerting rules for core platform monitoring</a>
</li></ul></div><section class="section" id="creating-new-alerting-rules_managing-alerts-as-an-administrator"><div class="titlepage"><div><div><h5 class="title">6.1.4.1. Creating new alerting rules</h5></div></div></div><p>
						As a cluster administrator, you can create new alerting rules based on platform metrics. These alerting rules trigger alerts based on the values of chosen metrics.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
									If you create a customized <code class="literal">AlertingRule</code> resource based on an existing platform alerting rule, silence the original alert to avoid receiving conflicting alerts.
								</li><li class="listitem">
									To help users understand the impact and cause of the alert, ensure that your alerting rule contains an alert message and severity value.
								</li></ul></div></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user that has the <code class="literal">cluster-admin</code> cluster role.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Create a new YAML configuration file named <code class="literal">example-alerting-rule.yaml</code>.
							</li><li class="listitem"><p class="simpara">
								Add an <code class="literal">AlertingRule</code> resource to the YAML file. The following example creates a new alerting rule named <code class="literal">example</code>, similar to the default <code class="literal">Watchdog</code> alert:
							</p><pre class="programlisting language-yaml">apiVersion: monitoring.openshift.io/v1
kind: AlertingRule
metadata:
  name: example
  namespace: openshift-monitoring <span id="CO80-1"><!--Empty--></span><span class="callout">1</span>
spec:
  groups:
  - name: example-rules
    rules:
    - alert: ExampleAlert <span id="CO80-2"><!--Empty--></span><span class="callout">2</span>
      for: 1m <span id="CO80-3"><!--Empty--></span><span class="callout">3</span>
      expr: vector(1) <span id="CO80-4"><!--Empty--></span><span class="callout">4</span>
      labels:
        severity: warning <span id="CO80-5"><!--Empty--></span><span class="callout">5</span>
      annotations:
        message: This is an example alert. <span id="CO80-6"><!--Empty--></span><span class="callout">6</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO80-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Ensure that the namespace is <code class="literal">openshift-monitoring</code>.
									</div></dd><dt><a href="#CO80-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The name of the alerting rule you want to create.
									</div></dd><dt><a href="#CO80-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The duration for which the condition should be true before an alert is fired.
									</div></dd><dt><a href="#CO80-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										The PromQL query expression that defines the new rule.
									</div></dd><dt><a href="#CO80-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										The severity that alerting rule assigns to the alert.
									</div></dd><dt><a href="#CO80-6"><span class="callout">6</span></a> </dt><dd><div class="para">
										The message associated with the alert.
									</div></dd></dl></div><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
									You must create the <code class="literal">AlertingRule</code> object in the <code class="literal">openshift-monitoring</code> namespace. Otherwise, the alerting rule is not accepted.
								</p></div></rh-alert></li><li class="listitem"><p class="simpara">
								Apply the configuration file to the cluster:
							</p><pre class="programlisting language-terminal">$ oc apply -f example-alerting-rule.yaml</pre></li></ol></div></section><section class="section" id="modifying-core-platform-alerting-rules_managing-alerts-as-an-administrator"><div class="titlepage"><div><div><h5 class="title">6.1.4.2. Modifying core platform alerting rules</h5></div></div></div><p>
						As a cluster administrator, you can modify core platform alerts before Alertmanager routes them to a receiver. For example, you can change the severity label of an alert, add a custom label, or exclude an alert from being sent to Alertmanager.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Create a new YAML configuration file named <code class="literal">example-modified-alerting-rule.yaml</code>.
							</li><li class="listitem"><p class="simpara">
								Add an <code class="literal">AlertRelabelConfig</code> resource to the YAML file. The following example modifies the <code class="literal">severity</code> setting to <code class="literal">critical</code> for the default platform <code class="literal">watchdog</code> alerting rule:
							</p><pre class="programlisting language-yaml">apiVersion: monitoring.openshift.io/v1
kind: AlertRelabelConfig
metadata:
  name: watchdog
  namespace: openshift-monitoring <span id="CO81-1"><!--Empty--></span><span class="callout">1</span>
spec:
  configs:
  - sourceLabels: [alertname,severity] <span id="CO81-2"><!--Empty--></span><span class="callout">2</span>
    regex: "Watchdog;none" <span id="CO81-3"><!--Empty--></span><span class="callout">3</span>
    targetLabel: severity <span id="CO81-4"><!--Empty--></span><span class="callout">4</span>
    replacement: critical <span id="CO81-5"><!--Empty--></span><span class="callout">5</span>
    action: Replace <span id="CO81-6"><!--Empty--></span><span class="callout">6</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO81-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Ensure that the namespace is <code class="literal">openshift-monitoring</code>.
									</div></dd><dt><a href="#CO81-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The source labels for the values you want to modify.
									</div></dd><dt><a href="#CO81-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The regular expression against which the value of <code class="literal">sourceLabels</code> is matched.
									</div></dd><dt><a href="#CO81-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										The target label of the value you want to modify.
									</div></dd><dt><a href="#CO81-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										The new value to replace the target label.
									</div></dd><dt><a href="#CO81-6"><span class="callout">6</span></a> </dt><dd><div class="para">
										The relabel action that replaces the old value based on regex matching. The default action is <code class="literal">Replace</code>. Other possible values are <code class="literal">Keep</code>, <code class="literal">Drop</code>, <code class="literal">HashMod</code>, <code class="literal">LabelMap</code>, <code class="literal">LabelDrop</code>, and <code class="literal">LabelKeep</code>.
									</div></dd></dl></div><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
									You must create the <code class="literal">AlertRelabelConfig</code> object in the <code class="literal">openshift-monitoring</code> namespace. Otherwise, the alert label will not change.
								</p></div></rh-alert></li><li class="listitem"><p class="simpara">
								Apply the configuration file to the cluster:
							</p><pre class="programlisting language-terminal">$ oc apply -f example-modified-alerting-rule.yaml</pre></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#monitoring-stack-architecture" title="1.2. Monitoring stack architecture">Monitoring stack architecture</a>
</li><li class="listitem">
<a class="link" href="https://prometheus.io/docs/alerting/alertmanager/">Alertmanager</a> (Prometheus documentation)
							</li><li class="listitem">
<a class="link" href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config">relabel_config configuration</a> (Prometheus documentation)
							</li><li class="listitem">
<a class="link" href="https://prometheus.io/docs/practices/alerting/">Alerting</a> (Prometheus documentation)
							</li></ul></div></section></section><section class="section" id="managing-alerting-rules-for-user-defined-projects_managing-alerts-as-an-administrator"><div class="titlepage"><div><div><h4 class="title">6.1.5. Managing alerting rules for user-defined projects</h4></div></div></div><p>
					In OpenShift Container Platform, you can create, view, edit, and remove alerting rules for user-defined projects. Those alerting rules will trigger alerts based on the values of the chosen metrics.
				</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#about-creating-alerting-rules-for-user-defined-projects_key-concepts" title="1.3.5.4. Creating alerting rules for user-defined projects">Creating alerting rules for user-defined projects</a>
</li><li class="listitem">
<a class="link" href="#managing-alerting-rules-for-user-defined-projects_key-concepts" title="1.3.5.5. Managing alerting rules for user-defined projects">Managing alerting rules for user-defined projects</a>
</li><li class="listitem">
<a class="link" href="#optimizing-alerting-for-user-defined-projects_key-concepts" title="1.3.5.6. Optimizing alerting for user-defined projects">Optimizing alerting for user-defined projects</a>
</li></ul></div><section class="section" id="creating-alerting-rules-for-user-defined-projects_managing-alerts-as-an-administrator"><div class="titlepage"><div><div><h5 class="title">6.1.5.1. Creating alerting rules for user-defined projects</h5></div></div></div><p>
						You can create alerting rules for user-defined projects. Those alerting rules will trigger alerts based on the values of the chosen metrics.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							To help users understand the impact and cause of the alert, ensure that your alerting rule contains an alert message and severity value.
						</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have enabled monitoring for user-defined projects.
							</li><li class="listitem">
								You are logged in as a cluster administrator or as a user that has the <code class="literal">monitoring-rules-edit</code> cluster role for the project where you want to create an alerting rule.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Create a YAML file for alerting rules. In this example, it is called <code class="literal">example-app-alerting-rule.yaml</code>.
							</li><li class="listitem"><p class="simpara">
								Add an alerting rule configuration to the YAML file. The following example creates a new alerting rule named <code class="literal">example-alert</code>. The alerting rule fires an alert when the <code class="literal">version</code> metric exposed by the sample service becomes <code class="literal">0</code>:
							</p><pre class="programlisting language-yaml">apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: example-alert
  namespace: ns1
spec:
  groups:
  - name: example
    rules:
    - alert: VersionAlert <span id="CO82-1"><!--Empty--></span><span class="callout">1</span>
      for: 1m <span id="CO82-2"><!--Empty--></span><span class="callout">2</span>
      expr: version{job="prometheus-example-app"} == 0 <span id="CO82-3"><!--Empty--></span><span class="callout">3</span>
      labels:
        severity: warning <span id="CO82-4"><!--Empty--></span><span class="callout">4</span>
      annotations:
        message: This is an example alert. <span id="CO82-5"><!--Empty--></span><span class="callout">5</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO82-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The name of the alerting rule you want to create.
									</div></dd><dt><a href="#CO82-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The duration for which the condition should be true before an alert is fired.
									</div></dd><dt><a href="#CO82-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The PromQL query expression that defines the new rule.
									</div></dd><dt><a href="#CO82-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										The severity that alerting rule assigns to the alert.
									</div></dd><dt><a href="#CO82-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										The message associated with the alert.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Apply the configuration file to the cluster:
							</p><pre class="programlisting language-terminal">$ oc apply -f example-app-alerting-rule.yaml</pre></li></ol></div></section><section class="section" id="creating-cross-project-alerting-rules-for-user-defined-projects_managing-alerts-as-an-administrator"><div class="titlepage"><div><div><h5 class="title">6.1.5.2. Creating cross-project alerting rules for user-defined projects</h5></div></div></div><p>
						You can create alerting rules that are not bound to their project of origin by configuring a project in the <code class="literal">user-workload-monitoring-config</code> config map. The <code class="literal">PrometheusRule</code> objects created in these projects are then applicable to all projects.
					</p><p>
						Therefore, you can have generic alerting rules that apply to multiple user-defined projects instead of having individual <code class="literal">PrometheusRule</code> objects in each user project. You can filter which projects are included or excluded from the alerting rule by using PromQL queries in the <code class="literal">PrometheusRule</code> object.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								If you are a cluster administrator, you have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
							</li><li class="listitem"><p class="simpara">
								If you are a non-administrator user, you have access to the cluster as a user with the following user roles:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										The <code class="literal">user-workload-monitoring-config-edit</code> role in the <code class="literal">openshift-user-workload-monitoring</code> project to edit the <code class="literal">user-workload-monitoring-config</code> config map.
									</li><li class="listitem">
										The <code class="literal">monitoring-rules-edit</code> cluster role for the project where you want to create an alerting rule.
									</li></ul></div></li><li class="listitem">
								A cluster administrator has enabled monitoring for user-defined projects.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Edit the <code class="literal">user-workload-monitoring-config</code> config map in the <code class="literal">openshift-user-workload-monitoring</code> project:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</pre></li><li class="listitem"><p class="simpara">
								Configure projects in which you want to create alerting rules that are not bound to a specific project:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    namespacesWithoutLabelEnforcement: [ &lt;namespace1&gt;, &lt;namespace2&gt; ] <span id="CO83-1"><!--Empty--></span><span class="callout">1</span>
    # ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO83-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specify one or more projects in which you want to create cross-project alerting rules. Prometheus and Thanos Ruler for user-defined monitoring do not enforce the <code class="literal">namespace</code> label in <code class="literal">PrometheusRule</code> objects created in these projects, making the <code class="literal">PrometheusRule</code> objects applicable to all projects.
									</div></dd></dl></div></li><li class="listitem">
								Create a YAML file for alerting rules. In this example, it is called <code class="literal">example-cross-project-alerting-rule.yaml</code>.
							</li><li class="listitem"><p class="simpara">
								Add an alerting rule configuration to the YAML file. The following example creates a new cross-project alerting rule called <code class="literal">example-security</code>. The alerting rule fires when a user project does not enforce the restricted pod security policy:
							</p><div class="formalpara"><p class="title"><strong>Example cross-project alerting rule</strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: example-security
  namespace: ns1 <span id="CO84-1"><!--Empty--></span><span class="callout">1</span>
spec:
  groups:
    - name: pod-security-policy
      rules:
        - alert: "ProjectNotEnforcingRestrictedPolicy" <span id="CO84-2"><!--Empty--></span><span class="callout">2</span>
          for: 5m <span id="CO84-3"><!--Empty--></span><span class="callout">3</span>
          expr: kube_namespace_labels{namespace!~"(openshift|kube).*|default",label_pod_security_kubernetes_io_enforce!="restricted"} <span id="CO84-4"><!--Empty--></span><span class="callout">4</span>
          annotations:
            message: "Restricted policy not enforced. Project {{ $labels.namespace }} does not enforce the restricted pod security policy." <span id="CO84-5"><!--Empty--></span><span class="callout">5</span>
          labels:
            severity: warning <span id="CO84-6"><!--Empty--></span><span class="callout">6</span></pre>
<p></p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO84-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Ensure that you specify the project that you defined in the <code class="literal">namespacesWithoutLabelEnforcement</code> field.
									</div></dd><dt><a href="#CO84-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The name of the alerting rule you want to create.
									</div></dd><dt><a href="#CO84-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The duration for which the condition should be true before an alert is fired.
									</div></dd><dt><a href="#CO84-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										The PromQL query expression that defines the new rule. You can use label matchers on the <code class="literal">namespace</code> label to filter which projects are included or excluded from the alerting rule.
									</div></dd><dt><a href="#CO84-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										The message associated with the alert.
									</div></dd><dt><a href="#CO84-6"><span class="callout">6</span></a> </dt><dd><div class="para">
										The severity that alerting rule assigns to the alert.
									</div></dd></dl></div><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
									Ensure that you create a specific cross-project alerting rule in only one of the projects that you specified in the <code class="literal">namespacesWithoutLabelEnforcement</code> field. If you create the same cross-project alerting rule in multiple projects, it results in repeated alerts.
								</p></div></rh-alert></li><li class="listitem"><p class="simpara">
								Apply the configuration file to the cluster:
							</p><pre class="programlisting language-terminal">$ oc apply -f example-cross-project-alerting-rule.yaml</pre></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#monitoring-stack-architecture" title="1.2. Monitoring stack architecture">Monitoring stack architecture</a>
</li><li class="listitem">
<a class="link" href="https://prometheus.io/docs/practices/alerting/">Alerting</a> (Prometheus documentation)
							</li></ul></div></section><section class="section" id="listing-alerting-rules-for-all-projects-in-a-single-view_managing-alerts-as-an-administrator"><div class="titlepage"><div><div><h5 class="title">6.1.5.3. Listing alerting rules for all projects in a single view</h5></div></div></div><p>
						As a cluster administrator, you can list alerting rules for core OpenShift Container Platform and user-defined projects together in a single view.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								From the <span class="strong strong"><strong>Administrator</strong></span> perspective of the OpenShift Container Platform web console, go to <span class="strong strong"><strong>Observe</strong></span> → <span class="strong strong"><strong>Alerting</strong></span> → <span class="strong strong"><strong>Alerting rules</strong></span>.
							</li><li class="listitem"><p class="simpara">
								Select the <span class="strong strong"><strong>Platform</strong></span> and <span class="strong strong"><strong>User</strong></span> sources in the <span class="strong strong"><strong>Filter</strong></span> drop-down menu.
							</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
									The <span class="strong strong"><strong>Platform</strong></span> source is selected by default.
								</p></div></rh-alert></li></ol></div></section><section class="section" id="removing-alerting-rules-for-user-defined-projects_managing-alerts-as-an-administrator"><div class="titlepage"><div><div><h5 class="title">6.1.5.4. Removing alerting rules for user-defined projects</h5></div></div></div><p>
						You can remove alerting rules for user-defined projects.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have enabled monitoring for user-defined projects.
							</li><li class="listitem">
								You are logged in as a cluster administrator or as a user that has the <code class="literal">monitoring-rules-edit</code> cluster role for the project where you want to create an alerting rule.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								To remove rule <code class="literal">&lt;foo&gt;</code> in <code class="literal">&lt;namespace&gt;</code>, run the following:
							</p><pre class="programlisting language-terminal">$ oc -n &lt;namespace&gt; delete prometheusrule &lt;foo&gt;</pre></li></ul></div></section><section class="section" id="disabling-cross-project-alerting-rules-for-user-defined-projects_managing-alerts-as-an-administrator"><div class="titlepage"><div><div><h5 class="title">6.1.5.5. Disabling cross-project alerting rules for user-defined projects</h5></div></div></div><p>
						Creating cross-project alerting rules for user-defined projects is enabled by default. Cluster administrators can disable the capability in the <code class="literal">cluster-monitoring-config</code> config map for the following reasons:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
								To prevent user-defined monitoring from overloading the cluster monitoring stack.
							</li><li class="listitem">
								To prevent buggy alerting rules from being applied to the cluster without having to identify the rule that causes the issue.
							</li></ul></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Edit the <code class="literal">cluster-monitoring-config</code> config map in the <code class="literal">openshift-monitoring</code> project:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</pre></li><li class="listitem"><p class="simpara">
								In the <code class="literal">cluster-monitoring-config</code> config map, disable the option to create cross-project alerting rules by setting the <code class="literal">rulesWithoutLabelEnforcementAllowed</code> value under <code class="literal">data/config.yaml/userWorkload</code> to <code class="literal">false</code>:
							</p><pre class="programlisting language-yaml">kind: ConfigMap
apiVersion: v1
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    userWorkload:
      rulesWithoutLabelEnforcementAllowed: false
    # ...</pre></li><li class="listitem">
								Save the file to apply the changes.
							</li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="https://prometheus.io/docs/alerting/alertmanager/">Alertmanager</a> (Prometheus documentation)
							</li></ul></div></section></section></section><section class="section" id="managing-alerts-as-a-developer"><div class="titlepage"><div><div><h3 class="title">6.2. Managing alerts as a Developer</h3></div></div></div><p>
				In OpenShift Container Platform, the Alerting UI enables you to manage alerts, silences, and alerting rules.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					The alerts, silences, and alerting rules that are available in the Alerting UI relate to the projects that you have access to.
				</p></div></rh-alert><section class="section" id="monitoring-accessing-the-alerting-ui-dev_managing-alerts-as-a-developer"><div class="titlepage"><div><div><h4 class="title">6.2.1. Accessing the Alerting UI from the Developer perspective</h4></div></div></div><p>
					The Alerting UI is accessible through the <span class="strong strong"><strong>Developer</strong></span> perspective of the OpenShift Container Platform web console.
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							From the <span class="strong strong"><strong>Developer</strong></span> perspective, go to <span class="strong strong"><strong>Observe</strong></span> and go to the <span class="strong strong"><strong>Alerts</strong></span> tab.
						</li><li class="listitem">
							Select the project that you want to manage alerts for from the <span class="strong strong"><strong>Project:</strong></span> list.
						</li></ul></div><p>
					In this perspective, alerts, silences, and alerting rules are all managed from the <span class="strong strong"><strong>Alerts</strong></span> tab. The results shown in the <span class="strong strong"><strong>Alerts</strong></span> tab are specific to the selected project.
				</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						In the <span class="strong strong"><strong>Developer</strong></span> perspective, you can select from core OpenShift Container Platform and user-defined projects that you have access to in the <span class="strong strong"><strong>Project: &lt;project_name&gt;</strong></span> list. However, alerts, silences, and alerting rules relating to core OpenShift Container Platform projects are not displayed if you are not logged in as a cluster administrator.
					</p></div></rh-alert><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#searching-alerts-silences-and-alerting-rules_key-concepts" title="1.3.5.7. Searching and filtering alerts, silences, and alerting rules">Searching and filtering alerts, silences, and alerting rules</a>
</li></ul></div></section><section class="section" id="getting-information-about-alerts-silences-and-alerting-rules-dev_managing-alerts-as-a-developer"><div class="titlepage"><div><div><h4 class="title">6.2.2. Getting information about alerts, silences, and alerting rules from the Developer perspective</h4></div></div></div><p>
					The Alerting UI provides detailed information about alerts and their governing alerting rules and silences.
				</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
							You have access to the cluster as a user with view permissions for the project that you are viewing alerts for.
						</li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
						To obtain information about alerts, silences, and alerting rules:
					</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
							From the <span class="strong strong"><strong>Developer</strong></span> perspective of the OpenShift Container Platform web console, go to the <span class="strong strong"><strong>Observe</strong></span> → <span class="strong strong"><strong>&lt;project_name&gt;</strong></span> → <span class="strong strong"><strong>Alerts</strong></span> page.
						</li><li class="listitem"><p class="simpara">
							View details for an alert, silence, or an alerting rule:
						</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<span class="strong strong"><strong>Alert details</strong></span> can be viewed by clicking a greater than symbol (<span class="strong strong"><strong>&gt;</strong></span>) next to an alert name and then selecting the alert from the list.
								</li><li class="listitem"><p class="simpara">
<span class="strong strong"><strong>Silence details</strong></span> can be viewed by clicking a silence in the <span class="strong strong"><strong>Silenced by</strong></span> section of the <span class="strong strong"><strong>Alert details</strong></span> page. The <span class="strong strong"><strong>Silence details</strong></span> page includes the following information:
								</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
											Alert specification
										</li><li class="listitem">
											Start time
										</li><li class="listitem">
											End time
										</li><li class="listitem">
											Silence state
										</li><li class="listitem">
											Number and list of firing alerts
										</li></ul></div></li><li class="listitem">
<span class="strong strong"><strong>Alerting rule details</strong></span> can be viewed by clicking the 
									<span class="inlinemediaobject"><img alt="kebab" src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.18-Monitoring-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png"/></span>
									 menu next to an alert in the <span class="strong strong"><strong>Alerts</strong></span> page and then clicking <span class="strong strong"><strong>View Alerting Rule</strong></span>.
								</li></ul></div></li></ol></div><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						Only alerts, silences, and alerting rules relating to the selected project are displayed in the <span class="strong strong"><strong>Developer</strong></span> perspective.
					</p></div></rh-alert><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="https://github.com/openshift/runbooks/tree/master/alerts/cluster-monitoring-operator">Cluster Monitoring Operator runbooks</a> (Cluster Monitoring Operator GitHub repository)
						</li></ul></div></section><section class="section" id="managing-silences_managing-alerts-as-a-developer"><div class="titlepage"><div><div><h4 class="title">6.2.3. Managing silences</h4></div></div></div><p>
					You can create a silence for an alert in the OpenShift Container Platform web console in the <span class="strong strong"><strong>Developer</strong></span> perspective. After you create silences, you can view, edit, and expire them. You also do not receive notifications about a silenced alert when the alert fires.
				</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
						When you create silences, they are replicated across Alertmanager pods. However, if you do not configure persistent storage for Alertmanager, silences might be lost. This can happen, for example, if all Alertmanager pods restart at the same time.
					</p></div></rh-alert><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#managing-silences_key-concepts" title="1.3.5.1. Managing silences">Managing silences</a>
</li><li class="listitem">
<a class="link" href="#configuring-persistent-storage_storing-and-recording-data" title="3.3.1. Configuring persistent storage">Configuring persistent storage</a>
</li></ul></div><section class="section" id="silencing-alerts-dev_managing-alerts-as-a-developer"><div class="titlepage"><div><div><h5 class="title">6.2.3.1. Silencing alerts from the Developer perspective</h5></div></div></div><p>
						You can silence a specific alert or silence alerts that match a specification that you define.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								If you are a cluster administrator, you have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem"><p class="simpara">
								If you are a non-administrator user, you have access to the cluster as a user with the following user roles:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										The <code class="literal">cluster-monitoring-view</code> cluster role, which allows you to access Alertmanager.
									</li><li class="listitem">
										The <code class="literal">monitoring-alertmanager-edit</code> role, which permits you to create and silence alerts in the <span class="strong strong"><strong>Administrator</strong></span> perspective in the web console.
									</li><li class="listitem">
										The <code class="literal">monitoring-rules-edit</code> cluster role, which permits you to create and silence alerts in the <span class="strong strong"><strong>Developer</strong></span> perspective in the web console.
									</li></ul></div></li></ul></div><div class="formalpara"><p class="title"><strong>Procedure</strong></p><p>
							To silence a specific alert:
						</p></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								From the <span class="strong strong"><strong>Developer</strong></span> perspective of the OpenShift Container Platform web console, go to <span class="strong strong"><strong>Observe</strong></span> and go to the <span class="strong strong"><strong>Alerts</strong></span> tab.
							</li><li class="listitem">
								Select the project that you want to silence an alert for from the <span class="strong strong"><strong>Project:</strong></span> list.
							</li><li class="listitem">
								If necessary, expand the details for the alert by clicking a greater than symbol (<span class="strong strong"><strong>&gt;</strong></span>) next to the alert name.
							</li><li class="listitem">
								Click the alert message in the expanded view to open the <span class="strong strong"><strong>Alert details</strong></span> page for the alert.
							</li><li class="listitem">
								Click <span class="strong strong"><strong>Silence alert</strong></span> to open the <span class="strong strong"><strong>Silence alert</strong></span> page with a default configuration for the alert.
							</li><li class="listitem"><p class="simpara">
								Optional: Change the default configuration details for the silence.
							</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
									You must add a comment before saving a silence.
								</p></div></rh-alert></li><li class="listitem">
								To save the silence, click <span class="strong strong"><strong>Silence</strong></span>.
							</li></ol></div><p>
						To silence a set of alerts:
					</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
								From the <span class="strong strong"><strong>Developer</strong></span> perspective of the OpenShift Container Platform web console, go to <span class="strong strong"><strong>Observe</strong></span> and go to the <span class="strong strong"><strong>Silences</strong></span> tab.
							</li><li class="listitem">
								Select the project that you want to silence alerts for from the <span class="strong strong"><strong>Project:</strong></span> list.
							</li><li class="listitem">
								Click <span class="strong strong"><strong>Create silence</strong></span>.
							</li><li class="listitem"><p class="simpara">
								On the <span class="strong strong"><strong>Create silence</strong></span> page, set the duration and label details for an alert.
							</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
									You must add a comment before saving a silence.
								</p></div></rh-alert></li><li class="listitem">
								To create silences for alerts that match the labels that you entered, click <span class="strong strong"><strong>Silence</strong></span>.
							</li></ol></div></section><section class="section" id="editing-silences-dev_managing-alerts-as-a-developer"><div class="titlepage"><div><div><h5 class="title">6.2.3.2. Editing silences from the Developer perspective</h5></div></div></div><p>
						You can edit a silence, which expires the existing silence and creates a new one with the changed configuration.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								If you are a cluster administrator, you have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem"><p class="simpara">
								If you are a non-administrator user, you have access to the cluster as a user with the following user roles:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										The <code class="literal">cluster-monitoring-view</code> cluster role, which allows you to access Alertmanager.
									</li><li class="listitem">
										The <code class="literal">monitoring-rules-edit</code> cluster role, which permits you to create and silence alerts in the <span class="strong strong"><strong>Developer</strong></span> perspective in the web console.
									</li></ul></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								From the <span class="strong strong"><strong>Developer</strong></span> perspective of the OpenShift Container Platform web console, go to <span class="strong strong"><strong>Observe</strong></span> and go to the <span class="strong strong"><strong>Silences</strong></span> tab.
							</li><li class="listitem">
								Select the project that you want to edit silences for from the <span class="strong strong"><strong>Project:</strong></span> list.
							</li><li class="listitem"><p class="simpara">
								For the silence you want to modify, click 
								<span class="inlinemediaobject"><img alt="kebab" src="https://access.redhat.com/webassets/avalon/d/OpenShift_Container_Platform-4.18-Monitoring-en-US/images/f468284ec3cc9bf27e6bd2c83849ca50/kebab.png"/></span>
								 and select <span class="strong strong"><strong>Edit silence</strong></span>.
							</p><p class="simpara">
								Alternatively, you can click <span class="strong strong"><strong>Actions</strong></span> and select <span class="strong strong"><strong>Edit silence</strong></span> on the <span class="strong strong"><strong>Silence details</strong></span> page for a silence.
							</p></li><li class="listitem">
								On the <span class="strong strong"><strong>Edit silence</strong></span> page, make changes and click <span class="strong strong"><strong>Silence</strong></span>. Doing so expires the existing silence and creates one with the updated configuration.
							</li></ol></div></section><section class="section" id="expiring-silences-dev_managing-alerts-as-a-developer"><div class="titlepage"><div><div><h5 class="title">6.2.3.3. Expiring silences from the Developer perspective</h5></div></div></div><p>
						You can expire a single silence or multiple silences. Expiring a silence deactivates it permanently.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							You cannot delete expired, silenced alerts. Expired silences older than 120 hours are garbage collected.
						</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								If you are a cluster administrator, you have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
							</li><li class="listitem"><p class="simpara">
								If you are a non-administrator user, you have access to the cluster as a user with the following user roles:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										The <code class="literal">cluster-monitoring-view</code> cluster role, which allows you to access Alertmanager.
									</li><li class="listitem">
										The <code class="literal">monitoring-rules-edit</code> cluster role, which permits you to create and silence alerts in the <span class="strong strong"><strong>Developer</strong></span> perspective in the web console.
									</li></ul></div></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								From the <span class="strong strong"><strong>Developer</strong></span> perspective of the OpenShift Container Platform web console, go to <span class="strong strong"><strong>Observe</strong></span> and go to the <span class="strong strong"><strong>Silences</strong></span> tab.
							</li><li class="listitem">
								Select the project that you want to expire a silence for from the <span class="strong strong"><strong>Project:</strong></span> list.
							</li><li class="listitem">
								For the silence or silences you want to expire, select the checkbox in the corresponding row.
							</li><li class="listitem"><p class="simpara">
								Click <span class="strong strong"><strong>Expire 1 silence</strong></span> to expire a single selected silence or <span class="strong strong"><strong>Expire <span class="emphasis"><em>&lt;n&gt;</em></span> silences</strong></span> to expire multiple selected silences, where <span class="emphasis"><em>&lt;n&gt;</em></span> is the number of silences you selected.
							</p><p class="simpara">
								Alternatively, to expire a single silence you can click <span class="strong strong"><strong>Actions</strong></span> and select <span class="strong strong"><strong>Expire silence</strong></span> on the <span class="strong strong"><strong>Silence details</strong></span> page for a silence.
							</p></li></ol></div></section></section><section class="section" id="managing-alerting-rules-for-user-defined-projects-uwm_managing-alerts-as-a-developer"><div class="titlepage"><div><div><h4 class="title">6.2.4. Managing alerting rules for user-defined projects</h4></div></div></div><p>
					In OpenShift Container Platform, you can create, view, edit, and remove alerting rules for user-defined projects. Those alerting rules will trigger alerts based on the values of the chosen metrics.
				</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#about-creating-alerting-rules-for-user-defined-projects_key-concepts" title="1.3.5.4. Creating alerting rules for user-defined projects">Creating alerting rules for user-defined projects</a>
</li><li class="listitem">
<a class="link" href="#managing-alerting-rules-for-user-defined-projects_key-concepts" title="1.3.5.5. Managing alerting rules for user-defined projects">Managing alerting rules for user-defined projects</a>
</li><li class="listitem">
<a class="link" href="#optimizing-alerting-for-user-defined-projects_key-concepts" title="1.3.5.6. Optimizing alerting for user-defined projects">Optimizing alerting for user-defined projects</a>
</li></ul></div><section class="section" id="creating-alerting-rules-for-user-defined-projects_managing-alerts-as-a-developer"><div class="titlepage"><div><div><h5 class="title">6.2.4.1. Creating alerting rules for user-defined projects</h5></div></div></div><p>
						You can create alerting rules for user-defined projects. Those alerting rules will trigger alerts based on the values of the chosen metrics.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							To help users understand the impact and cause of the alert, ensure that your alerting rule contains an alert message and severity value.
						</p></div></rh-alert><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have enabled monitoring for user-defined projects.
							</li><li class="listitem">
								You are logged in as a cluster administrator or as a user that has the <code class="literal">monitoring-rules-edit</code> cluster role for the project where you want to create an alerting rule.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
								Create a YAML file for alerting rules. In this example, it is called <code class="literal">example-app-alerting-rule.yaml</code>.
							</li><li class="listitem"><p class="simpara">
								Add an alerting rule configuration to the YAML file. The following example creates a new alerting rule named <code class="literal">example-alert</code>. The alerting rule fires an alert when the <code class="literal">version</code> metric exposed by the sample service becomes <code class="literal">0</code>:
							</p><pre class="programlisting language-yaml">apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: example-alert
  namespace: ns1
spec:
  groups:
  - name: example
    rules:
    - alert: VersionAlert <span id="CO85-1"><!--Empty--></span><span class="callout">1</span>
      for: 1m <span id="CO85-2"><!--Empty--></span><span class="callout">2</span>
      expr: version{job="prometheus-example-app"} == 0 <span id="CO85-3"><!--Empty--></span><span class="callout">3</span>
      labels:
        severity: warning <span id="CO85-4"><!--Empty--></span><span class="callout">4</span>
      annotations:
        message: This is an example alert. <span id="CO85-5"><!--Empty--></span><span class="callout">5</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO85-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										The name of the alerting rule you want to create.
									</div></dd><dt><a href="#CO85-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The duration for which the condition should be true before an alert is fired.
									</div></dd><dt><a href="#CO85-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The PromQL query expression that defines the new rule.
									</div></dd><dt><a href="#CO85-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										The severity that alerting rule assigns to the alert.
									</div></dd><dt><a href="#CO85-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										The message associated with the alert.
									</div></dd></dl></div></li><li class="listitem"><p class="simpara">
								Apply the configuration file to the cluster:
							</p><pre class="programlisting language-terminal">$ oc apply -f example-app-alerting-rule.yaml</pre></li></ol></div></section><section class="section" id="creating-cross-project-alerting-rules-for-user-defined-projects_managing-alerts-as-a-developer"><div class="titlepage"><div><div><h5 class="title">6.2.4.2. Creating cross-project alerting rules for user-defined projects</h5></div></div></div><p>
						You can create alerting rules that are not bound to their project of origin by configuring a project in the <code class="literal">user-workload-monitoring-config</code> config map. The <code class="literal">PrometheusRule</code> objects created in these projects are then applicable to all projects.
					</p><p>
						Therefore, you can have generic alerting rules that apply to multiple user-defined projects instead of having individual <code class="literal">PrometheusRule</code> objects in each user project. You can filter which projects are included or excluded from the alerting rule by using PromQL queries in the <code class="literal">PrometheusRule</code> object.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								If you are a cluster administrator, you have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
							</li><li class="listitem"><p class="simpara">
								If you are a non-administrator user, you have access to the cluster as a user with the following user roles:
							</p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem">
										The <code class="literal">user-workload-monitoring-config-edit</code> role in the <code class="literal">openshift-user-workload-monitoring</code> project to edit the <code class="literal">user-workload-monitoring-config</code> config map.
									</li><li class="listitem">
										The <code class="literal">monitoring-rules-edit</code> cluster role for the project where you want to create an alerting rule.
									</li></ul></div></li><li class="listitem">
								A cluster administrator has enabled monitoring for user-defined projects.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								Edit the <code class="literal">user-workload-monitoring-config</code> config map in the <code class="literal">openshift-user-workload-monitoring</code> project:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</pre></li><li class="listitem"><p class="simpara">
								Configure projects in which you want to create alerting rules that are not bound to a specific project:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    namespacesWithoutLabelEnforcement: [ &lt;namespace1&gt;, &lt;namespace2&gt; ] <span id="CO86-1"><!--Empty--></span><span class="callout">1</span>
    # ...</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO86-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Specify one or more projects in which you want to create cross-project alerting rules. Prometheus and Thanos Ruler for user-defined monitoring do not enforce the <code class="literal">namespace</code> label in <code class="literal">PrometheusRule</code> objects created in these projects, making the <code class="literal">PrometheusRule</code> objects applicable to all projects.
									</div></dd></dl></div></li><li class="listitem">
								Create a YAML file for alerting rules. In this example, it is called <code class="literal">example-cross-project-alerting-rule.yaml</code>.
							</li><li class="listitem"><p class="simpara">
								Add an alerting rule configuration to the YAML file. The following example creates a new cross-project alerting rule called <code class="literal">example-security</code>. The alerting rule fires when a user project does not enforce the restricted pod security policy:
							</p><div class="formalpara"><p class="title"><strong>Example cross-project alerting rule</strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: example-security
  namespace: ns1 <span id="CO87-1"><!--Empty--></span><span class="callout">1</span>
spec:
  groups:
    - name: pod-security-policy
      rules:
        - alert: "ProjectNotEnforcingRestrictedPolicy" <span id="CO87-2"><!--Empty--></span><span class="callout">2</span>
          for: 5m <span id="CO87-3"><!--Empty--></span><span class="callout">3</span>
          expr: kube_namespace_labels{namespace!~"(openshift|kube).*|default",label_pod_security_kubernetes_io_enforce!="restricted"} <span id="CO87-4"><!--Empty--></span><span class="callout">4</span>
          annotations:
            message: "Restricted policy not enforced. Project {{ $labels.namespace }} does not enforce the restricted pod security policy." <span id="CO87-5"><!--Empty--></span><span class="callout">5</span>
          labels:
            severity: warning <span id="CO87-6"><!--Empty--></span><span class="callout">6</span></pre>
<p></p></div><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO87-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Ensure that you specify the project that you defined in the <code class="literal">namespacesWithoutLabelEnforcement</code> field.
									</div></dd><dt><a href="#CO87-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										The name of the alerting rule you want to create.
									</div></dd><dt><a href="#CO87-3"><span class="callout">3</span></a> </dt><dd><div class="para">
										The duration for which the condition should be true before an alert is fired.
									</div></dd><dt><a href="#CO87-4"><span class="callout">4</span></a> </dt><dd><div class="para">
										The PromQL query expression that defines the new rule. You can use label matchers on the <code class="literal">namespace</code> label to filter which projects are included or excluded from the alerting rule.
									</div></dd><dt><a href="#CO87-5"><span class="callout">5</span></a> </dt><dd><div class="para">
										The message associated with the alert.
									</div></dd><dt><a href="#CO87-6"><span class="callout">6</span></a> </dt><dd><div class="para">
										The severity that alerting rule assigns to the alert.
									</div></dd></dl></div><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><p>
									Ensure that you create a specific cross-project alerting rule in only one of the projects that you specified in the <code class="literal">namespacesWithoutLabelEnforcement</code> field. If you create the same cross-project alerting rule in multiple projects, it results in repeated alerts.
								</p></div></rh-alert></li><li class="listitem"><p class="simpara">
								Apply the configuration file to the cluster:
							</p><pre class="programlisting language-terminal">$ oc apply -f example-cross-project-alerting-rule.yaml</pre></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#monitoring-stack-architecture" title="1.2. Monitoring stack architecture">Monitoring stack architecture</a>
</li><li class="listitem">
<a class="link" href="https://prometheus.io/docs/practices/alerting/">Alerting</a> (Prometheus documentation)
							</li></ul></div></section><section class="section" id="accessing-alerting-rules-for-your-project_managing-alerts-as-a-developer"><div class="titlepage"><div><div><h5 class="title">6.2.4.3. Accessing alerting rules for user-defined projects</h5></div></div></div><p>
						To list alerting rules for a user-defined project, you must have been assigned the <code class="literal">monitoring-rules-view</code> cluster role for the project.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have enabled monitoring for user-defined projects.
							</li><li class="listitem">
								You are logged in as a user that has the <code class="literal">monitoring-rules-view</code> cluster role for your project.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
								To list alerting rules in <code class="literal">&lt;project&gt;</code>:
							</p><pre class="programlisting language-terminal">$ oc -n &lt;project&gt; get prometheusrule</pre></li><li class="listitem"><p class="simpara">
								To list the configuration of an alerting rule, run the following:
							</p><pre class="programlisting language-terminal">$ oc -n &lt;project&gt; get prometheusrule &lt;rule&gt; -o yaml</pre></li></ol></div></section><section class="section" id="removing-alerting-rules-for-user-defined-projects_managing-alerts-as-a-developer"><div class="titlepage"><div><div><h5 class="title">6.2.4.4. Removing alerting rules for user-defined projects</h5></div></div></div><p>
						You can remove alerting rules for user-defined projects.
					</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
								You have enabled monitoring for user-defined projects.
							</li><li class="listitem">
								You are logged in as a cluster administrator or as a user that has the <code class="literal">monitoring-rules-edit</code> cluster role for the project where you want to create an alerting rule.
							</li><li class="listitem">
								You have installed the OpenShift CLI (<code class="literal">oc</code>).
							</li></ul></div><div class="itemizedlist"><p class="title"><strong>Procedure</strong></p><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								To remove rule <code class="literal">&lt;foo&gt;</code> in <code class="literal">&lt;namespace&gt;</code>, run the following:
							</p><pre class="programlisting language-terminal">$ oc -n &lt;namespace&gt; delete prometheusrule &lt;foo&gt;</pre></li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="https://prometheus.io/docs/alerting/alertmanager/">Alertmanager</a> (Prometheus documentation)
							</li></ul></div></section></section></section></section><section class="chapter" id="troubleshooting-monitoring-issues"><div class="titlepage"><div><div><h2 class="title">Chapter 7. Troubleshooting monitoring issues</h2></div></div></div><p>
			Find troubleshooting steps for common issues with core platform and user-defined project monitoring.
		</p><section class="section" id="investigating-why-user-defined-metrics-are-unavailable_troubleshooting-monitoring-issues"><div class="titlepage"><div><div><h3 class="title">7.1. Investigating why user-defined project metrics are unavailable</h3></div></div></div><p>
<code class="literal">ServiceMonitor</code> resources enable you to determine how to use the metrics exposed by a service in user-defined projects. Follow the steps outlined in this procedure if you have created a <code class="literal">ServiceMonitor</code> resource but cannot see any corresponding metrics in the Metrics UI.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have access to the cluster as a user with the <code class="literal">cluster-admin</code> role.
					</li><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li><li class="listitem">
						You have enabled and configured monitoring for user-defined projects.
					</li><li class="listitem">
						You have created a <code class="literal">ServiceMonitor</code> resource.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Ensure that your project and resources are not excluded from user workload monitoring. The following examples use the <code class="literal">ns1</code> project.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Verify that the project <span class="emphasis"><em>does not</em></span> have the <code class="literal">openshift.io/user-monitoring=false</code> label attached:
							</p><pre class="programlisting language-terminal">$ oc get namespace ns1 --show-labels | grep 'openshift.io/user-monitoring=false'</pre><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
									The default label set for user workload projects is <code class="literal">openshift.io/user-monitoring=true</code>. However, the label is not visible unless you manually apply it.
								</p></div></rh-alert></li><li class="listitem"><p class="simpara">
								Verify that the <code class="literal">ServiceMonitor</code> and <code class="literal">PodMonitor</code> resources <span class="emphasis"><em>do not</em></span> have the <code class="literal">openshift.io/user-monitoring=false</code> label attached. The following example checks the <code class="literal">prometheus-example-monitor</code> service monitor.
							</p><pre class="programlisting language-terminal">$ oc -n ns1 get servicemonitor prometheus-example-monitor --show-labels | grep 'openshift.io/user-monitoring=false'</pre></li><li class="listitem"><p class="simpara">
								If the label is attached, remove the label:
							</p><div class="formalpara"><p class="title"><strong>Example of removing the label from the project</strong></p><p>
</p><pre class="programlisting language-terminal">$ oc label namespace ns1 'openshift.io/user-monitoring-'</pre>
<p></p></div><div class="formalpara"><p class="title"><strong>Example of removing the label from the resource</strong></p><p>
</p><pre class="programlisting language-terminal">$ oc -n ns1 label servicemonitor prometheus-example-monitor 'openshift.io/user-monitoring-'</pre>
<p></p></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
</p><pre class="programlisting language-terminal">namespace/ns1 unlabeled</pre>
<p></p></div></li></ol></div></li><li class="listitem"><p class="simpara">
						Check that the corresponding labels match in the service and <code class="literal">ServiceMonitor</code> resource configurations. The following examples use the <code class="literal">prometheus-example-app</code> service, the <code class="literal">prometheus-example-monitor</code> service monitor, and the <code class="literal">ns1</code> project.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Obtain the label defined in the service.
							</p><pre class="programlisting language-terminal">$ oc -n ns1 get service prometheus-example-app -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
</p><pre class="programlisting language-terminal">  labels:
    app: prometheus-example-app</pre>
<p></p></div></li><li class="listitem"><p class="simpara">
								Check that the <code class="literal">matchLabels</code> definition in the <code class="literal">ServiceMonitor</code> resource configuration matches the label output in the previous step.
							</p><pre class="programlisting language-terminal">$ oc -n ns1 get servicemonitor prometheus-example-monitor -o yaml</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ServiceMonitor
metadata:
  name: prometheus-example-monitor
  namespace: ns1
spec:
  endpoints:
  - interval: 30s
    port: web
    scheme: http
  selector:
    matchLabels:
      app: prometheus-example-app</pre>
<p></p></div><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
									You can check service and <code class="literal">ServiceMonitor</code> resource labels as a developer with view permissions for the project.
								</p></div></rh-alert></li></ol></div></li><li class="listitem"><p class="simpara">
						Inspect the logs for the Prometheus Operator in the <code class="literal">openshift-user-workload-monitoring</code> project.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								List the pods in the <code class="literal">openshift-user-workload-monitoring</code> project:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring get pods</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
</p><pre class="programlisting language-terminal">NAME                                   READY   STATUS    RESTARTS   AGE
prometheus-operator-776fcbbd56-2nbfm   2/2     Running   0          132m
prometheus-user-workload-0             5/5     Running   1          132m
prometheus-user-workload-1             5/5     Running   1          132m
thanos-ruler-user-workload-0           3/3     Running   0          132m
thanos-ruler-user-workload-1           3/3     Running   0          132m</pre>
<p></p></div></li><li class="listitem"><p class="simpara">
								Obtain the logs from the <code class="literal">prometheus-operator</code> container in the <code class="literal">prometheus-operator</code> pod. In the following example, the pod is called <code class="literal">prometheus-operator-776fcbbd56-2nbfm</code>:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring logs prometheus-operator-776fcbbd56-2nbfm -c prometheus-operator</pre><p class="simpara">
								If there is a issue with the service monitor, the logs might include an error similar to this example:
							</p><pre class="programlisting language-terminal">level=warn ts=2020-08-10T11:48:20.906739623Z caller=operator.go:1829 component=prometheusoperator msg="skipping servicemonitor" error="it accesses file system via bearer token file which Prometheus specification prohibits" servicemonitor=eagle/eagle namespace=openshift-user-workload-monitoring prometheus=user-workload</pre></li></ol></div></li><li class="listitem"><p class="simpara">
						Review the target status for your endpoint on the <span class="strong strong"><strong>Metrics targets</strong></span> page in the OpenShift Container Platform web console UI.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem">
								Log in to the OpenShift Container Platform web console and navigate to <span class="strong strong"><strong>Observe</strong></span> → <span class="strong strong"><strong>Targets</strong></span> in the <span class="strong strong"><strong>Administrator</strong></span> perspective.
							</li><li class="listitem">
								Locate the metrics endpoint in the list, and review the status of the target in the <span class="strong strong"><strong>Status</strong></span> column.
							</li><li class="listitem">
								If the <span class="strong strong"><strong>Status</strong></span> is <span class="strong strong"><strong>Down</strong></span>, click the URL for the endpoint to view more information on the <span class="strong strong"><strong>Target Details</strong></span> page for that metrics target.
							</li></ol></div></li><li class="listitem"><p class="simpara">
						Configure debug level logging for the Prometheus Operator in the <code class="literal">openshift-user-workload-monitoring</code> project.
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Edit the <code class="literal">user-workload-monitoring-config</code> <code class="literal">ConfigMap</code> object in the <code class="literal">openshift-user-workload-monitoring</code> project:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</pre></li><li class="listitem"><p class="simpara">
								Add <code class="literal">logLevel: debug</code> for <code class="literal">prometheusOperator</code> under <code class="literal">data/config.yaml</code> to set the log level to <code class="literal">debug</code>:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheusOperator:
      logLevel: debug
# ...</pre></li><li class="listitem">
								Save the file to apply the changes. The affected <code class="literal">prometheus-operator</code> pod is automatically redeployed.
							</li><li class="listitem"><p class="simpara">
								Confirm that the <code class="literal">debug</code> log-level has been applied to the <code class="literal">prometheus-operator</code> deployment in the <code class="literal">openshift-user-workload-monitoring</code> project:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring get deploy prometheus-operator -o yaml |  grep "log-level"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
</p><pre class="programlisting language-terminal">        - --log-level=debug</pre>
<p></p></div><p class="simpara">
								Debug level logging will show all calls made by the Prometheus Operator.
							</p></li><li class="listitem"><p class="simpara">
								Check that the <code class="literal">prometheus-operator</code> pod is running:
							</p><pre class="programlisting language-terminal">$ oc -n openshift-user-workload-monitoring get pods</pre><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
									If an unrecognized Prometheus Operator <code class="literal">loglevel</code> value is included in the config map, the <code class="literal">prometheus-operator</code> pod might not restart successfully.
								</p></div></rh-alert></li><li class="listitem">
								Review the debug logs to see if the Prometheus Operator is using the <code class="literal">ServiceMonitor</code> resource. Review the logs for other related errors.
							</li></ol></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#enabling-monitoring-for-user-defined-projects-uwm_preparing-to-configure-the-monitoring-stack-uwm" title="4.1.2. Enabling monitoring for user-defined projects">Enabling monitoring for user-defined projects</a>
</li><li class="listitem">
<a class="link" href="#specifying-how-a-service-is-monitored_configuring-metrics-uwm" title="4.4.3.2. Specifying how a service is monitored">Specifying how a service is monitored</a>
</li><li class="listitem">
<a class="link" href="#getting-detailed-information-about-a-target_accessing-metrics-as-an-administrator" title="5.1.3. Getting detailed information about a metrics target">Getting detailed information about a metrics target</a>
</li></ul></div></section><section class="section" id="determining-why-prometheus-is-consuming-disk-space_troubleshooting-monitoring-issues"><div class="titlepage"><div><div><h3 class="title">7.2. Determining why Prometheus is consuming a lot of disk space</h3></div></div></div><p>
				Developers can create labels to define attributes for metrics in the form of key-value pairs. The number of potential key-value pairs corresponds to the number of possible values for an attribute. An attribute that has an unlimited number of potential values is called an unbound attribute. For example, a <code class="literal">customer_id</code> attribute is unbound because it has an infinite number of possible values.
			</p><p>
				Every assigned key-value pair has a unique time series. The use of many unbound attributes in labels can result in an exponential increase in the number of time series created. This can impact Prometheus performance and can consume a lot of disk space.
			</p><p>
				You can use the following measures when Prometheus consumes a lot of disk:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<span class="strong strong"><strong>Check the time series database (TSDB) status using the Prometheus HTTP API</strong></span> for more information about which labels are creating the most time series data. Doing so requires cluster administrator privileges.
					</li><li class="listitem">
<span class="strong strong"><strong>Check the number of scrape samples</strong></span> that are being collected.
					</li><li class="listitem"><p class="simpara">
<span class="strong strong"><strong>Reduce the number of unique time series that are created</strong></span> by reducing the number of unbound attributes that are assigned to user-defined metrics.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							Using attributes that are bound to a limited set of possible values reduces the number of potential key-value pair combinations.
						</p></div></rh-alert></li><li class="listitem">
<span class="strong strong"><strong>Enforce limits on the number of samples that can be scraped</strong></span> across user-defined projects. This requires cluster administrator privileges.
					</li></ul></div><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
					</li><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						In the <span class="strong strong"><strong>Administrator</strong></span> perspective, navigate to <span class="strong strong"><strong>Observe</strong></span> → <span class="strong strong"><strong>Metrics</strong></span>.
					</li><li class="listitem"><p class="simpara">
						Enter a Prometheus Query Language (PromQL) query in the <span class="strong strong"><strong>Expression</strong></span> field. The following example queries help to identify high cardinality metrics that might result in high disk space consumption:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p class="simpara">
								By running the following query, you can identify the ten jobs that have the highest number of scrape samples:
							</p><pre class="programlisting language-text">topk(10, max by(namespace, job) (topk by(namespace, job) (1, scrape_samples_post_metric_relabeling)))</pre></li><li class="listitem"><p class="simpara">
								By running the following query, you can pinpoint time series churn by identifying the ten jobs that have created the most time series data in the last hour:
							</p><pre class="programlisting language-text">topk(10, sum by(namespace, job) (sum_over_time(scrape_series_added[1h])))</pre></li></ul></div></li><li class="listitem"><p class="simpara">
						Investigate the number of unbound label values assigned to metrics with higher than expected scrape sample counts:
					</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<span class="strong strong"><strong>If the metrics relate to a user-defined project</strong></span>, review the metrics key-value pairs assigned to your workload. These are implemented through Prometheus client libraries at the application level. Try to limit the number of unbound attributes referenced in your labels.
							</li><li class="listitem">
<span class="strong strong"><strong>If the metrics relate to a core OpenShift Container Platform project</strong></span>, create a Red Hat support case on the <a class="link" href="https://access.redhat.com/">Red Hat Customer Portal</a>.
							</li></ul></div></li><li class="listitem"><p class="simpara">
						Review the TSDB status using the Prometheus HTTP API by following these steps when logged in as a cluster administrator:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Get the Prometheus API route URL by running the following command:
							</p><pre class="programlisting language-terminal">$ HOST=$(oc -n openshift-monitoring get route prometheus-k8s -ojsonpath='{.status.ingress[].host}')</pre></li><li class="listitem"><p class="simpara">
								Extract an authentication token by running the following command:
							</p><pre class="programlisting language-terminal">$ TOKEN=$(oc whoami -t)</pre></li><li class="listitem"><p class="simpara">
								Query the TSDB status for Prometheus by running the following command:
							</p><pre class="programlisting language-terminal">$ curl -H "Authorization: Bearer $TOKEN" -k "https://$HOST/api/v1/status/tsdb"</pre><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
</p><pre class="programlisting language-terminal">"status": "success","data":{"headStats":{"numSeries":507473,
"numLabelPairs":19832,"chunkCount":946298,"minTime":1712253600010,
"maxTime":1712257935346},"seriesCountByMetricName":
[{"name":"etcd_request_duration_seconds_bucket","value":51840},
{"name":"apiserver_request_sli_duration_seconds_bucket","value":47718},
...</pre>
<p></p></div></li></ol></div></li></ol></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#accessing-monitoring-apis-by-using-the-cli" title="5.3. Accessing monitoring APIs by using the CLI">Accessing monitoring APIs by using the CLI</a>
</li><li class="listitem">
<a class="link" href="#setting-scrape-and-evaluation-intervals-limits-for-user-defined-projects_configuring-performance-and-scalability-uwm" title="4.2.3.1. Setting scrape intervals, evaluation intervals, and enforced limits for user-defined projects">Setting scrape intervals, evaluation intervals, and enforced limits for user-defined projects</a>
</li><li class="listitem">
<a class="link" href="https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html-single/support/#support-submitting-a-case_getting-support">Submitting a support case</a>
</li></ul></div></section><section class="section" id="resolving-the-kubepersistentvolumefillingup-alert-firing-for-prometheus_troubleshooting-monitoring-issues"><div class="titlepage"><div><div><h3 class="title">7.3. Resolving the KubePersistentVolumeFillingUp alert firing for Prometheus</h3></div></div></div><p>
				As a cluster administrator, you can resolve the <code class="literal">KubePersistentVolumeFillingUp</code> alert being triggered for Prometheus.
			</p><p>
				The critical alert fires when a persistent volume (PV) claimed by a <code class="literal">prometheus-k8s-*</code> pod in the <code class="literal">openshift-monitoring</code> project has less than 3% total space remaining. This can cause Prometheus to function abnormally.
			</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
					There are two <code class="literal">KubePersistentVolumeFillingUp</code> alerts:
				</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<span class="strong strong"><strong>Critical alert</strong></span>: The alert with the <code class="literal">severity="critical"</code> label is triggered when the mounted PV has less than 3% total space remaining.
						</li><li class="listitem">
<span class="strong strong"><strong>Warning alert</strong></span>: The alert with the <code class="literal">severity="warning"</code> label is triggered when the mounted PV has less than 15% total space remaining and is expected to fill up within four days.
						</li></ul></div></div></rh-alert><p>
				To address this issue, you can remove Prometheus time-series database (TSDB) blocks to create more space for the PV.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have access to the cluster as a user with the <code class="literal">cluster-admin</code> cluster role.
					</li><li class="listitem">
						You have installed the OpenShift CLI (<code class="literal">oc</code>).
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						List the size of all TSDB blocks, sorted from oldest to newest, by running the following command:
					</p><pre class="programlisting language-terminal">$ oc debug &lt;prometheus_k8s_pod_name&gt; -n openshift-monitoring \<span id="CO88-1"><!--Empty--></span><span class="callout">1</span>
-c prometheus --image=$(oc get po -n openshift-monitoring &lt;prometheus_k8s_pod_name&gt; \<span id="CO88-2"><!--Empty--></span><span class="callout">2</span>
-o jsonpath='{.spec.containers[?(@.name=="prometheus")].image}') \
-- sh -c 'cd /prometheus/;du -hs $(ls -dtr */ | grep -Eo "[0-9|A-Z]{26}")'</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO88-1"><span class="callout">1</span></a> <a href="#CO88-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Replace <code class="literal">&lt;prometheus_k8s_pod_name&gt;</code> with the pod mentioned in the <code class="literal">KubePersistentVolumeFillingUp</code> alert description.
							</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
</p><pre class="programlisting language-terminal">308M    01HVKMPKQWZYWS8WVDAYQHNMW6
52M     01HVK64DTDA81799TBR9QDECEZ
102M    01HVK64DS7TRZRWF2756KHST5X
140M    01HVJS59K11FBVAPVY57K88Z11
90M     01HVH2A5Z58SKT810EM6B9AT50
152M    01HV8ZDVQMX41MKCN84S32RRZ1
354M    01HV6Q2N26BK63G4RYTST71FBF
156M    01HV664H9J9Z1FTZD73RD1563E
216M    01HTHXB60A7F239HN7S2TENPNS
104M    01HTHMGRXGS0WXA3WATRXHR36B</pre>
<p></p></div></li><li class="listitem"><p class="simpara">
						Identify which and how many blocks could be removed, then remove the blocks. The following example command removes the three oldest Prometheus TSDB blocks from the <code class="literal">prometheus-k8s-0</code> pod:
					</p><pre class="programlisting language-terminal">$ oc debug prometheus-k8s-0 -n openshift-monitoring \
-c prometheus --image=$(oc get po -n openshift-monitoring prometheus-k8s-0 \
-o jsonpath='{.spec.containers[?(@.name=="prometheus")].image}') \
-- sh -c 'ls -latr /prometheus/ | egrep -o "[0-9|A-Z]{26}" | head -3 | \
while read BLOCK; do rm -r /prometheus/$BLOCK; done'</pre></li><li class="listitem"><p class="simpara">
						Verify the usage of the mounted PV and ensure there is enough space available by running the following command:
					</p><pre class="programlisting language-terminal">$ oc debug &lt;prometheus_k8s_pod_name&gt; -n openshift-monitoring \<span id="CO89-1"><!--Empty--></span><span class="callout">1</span>
--image=$(oc get po -n openshift-monitoring &lt;prometheus_k8s_pod_name&gt; \<span id="CO89-2"><!--Empty--></span><span class="callout">2</span>
-o jsonpath='{.spec.containers[?(@.name=="prometheus")].image}') -- df -h /prometheus/</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO89-1"><span class="callout">1</span></a> <a href="#CO89-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Replace <code class="literal">&lt;prometheus_k8s_pod_name&gt;</code> with the pod mentioned in the <code class="literal">KubePersistentVolumeFillingUp</code> alert description.
							</div></dd></dl></div><p class="simpara">
						The following example output shows the mounted PV claimed by the <code class="literal">prometheus-k8s-0</code> pod that has 63% of space remaining:
					</p><div class="formalpara"><p class="title"><strong>Example output</strong></p><p>
</p><pre class="programlisting language-terminal">Starting pod/prometheus-k8s-0-debug-j82w4 ...
Filesystem      Size  Used Avail Use% Mounted on
/dev/nvme0n1p4  40G   15G  40G  37% /prometheus

Removing debug pod ...</pre>
<p></p></div></li></ol></div></section><section class="section" id="resolving-the-alertmanagerreceiversnotconfigured-alert_troubleshooting-monitoring-issues"><div class="titlepage"><div><div><h3 class="title">7.4. Resolving the AlertmanagerReceiversNotConfigured alert</h3></div></div></div><p>
				Every cluster that is deployed has the <code class="literal">AlertmanagerReceiversNotConfigured</code> alert firing by default. To resolve the issue, you must configure alert receivers.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						For default platform monitoring, follow the steps in "Configuring alert notifications" in <span class="emphasis"><em>Configuring core platform monitoring</em></span>.
					</li><li class="listitem">
						For user workload monitoring, follow the steps in "Configuring alert notifications" in <span class="emphasis"><em>Configuring user workload monitoring</em></span>.
					</li></ul></div><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#configuring-alert-notifications_configuring-alerts-and-notifications" title="3.5.4. Configuring alert notifications">Configuring alert notifications for default platform monitoring</a>
</li><li class="listitem">
<a class="link" href="#configuring-alert-notifications_configuring-alerts-and-notifications-uwm" title="4.5.4. Configuring alert notifications">Configuring alert notifications for user workload monitoring</a>
</li></ul></div></section></section><section class="chapter" id="config-map-reference-for-the-cluster-monitoring-operator"><div class="titlepage"><div><div><h2 class="title">Chapter 8. Config map reference for the Cluster Monitoring Operator</h2></div></div></div><section class="section" id="cluster-monitoring-operator-configuration-reference"><div class="titlepage"><div><div><h3 class="title">8.1. Cluster Monitoring Operator configuration reference</h3></div></div></div><p class="_abstract _abstract">
				Parts of OpenShift Container Platform cluster monitoring are configurable. The API is accessible by setting parameters defined in various config maps.
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						To configure monitoring components, edit the <code class="literal">ConfigMap</code> object named <code class="literal">cluster-monitoring-config</code> in the <code class="literal">openshift-monitoring</code> namespace. These configurations are defined by <a class="link" href="#clustermonitoringconfiguration" title="8.5. ClusterMonitoringConfiguration">ClusterMonitoringConfiguration</a>.
					</li><li class="listitem">
						To configure monitoring components that monitor user-defined projects, edit the <code class="literal">ConfigMap</code> object named <code class="literal">user-workload-monitoring-config</code> in the <code class="literal">openshift-user-workload-monitoring</code> namespace. These configurations are defined by <a class="link" href="#userworkloadconfiguration" title="8.31. UserWorkloadConfiguration">UserWorkloadConfiguration</a>.
					</li></ul></div><p>
				The configuration file is always defined under the <code class="literal">config.yaml</code> key in the config map data.
			</p><rh-alert class="admonition important" state="warning"><div class="admonition_header" slot="header">Important</div><div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
							Not all configuration parameters for the monitoring stack are exposed. Only the parameters and fields listed in this reference are supported for configuration. For more information about supported configurations, see
						</li><li class="listitem">
<a class="link" href="#maintenance-and-support-for-monitoring" title="2.1. Maintenance and support for monitoring">Maintenance and support for monitoring</a>
</li><li class="listitem">
							Configuring cluster monitoring is optional.
						</li><li class="listitem">
							If a configuration does not exist or is empty, default values are used.
						</li><li class="listitem">
							If the configuration has invalid YAML data, or if it contains unsupported or duplicated fields that bypassed early validation, the Cluster Monitoring Operator stops reconciling the resources and reports the <code class="literal">Degraded=True</code> status in the status conditions of the Operator.
						</li></ul></div></div></rh-alert></section><section class="section" id="additionalalertmanagerconfig"><div class="titlepage"><div><div><h3 class="title">8.2. AdditionalAlertmanagerConfig</h3></div></div></div><section class="section" id="description"><div class="titlepage"><div><div><h4 class="title">8.2.1. Description</h4></div></div></div><p>
					The <code class="literal">AdditionalAlertmanagerConfig</code> resource defines settings for how a component communicates with additional Alertmanager instances.
				</p></section><section class="section" id="required"><div class="titlepage"><div><div><h4 class="title">8.2.2. Required</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<code class="literal">apiVersion</code>
</li></ul></div><p>
					Appears in: <a class="link" href="#prometheusk8sconfig" title="8.21. PrometheusK8sConfig">PrometheusK8sConfig</a>, <a class="link" href="#prometheusrestrictedconfig" title="8.24. PrometheusRestrictedConfig">PrometheusRestrictedConfig</a>, <a class="link" href="#thanosrulerconfig" title="8.29. ThanosRulerConfig">ThanosRulerConfig</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059289617360" scope="col" valign="top">Property</th><th align="left" id="idm140059289616272" scope="col" valign="top">Type</th><th align="left" id="idm140059289615184" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059289617360" valign="top"> <p>
									apiVersion
								</p>
</td><td align="left" headers="idm140059289616272" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059289615184" valign="top"> <p>
									Defines the API version of Alertmanager. Possible values are <code class="literal">v1</code> or <code class="literal">v2</code>. The default is <code class="literal">v2</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059289617360" valign="top"> <p>
									bearerToken
								</p>
</td><td align="left" headers="idm140059289616272" valign="top"> <p>
									*v1.SecretKeySelector
								</p>
</td><td align="left" headers="idm140059289615184" valign="top"> <p>
									Defines the secret key reference containing the bearer token to use when authenticating to Alertmanager.
								</p>
</td></tr><tr><td align="left" headers="idm140059289617360" valign="top"> <p>
									pathPrefix
								</p>
</td><td align="left" headers="idm140059289616272" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059289615184" valign="top"> <p>
									Defines the path prefix to add in front of the push endpoint path.
								</p>
</td></tr><tr><td align="left" headers="idm140059289617360" valign="top"> <p>
									scheme
								</p>
</td><td align="left" headers="idm140059289616272" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059289615184" valign="top"> <p>
									Defines the URL scheme to use when communicating with Alertmanager instances. Possible values are <code class="literal">http</code> or <code class="literal">https</code>. The default value is <code class="literal">http</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059289617360" valign="top"> <p>
									staticConfigs
								</p>
</td><td align="left" headers="idm140059289616272" valign="top"> <p>
									[]string
								</p>
</td><td align="left" headers="idm140059289615184" valign="top"> <p>
									A list of statically configured Alertmanager endpoints in the form of <code class="literal">&lt;hosts&gt;:&lt;port&gt;</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059289617360" valign="top"> <p>
									timeout
								</p>
</td><td align="left" headers="idm140059289616272" valign="top"> <p>
									*string
								</p>
</td><td align="left" headers="idm140059289615184" valign="top"> <p>
									Defines the timeout value used when sending alerts.
								</p>
</td></tr><tr><td align="left" headers="idm140059289617360" valign="top"> <p>
									tlsConfig
								</p>
</td><td align="left" headers="idm140059289616272" valign="top"> <p>
<a class="link" href="#tlsconfig" title="8.26. TLSConfig">TLSConfig</a>
</p>
</td><td align="left" headers="idm140059289615184" valign="top"> <p>
									Defines the TLS settings to use for Alertmanager connections.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="alertmanagermainconfig"><div class="titlepage"><div><div><h3 class="title">8.3. AlertmanagerMainConfig</h3></div></div></div><section class="section" id="description-2"><div class="titlepage"><div><div><h4 class="title">8.3.1. Description</h4></div></div></div><p>
					The <code class="literal">AlertmanagerMainConfig</code> resource defines settings for the Alertmanager component in the <code class="literal">openshift-monitoring</code> namespace.
				</p><p>
					Appears in: <a class="link" href="#clustermonitoringconfiguration" title="8.5. ClusterMonitoringConfiguration">ClusterMonitoringConfiguration</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059287003232" scope="col" valign="top">Property</th><th align="left" id="idm140059287002144" scope="col" valign="top">Type</th><th align="left" id="idm140059287001056" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059287003232" valign="top"> <p>
									enabled
								</p>
</td><td align="left" headers="idm140059287002144" valign="top"> <p>
									*bool
								</p>
</td><td align="left" headers="idm140059287001056" valign="top"> <p>
									A Boolean flag that enables or disables the main Alertmanager instance in the <code class="literal">openshift-monitoring</code> namespace. The default value is <code class="literal">true</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059287003232" valign="top"> <p>
									enableUserAlertmanagerConfig
								</p>
</td><td align="left" headers="idm140059287002144" valign="top"> <p>
									bool
								</p>
</td><td align="left" headers="idm140059287001056" valign="top"> <p>
									A Boolean flag that enables or disables user-defined namespaces to be selected for <code class="literal">AlertmanagerConfig</code> lookups. This setting only applies if the user workload monitoring instance of Alertmanager is not enabled. The default value is <code class="literal">false</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059287003232" valign="top"> <p>
									logLevel
								</p>
</td><td align="left" headers="idm140059287002144" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059287001056" valign="top"> <p>
									Defines the log level setting for Alertmanager. The possible values are: <code class="literal">error</code>, <code class="literal">warn</code>, <code class="literal">info</code>, <code class="literal">debug</code>. The default value is <code class="literal">info</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059287003232" valign="top"> <p>
									nodeSelector
								</p>
</td><td align="left" headers="idm140059287002144" valign="top"> <p>
									map[string]string
								</p>
</td><td align="left" headers="idm140059287001056" valign="top"> <p>
									Defines the nodes on which the Pods are scheduled.
								</p>
</td></tr><tr><td align="left" headers="idm140059287003232" valign="top"> <p>
									resources
								</p>
</td><td align="left" headers="idm140059287002144" valign="top"> <p>
									*v1.ResourceRequirements
								</p>
</td><td align="left" headers="idm140059287001056" valign="top"> <p>
									Defines resource requests and limits for the Alertmanager container.
								</p>
</td></tr><tr><td align="left" headers="idm140059287003232" valign="top"> <p>
									secrets
								</p>
</td><td align="left" headers="idm140059287002144" valign="top"> <p>
									[]string
								</p>
</td><td align="left" headers="idm140059287001056" valign="top"> <p>
									Defines a list of secrets to be mounted into Alertmanager. The secrets must reside within the same namespace as the Alertmanager object. They are added as volumes named <code class="literal">secret-&lt;secret-name&gt;</code> and mounted at <code class="literal">/etc/alertmanager/secrets/&lt;secret-name&gt;</code> in the <code class="literal">alertmanager</code> container of the Alertmanager pods.
								</p>
</td></tr><tr><td align="left" headers="idm140059287003232" valign="top"> <p>
									tolerations
								</p>
</td><td align="left" headers="idm140059287002144" valign="top"> <p>
									[]v1.Toleration
								</p>
</td><td align="left" headers="idm140059287001056" valign="top"> <p>
									Defines tolerations for the pods.
								</p>
</td></tr><tr><td align="left" headers="idm140059287003232" valign="top"> <p>
									topologySpreadConstraints
								</p>
</td><td align="left" headers="idm140059287002144" valign="top"> <p>
									[]v1.TopologySpreadConstraint
								</p>
</td><td align="left" headers="idm140059287001056" valign="top"> <p>
									Defines a pod’s topology spread constraints.
								</p>
</td></tr><tr><td align="left" headers="idm140059287003232" valign="top"> <p>
									volumeClaimTemplate
								</p>
</td><td align="left" headers="idm140059287002144" valign="top"> <p>
									*monv1.EmbeddedPersistentVolumeClaim
								</p>
</td><td align="left" headers="idm140059287001056" valign="top"> <p>
									Defines persistent storage for Alertmanager. Use this setting to configure the persistent volume claim, including storage class, volume size, and name.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="alertmanageruserworkloadconfig"><div class="titlepage"><div><div><h3 class="title">8.4. AlertmanagerUserWorkloadConfig</h3></div></div></div><section class="section" id="description-3"><div class="titlepage"><div><div><h4 class="title">8.4.1. Description</h4></div></div></div><p>
					The <code class="literal">AlertmanagerUserWorkloadConfig</code> resource defines the settings for the Alertmanager instance used for user-defined projects.
				</p><p>
					Appears in: <a class="link" href="#userworkloadconfiguration" title="8.31. UserWorkloadConfiguration">UserWorkloadConfiguration</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059285611072" scope="col" valign="top">Property</th><th align="left" id="idm140059285609984" scope="col" valign="top">Type</th><th align="left" id="idm140059285608896" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059285611072" valign="top"> <p>
									enabled
								</p>
</td><td align="left" headers="idm140059285609984" valign="top"> <p>
									bool
								</p>
</td><td align="left" headers="idm140059285608896" valign="top"> <p>
									A Boolean flag that enables or disables a dedicated instance of Alertmanager for user-defined alerts in the <code class="literal">openshift-user-workload-monitoring</code> namespace. The default value is <code class="literal">false</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059285611072" valign="top"> <p>
									enableAlertmanagerConfig
								</p>
</td><td align="left" headers="idm140059285609984" valign="top"> <p>
									bool
								</p>
</td><td align="left" headers="idm140059285608896" valign="top"> <p>
									A Boolean flag to enable or disable user-defined namespaces to be selected for <code class="literal">AlertmanagerConfig</code> lookup. The default value is <code class="literal">false</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059285611072" valign="top"> <p>
									logLevel
								</p>
</td><td align="left" headers="idm140059285609984" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059285608896" valign="top"> <p>
									Defines the log level setting for Alertmanager for user workload monitoring. The possible values are <code class="literal">error</code>, <code class="literal">warn</code>, <code class="literal">info</code>, and <code class="literal">debug</code>. The default value is <code class="literal">info</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059285611072" valign="top"> <p>
									resources
								</p>
</td><td align="left" headers="idm140059285609984" valign="top"> <p>
									*v1.ResourceRequirements
								</p>
</td><td align="left" headers="idm140059285608896" valign="top"> <p>
									Defines resource requests and limits for the Alertmanager container.
								</p>
</td></tr><tr><td align="left" headers="idm140059285611072" valign="top"> <p>
									secrets
								</p>
</td><td align="left" headers="idm140059285609984" valign="top"> <p>
									[]string
								</p>
</td><td align="left" headers="idm140059285608896" valign="top"> <p>
									Defines a list of secrets to be mounted into Alertmanager. The secrets must be located within the same namespace as the Alertmanager object. They are added as volumes named <code class="literal">secret-&lt;secret-name&gt;</code> and mounted at <code class="literal">/etc/alertmanager/secrets/&lt;secret-name&gt;</code> in the <code class="literal">alertmanager</code> container of the Alertmanager pods.
								</p>
</td></tr><tr><td align="left" headers="idm140059285611072" valign="top"> <p>
									nodeSelector
								</p>
</td><td align="left" headers="idm140059285609984" valign="top"> <p>
									map[string]string
								</p>
</td><td align="left" headers="idm140059285608896" valign="top"> <p>
									Defines the nodes on which the pods are scheduled.
								</p>
</td></tr><tr><td align="left" headers="idm140059285611072" valign="top"> <p>
									tolerations
								</p>
</td><td align="left" headers="idm140059285609984" valign="top"> <p>
									[]v1.Toleration
								</p>
</td><td align="left" headers="idm140059285608896" valign="top"> <p>
									Defines tolerations for the pods.
								</p>
</td></tr><tr><td align="left" headers="idm140059285611072" valign="top"> <p>
									topologySpreadConstraints
								</p>
</td><td align="left" headers="idm140059285609984" valign="top"> <p>
									[]v1.TopologySpreadConstraint
								</p>
</td><td align="left" headers="idm140059285608896" valign="top"> <p>
									Defines a pod’s topology spread constraints.
								</p>
</td></tr><tr><td align="left" headers="idm140059285611072" valign="top"> <p>
									volumeClaimTemplate
								</p>
</td><td align="left" headers="idm140059285609984" valign="top"> <p>
									*monv1.EmbeddedPersistentVolumeClaim
								</p>
</td><td align="left" headers="idm140059285608896" valign="top"> <p>
									Defines persistent storage for Alertmanager. Use this setting to configure the persistent volume claim, including storage class, volume size and name.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="clustermonitoringconfiguration"><div class="titlepage"><div><div><h3 class="title">8.5. ClusterMonitoringConfiguration</h3></div></div></div><section class="section" id="description-4"><div class="titlepage"><div><div><h4 class="title">8.5.1. Description</h4></div></div></div><p>
					The <code class="literal">ClusterMonitoringConfiguration</code> resource defines settings that customize the default platform monitoring stack through the <code class="literal">cluster-monitoring-config</code> config map in the <code class="literal">openshift-monitoring</code> namespace.
				</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059288308688" scope="col" valign="top">Property</th><th align="left" id="idm140059288307600" scope="col" valign="top">Type</th><th align="left" id="idm140059288306512" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059288308688" valign="top"> <p>
									alertmanagerMain
								</p>
</td><td align="left" headers="idm140059288307600" valign="top"> <p>
									*<a class="link" href="#alertmanagermainconfig" title="8.3. AlertmanagerMainConfig">AlertmanagerMainConfig</a>
</p>
</td><td align="left" headers="idm140059288306512" valign="top"> <p>
<code class="literal">AlertmanagerMainConfig</code> defines settings for the Alertmanager component in the <code class="literal">openshift-monitoring</code> namespace.
								</p>
</td></tr><tr><td align="left" headers="idm140059288308688" valign="top"> <p>
									enableUserWorkload
								</p>
</td><td align="left" headers="idm140059288307600" valign="top"> <p>
									*bool
								</p>
</td><td align="left" headers="idm140059288306512" valign="top"> <p>
<code class="literal">UserWorkloadEnabled</code> is a Boolean flag that enables monitoring for user-defined projects.
								</p>
</td></tr><tr><td align="left" headers="idm140059288308688" valign="top"> <p>
									userWorkload
								</p>
</td><td align="left" headers="idm140059288307600" valign="top"> <p>
									*<a class="link" href="#userworkloadconfig" title="8.30. UserWorkloadConfig">UserWorkloadConfig</a>
</p>
</td><td align="left" headers="idm140059288306512" valign="top"> <p>
<code class="literal">UserWorkload</code> defines settings for the monitoring of user-defined projects.
								</p>
</td></tr><tr><td align="left" headers="idm140059288308688" valign="top"> <p>
									kubeStateMetrics
								</p>
</td><td align="left" headers="idm140059288307600" valign="top"> <p>
									*<a class="link" href="#kubestatemetricsconfig" title="8.6. KubeStateMetricsConfig">KubeStateMetricsConfig</a>
</p>
</td><td align="left" headers="idm140059288306512" valign="top"> <p>
<code class="literal">KubeStateMetricsConfig</code> defines settings for the <code class="literal">kube-state-metrics</code> agent.
								</p>
</td></tr><tr><td align="left" headers="idm140059288308688" valign="top"> <p>
									metricsServer
								</p>
</td><td align="left" headers="idm140059288307600" valign="top"> <p>
									*<a class="link" href="#metricsserverconfig" title="8.7. MetricsServerConfig">MetricsServerConfig</a>
</p>
</td><td align="left" headers="idm140059288306512" valign="top"> <p>
<code class="literal">MetricsServer</code> defines settings for the Metrics Server component.
								</p>
</td></tr><tr><td align="left" headers="idm140059288308688" valign="top"> <p>
									prometheusK8s
								</p>
</td><td align="left" headers="idm140059288307600" valign="top"> <p>
									*<a class="link" href="#prometheusk8sconfig" title="8.21. PrometheusK8sConfig">PrometheusK8sConfig</a>
</p>
</td><td align="left" headers="idm140059288306512" valign="top"> <p>
<code class="literal">PrometheusK8sConfig</code> defines settings for the Prometheus component.
								</p>
</td></tr><tr><td align="left" headers="idm140059288308688" valign="top"> <p>
									prometheusOperator
								</p>
</td><td align="left" headers="idm140059288307600" valign="top"> <p>
									*<a class="link" href="#prometheusoperatorconfig" title="8.22. PrometheusOperatorConfig">PrometheusOperatorConfig</a>
</p>
</td><td align="left" headers="idm140059288306512" valign="top"> <p>
<code class="literal">PrometheusOperatorConfig</code> defines settings for the Prometheus Operator component.
								</p>
</td></tr><tr><td align="left" headers="idm140059288308688" valign="top"> <p>
									prometheusOperatorAdmissionWebhook
								</p>
</td><td align="left" headers="idm140059288307600" valign="top"> <p>
									*<a class="link" href="#prometheusoperatoradmissionwebhookconfig" title="8.23. PrometheusOperatorAdmissionWebhookConfig">PrometheusOperatorAdmissionWebhookConfig</a>
</p>
</td><td align="left" headers="idm140059288306512" valign="top"> <p>
<code class="literal">PrometheusOperatorAdmissionWebhookConfig</code> defines settings for the admission webhook component of Prometheus Operator.
								</p>
</td></tr><tr><td align="left" headers="idm140059288308688" valign="top"> <p>
									openshiftStateMetrics
								</p>
</td><td align="left" headers="idm140059288307600" valign="top"> <p>
									*<a class="link" href="#openshiftstatemetricsconfig" title="8.20. OpenShiftStateMetricsConfig">OpenShiftStateMetricsConfig</a>
</p>
</td><td align="left" headers="idm140059288306512" valign="top"> <p>
<code class="literal">OpenShiftMetricsConfig</code> defines settings for the <code class="literal">openshift-state-metrics</code> agent.
								</p>
</td></tr><tr><td align="left" headers="idm140059288308688" valign="top"> <p>
									telemeterClient
								</p>
</td><td align="left" headers="idm140059288307600" valign="top"> <p>
									*<a class="link" href="#telemeterclientconfig" title="8.27. TelemeterClientConfig">TelemeterClientConfig</a>
</p>
</td><td align="left" headers="idm140059288306512" valign="top"> <p>
<code class="literal">TelemeterClientConfig</code> defines settings for the Telemeter Client component.
								</p>
</td></tr><tr><td align="left" headers="idm140059288308688" valign="top"> <p>
									thanosQuerier
								</p>
</td><td align="left" headers="idm140059288307600" valign="top"> <p>
									*<a class="link" href="#thanosquerierconfig" title="8.28. ThanosQuerierConfig">ThanosQuerierConfig</a>
</p>
</td><td align="left" headers="idm140059288306512" valign="top"> <p>
<code class="literal">ThanosQuerierConfig</code> defines settings for the Thanos Querier component.
								</p>
</td></tr><tr><td align="left" headers="idm140059288308688" valign="top"> <p>
									nodeExporter
								</p>
</td><td align="left" headers="idm140059288307600" valign="top"> <p>
<a class="link" href="#nodeexporterconfig" title="8.19. NodeExporterConfig">NodeExporterConfig</a>
</p>
</td><td align="left" headers="idm140059288306512" valign="top"> <p>
<code class="literal">NodeExporterConfig</code> defines settings for the <code class="literal">node-exporter</code> agent.
								</p>
</td></tr><tr><td align="left" headers="idm140059288308688" valign="top"> <p>
									monitoringPlugin
								</p>
</td><td align="left" headers="idm140059288307600" valign="top"> <p>
									*<a class="link" href="#monitoringpluginconfig" title="8.8. MonitoringPluginConfig">MonitoringPluginConfig</a>
</p>
</td><td align="left" headers="idm140059288306512" valign="top"> <p>
<code class="literal">MonitoringPluginConfig</code> defines settings for the monitoring <code class="literal">console-plugin</code> component.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="kubestatemetricsconfig"><div class="titlepage"><div><div><h3 class="title">8.6. KubeStateMetricsConfig</h3></div></div></div><section class="section" id="description-5"><div class="titlepage"><div><div><h4 class="title">8.6.1. Description</h4></div></div></div><p>
					The <code class="literal">KubeStateMetricsConfig</code> resource defines settings for the <code class="literal">kube-state-metrics</code> agent.
				</p><p>
					Appears in: <a class="link" href="#clustermonitoringconfiguration" title="8.5. ClusterMonitoringConfiguration">ClusterMonitoringConfiguration</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059285453968" scope="col" valign="top">Property</th><th align="left" id="idm140059285452880" scope="col" valign="top">Type</th><th align="left" id="idm140059285451792" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059285453968" valign="top"> <p>
									nodeSelector
								</p>
</td><td align="left" headers="idm140059285452880" valign="top"> <p>
									map[string]string
								</p>
</td><td align="left" headers="idm140059285451792" valign="top"> <p>
									Defines the nodes on which the pods are scheduled.
								</p>
</td></tr><tr><td align="left" headers="idm140059285453968" valign="top"> <p>
									resources
								</p>
</td><td align="left" headers="idm140059285452880" valign="top"> <p>
									*v1.ResourceRequirements
								</p>
</td><td align="left" headers="idm140059285451792" valign="top"> <p>
									Defines resource requests and limits for the <code class="literal">KubeStateMetrics</code> container.
								</p>
</td></tr><tr><td align="left" headers="idm140059285453968" valign="top"> <p>
									tolerations
								</p>
</td><td align="left" headers="idm140059285452880" valign="top"> <p>
									[]v1.Toleration
								</p>
</td><td align="left" headers="idm140059285451792" valign="top"> <p>
									Defines tolerations for the pods.
								</p>
</td></tr><tr><td align="left" headers="idm140059285453968" valign="top"> <p>
									topologySpreadConstraints
								</p>
</td><td align="left" headers="idm140059285452880" valign="top"> <p>
									[]v1.TopologySpreadConstraint
								</p>
</td><td align="left" headers="idm140059285451792" valign="top"> <p>
									Defines a pod’s topology spread constraints.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="metricsserverconfig"><div class="titlepage"><div><div><h3 class="title">8.7. MetricsServerConfig</h3></div></div></div><section class="section" id="description-6"><div class="titlepage"><div><div><h4 class="title">8.7.1. Description</h4></div></div></div><p>
					The <code class="literal">MetricsServerConfig</code> resource defines settings for the Metrics Server component.
				</p><p>
					Appears in: <a class="link" href="#clustermonitoringconfiguration" title="8.5. ClusterMonitoringConfiguration">ClusterMonitoringConfiguration</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059285416864" scope="col" valign="top">Property</th><th align="left" id="idm140059285415776" scope="col" valign="top">Type</th><th align="left" id="idm140059285414688" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059285416864" valign="top"> <p>
									audit
								</p>
</td><td align="left" headers="idm140059285415776" valign="top"> <p>
									*Audit
								</p>
</td><td align="left" headers="idm140059285414688" valign="top"> <p>
									Defines the audit configuration used by the Metrics Server instance. Possible profile values are <code class="literal">Metadata</code>, <code class="literal">Request</code>, <code class="literal">RequestResponse</code>, and <code class="literal">None</code>. The default value is <code class="literal">Metadata</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059285416864" valign="top"> <p>
									nodeSelector
								</p>
</td><td align="left" headers="idm140059285415776" valign="top"> <p>
									map[string]string
								</p>
</td><td align="left" headers="idm140059285414688" valign="top"> <p>
									Defines the nodes on which the pods are scheduled.
								</p>
</td></tr><tr><td align="left" headers="idm140059285416864" valign="top"> <p>
									tolerations
								</p>
</td><td align="left" headers="idm140059285415776" valign="top"> <p>
									[]v1.Toleration
								</p>
</td><td align="left" headers="idm140059285414688" valign="top"> <p>
									Defines tolerations for the pods.
								</p>
</td></tr><tr><td align="left" headers="idm140059285416864" valign="top"> <p>
									resources
								</p>
</td><td align="left" headers="idm140059285415776" valign="top"> <p>
									*v1.ResourceRequirements
								</p>
</td><td align="left" headers="idm140059285414688" valign="top"> <p>
									Defines resource requests and limits for the Metrics Server container.
								</p>
</td></tr><tr><td align="left" headers="idm140059285416864" valign="top"> <p>
									topologySpreadConstraints
								</p>
</td><td align="left" headers="idm140059285415776" valign="top"> <p>
									[]v1.TopologySpreadConstraint
								</p>
</td><td align="left" headers="idm140059285414688" valign="top"> <p>
									Defines a pod’s topology spread constraints.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="monitoringpluginconfig"><div class="titlepage"><div><div><h3 class="title">8.8. MonitoringPluginConfig</h3></div></div></div><section class="section" id="description-7"><div class="titlepage"><div><div><h4 class="title">8.8.1. Description</h4></div></div></div><p>
					The <code class="literal">MonitoringPluginConfig</code> resource defines settings for the web console plugin component in the <code class="literal">openshift-monitoring</code> namespace.
				</p><p>
					Appears in: <a class="link" href="#clustermonitoringconfiguration" title="8.5. ClusterMonitoringConfiguration">ClusterMonitoringConfiguration</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059287661440" scope="col" valign="top">Property</th><th align="left" id="idm140059287660352" scope="col" valign="top">Type</th><th align="left" id="idm140059287659264" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059287661440" valign="top"> <p>
									nodeSelector
								</p>
</td><td align="left" headers="idm140059287660352" valign="top"> <p>
									map[string]string
								</p>
</td><td align="left" headers="idm140059287659264" valign="top"> <p>
									Defines the nodes on which the pods are scheduled.
								</p>
</td></tr><tr><td align="left" headers="idm140059287661440" valign="top"> <p>
									resources
								</p>
</td><td align="left" headers="idm140059287660352" valign="top"> <p>
									*v1.ResourceRequirements
								</p>
</td><td align="left" headers="idm140059287659264" valign="top"> <p>
									Defines resource requests and limits for the <code class="literal">console-plugin</code> container.
								</p>
</td></tr><tr><td align="left" headers="idm140059287661440" valign="top"> <p>
									tolerations
								</p>
</td><td align="left" headers="idm140059287660352" valign="top"> <p>
									[]v1.Toleration
								</p>
</td><td align="left" headers="idm140059287659264" valign="top"> <p>
									Defines tolerations for the pods.
								</p>
</td></tr><tr><td align="left" headers="idm140059287661440" valign="top"> <p>
									topologySpreadConstraints
								</p>
</td><td align="left" headers="idm140059287660352" valign="top"> <p>
									[]v1.TopologySpreadConstraint
								</p>
</td><td align="left" headers="idm140059287659264" valign="top"> <p>
									Defines a pod’s topology spread constraints.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="nodeexportercollectorbuddyinfoconfig"><div class="titlepage"><div><div><h3 class="title">8.9. NodeExporterCollectorBuddyInfoConfig</h3></div></div></div><section class="section" id="description-8"><div class="titlepage"><div><div><h4 class="title">8.9.1. Description</h4></div></div></div><p>
					The <code class="literal">NodeExporterCollectorBuddyInfoConfig</code> resource works as an on/off switch for the <code class="literal">buddyinfo</code> collector of the <code class="literal">node-exporter</code> agent. By default, the <code class="literal">buddyinfo</code> collector is disabled.
				</p><p>
					Appears in: <a class="link" href="#nodeexportercollectorconfig" title="8.10. NodeExporterCollectorConfig">NodeExporterCollectorConfig</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059289324224" scope="col" valign="top">Property</th><th align="left" id="idm140059289323136" scope="col" valign="top">Type</th><th align="left" id="idm140059289322048" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059289324224" valign="top"> <p>
									enabled
								</p>
</td><td align="left" headers="idm140059289323136" valign="top"> <p>
									bool
								</p>
</td><td align="left" headers="idm140059289322048" valign="top"> <p>
									A Boolean flag that enables or disables the <code class="literal">buddyinfo</code> collector.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="nodeexportercollectorconfig"><div class="titlepage"><div><div><h3 class="title">8.10. NodeExporterCollectorConfig</h3></div></div></div><section class="section" id="description-9"><div class="titlepage"><div><div><h4 class="title">8.10.1. Description</h4></div></div></div><p>
					The <code class="literal">NodeExporterCollectorConfig</code> resource defines settings for individual collectors of the <code class="literal">node-exporter</code> agent.
				</p><p>
					Appears in: <a class="link" href="#nodeexporterconfig" title="8.19. NodeExporterConfig">NodeExporterConfig</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059289302560" scope="col" valign="top">Property</th><th align="left" id="idm140059289301472" scope="col" valign="top">Type</th><th align="left" id="idm140059289300384" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059289302560" valign="top"> <p>
									cpufreq
								</p>
</td><td align="left" headers="idm140059289301472" valign="top"> <p>
<a class="link" href="#nodeexportercollectorcpufreqconfig" title="8.11. NodeExporterCollectorCpufreqConfig">NodeExporterCollectorCpufreqConfig</a>
</p>
</td><td align="left" headers="idm140059289300384" valign="top"> <p>
									Defines the configuration of the <code class="literal">cpufreq</code> collector, which collects CPU frequency statistics. Disabled by default.
								</p>
</td></tr><tr><td align="left" headers="idm140059289302560" valign="top"> <p>
									tcpstat
								</p>
</td><td align="left" headers="idm140059289301472" valign="top"> <p>
<a class="link" href="#nodeexportercollectortcpstatconfig" title="8.18. NodeExporterCollectorTcpStatConfig">NodeExporterCollectorTcpStatConfig</a>
</p>
</td><td align="left" headers="idm140059289300384" valign="top"> <p>
									Defines the configuration of the <code class="literal">tcpstat</code> collector, which collects TCP connection statistics. Disabled by default.
								</p>
</td></tr><tr><td align="left" headers="idm140059289302560" valign="top"> <p>
									netdev
								</p>
</td><td align="left" headers="idm140059289301472" valign="top"> <p>
<a class="link" href="#nodeexportercollectornetdevconfig" title="8.15. NodeExporterCollectorNetDevConfig">NodeExporterCollectorNetDevConfig</a>
</p>
</td><td align="left" headers="idm140059289300384" valign="top"> <p>
									Defines the configuration of the <code class="literal">netdev</code> collector, which collects network devices statistics. Enabled by default.
								</p>
</td></tr><tr><td align="left" headers="idm140059289302560" valign="top"> <p>
									netclass
								</p>
</td><td align="left" headers="idm140059289301472" valign="top"> <p>
<a class="link" href="#nodeexportercollectornetclassconfig" title="8.14. NodeExporterCollectorNetClassConfig">NodeExporterCollectorNetClassConfig</a>
</p>
</td><td align="left" headers="idm140059289300384" valign="top"> <p>
									Defines the configuration of the <code class="literal">netclass</code> collector, which collects information about network devices. Enabled by default.
								</p>
</td></tr><tr><td align="left" headers="idm140059289302560" valign="top"> <p>
									buddyinfo
								</p>
</td><td align="left" headers="idm140059289301472" valign="top"> <p>
<a class="link" href="#nodeexportercollectorbuddyinfoconfig" title="8.9. NodeExporterCollectorBuddyInfoConfig">NodeExporterCollectorBuddyInfoConfig</a>
</p>
</td><td align="left" headers="idm140059289300384" valign="top"> <p>
									Defines the configuration of the <code class="literal">buddyinfo</code> collector, which collects statistics about memory fragmentation from the <code class="literal">node_buddyinfo_blocks</code> metric. This metric collects data from <code class="literal">/proc/buddyinfo</code>. Disabled by default.
								</p>
</td></tr><tr><td align="left" headers="idm140059289302560" valign="top"> <p>
									mountstats
								</p>
</td><td align="left" headers="idm140059289301472" valign="top"> <p>
<a class="link" href="#nodeexportercollectormountstatsconfig" title="8.13. NodeExporterCollectorMountStatsConfig">NodeExporterCollectorMountStatsConfig</a>
</p>
</td><td align="left" headers="idm140059289300384" valign="top"> <p>
									Defines the configuration of the <code class="literal">mountstats</code> collector, which collects statistics about NFS volume I/O activities. Disabled by default.
								</p>
</td></tr><tr><td align="left" headers="idm140059289302560" valign="top"> <p>
									ksmd
								</p>
</td><td align="left" headers="idm140059289301472" valign="top"> <p>
<a class="link" href="#nodeexportercollectorksmdconfig" title="8.12. NodeExporterCollectorKSMDConfig">NodeExporterCollectorKSMDConfig</a>
</p>
</td><td align="left" headers="idm140059289300384" valign="top"> <p>
									Defines the configuration of the <code class="literal">ksmd</code> collector, which collects statistics from the kernel same-page merger daemon. Disabled by default.
								</p>
</td></tr><tr><td align="left" headers="idm140059289302560" valign="top"> <p>
									processes
								</p>
</td><td align="left" headers="idm140059289301472" valign="top"> <p>
<a class="link" href="#nodeexportercollectorprocessesconfig" title="8.16. NodeExporterCollectorProcessesConfig">NodeExporterCollectorProcessesConfig</a>
</p>
</td><td align="left" headers="idm140059289300384" valign="top"> <p>
									Defines the configuration of the <code class="literal">processes</code> collector, which collects statistics from processes and threads running in the system. Disabled by default.
								</p>
</td></tr><tr><td align="left" headers="idm140059289302560" valign="top"> <p>
									systemd
								</p>
</td><td align="left" headers="idm140059289301472" valign="top"> <p>
<a class="link" href="#nodeexportercollectorsystemdconfig" title="8.17. NodeExporterCollectorSystemdConfig">NodeExporterCollectorSystemdConfig</a>
</p>
</td><td align="left" headers="idm140059289300384" valign="top"> <p>
									Defines the configuration of the <code class="literal">systemd</code> collector, which collects statistics on the systemd daemon and its managed services. Disabled by default.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="nodeexportercollectorcpufreqconfig"><div class="titlepage"><div><div><h3 class="title">8.11. NodeExporterCollectorCpufreqConfig</h3></div></div></div><section class="section" id="description-10"><div class="titlepage"><div><div><h4 class="title">8.11.1. Description</h4></div></div></div><p>
					Use the <code class="literal">NodeExporterCollectorCpufreqConfig</code> resource to enable or disable the <code class="literal">cpufreq</code> collector of the <code class="literal">node-exporter</code> agent. By default, the <code class="literal">cpufreq</code> collector is disabled. Under certain circumstances, enabling the <code class="literal">cpufreq</code> collector increases CPU usage on machines with many cores. If you enable this collector and have machines with many cores, monitor your systems closely for excessive CPU usage.
				</p><p>
					Appears in: <a class="link" href="#nodeexportercollectorconfig" title="8.10. NodeExporterCollectorConfig">NodeExporterCollectorConfig</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059285855808" scope="col" valign="top">Property</th><th align="left" id="idm140059285854720" scope="col" valign="top">Type</th><th align="left" id="idm140059285853632" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059285855808" valign="top"> <p>
									enabled
								</p>
</td><td align="left" headers="idm140059285854720" valign="top"> <p>
									bool
								</p>
</td><td align="left" headers="idm140059285853632" valign="top"> <p>
									A Boolean flag that enables or disables the <code class="literal">cpufreq</code> collector.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="nodeexportercollectorksmdconfig"><div class="titlepage"><div><div><h3 class="title">8.12. NodeExporterCollectorKSMDConfig</h3></div></div></div><section class="section" id="description-11"><div class="titlepage"><div><div><h4 class="title">8.12.1. Description</h4></div></div></div><p>
					Use the <code class="literal">NodeExporterCollectorKSMDConfig</code> resource to enable or disable the <code class="literal">ksmd</code> collector of the <code class="literal">node-exporter</code> agent. By default, the <code class="literal">ksmd</code> collector is disabled.
				</p><p>
					Appears in: <a class="link" href="#nodeexportercollectorconfig" title="8.10. NodeExporterCollectorConfig">NodeExporterCollectorConfig</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059285343824" scope="col" valign="top">Property</th><th align="left" id="idm140059285342736" scope="col" valign="top">Type</th><th align="left" id="idm140059285341648" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059285343824" valign="top"> <p>
									enabled
								</p>
</td><td align="left" headers="idm140059285342736" valign="top"> <p>
									bool
								</p>
</td><td align="left" headers="idm140059285341648" valign="top"> <p>
									A Boolean flag that enables or disables the <code class="literal">ksmd</code> collector.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="nodeexportercollectormountstatsconfig"><div class="titlepage"><div><div><h3 class="title">8.13. NodeExporterCollectorMountStatsConfig</h3></div></div></div><section class="section" id="description-12"><div class="titlepage"><div><div><h4 class="title">8.13.1. Description</h4></div></div></div><p>
					Use the <code class="literal">NodeExporterCollectorMountStatsConfig</code> resource to enable or disable the <code class="literal">mountstats</code> collector of the <code class="literal">node-exporter</code> agent. By default, the <code class="literal">mountstats</code> collector is disabled. If you enable the collector, the following metrics become available: <code class="literal">node_mountstats_nfs_read_bytes_total</code>, <code class="literal">node_mountstats_nfs_write_bytes_total</code>, and <code class="literal">node_mountstats_nfs_operations_requests_total</code>. Be aware that these metrics can have a high cardinality. If you enable this collector, closely monitor any increases in memory usage for the <code class="literal">prometheus-k8s</code> pods.
				</p><p>
					Appears in: <a class="link" href="#nodeexportercollectorconfig" title="8.10. NodeExporterCollectorConfig">NodeExporterCollectorConfig</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059285318912" scope="col" valign="top">Property</th><th align="left" id="idm140059285317824" scope="col" valign="top">Type</th><th align="left" id="idm140059285316736" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059285318912" valign="top"> <p>
									enabled
								</p>
</td><td align="left" headers="idm140059285317824" valign="top"> <p>
									bool
								</p>
</td><td align="left" headers="idm140059285316736" valign="top"> <p>
									A Boolean flag that enables or disables the <code class="literal">mountstats</code> collector.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="nodeexportercollectornetclassconfig"><div class="titlepage"><div><div><h3 class="title">8.14. NodeExporterCollectorNetClassConfig</h3></div></div></div><section class="section" id="description-13"><div class="titlepage"><div><div><h4 class="title">8.14.1. Description</h4></div></div></div><p>
					Use the <code class="literal">NodeExporterCollectorNetClassConfig</code> resource to enable or disable the <code class="literal">netclass</code> collector of the <code class="literal">node-exporter</code> agent. By default, the <code class="literal">netclass</code> collector is enabled. If you disable this collector, these metrics become unavailable: <code class="literal">node_network_info</code>, <code class="literal">node_network_address_assign_type</code>, <code class="literal">node_network_carrier</code>, <code class="literal">node_network_carrier_changes_total</code>, <code class="literal">node_network_carrier_up_changes_total</code>, <code class="literal">node_network_carrier_down_changes_total</code>, <code class="literal">node_network_device_id</code>, <code class="literal">node_network_dormant</code>, <code class="literal">node_network_flags</code>, <code class="literal">node_network_iface_id</code>, <code class="literal">node_network_iface_link</code>, <code class="literal">node_network_iface_link_mode</code>, <code class="literal">node_network_mtu_bytes</code>, <code class="literal">node_network_name_assign_type</code>, <code class="literal">node_network_net_dev_group</code>, <code class="literal">node_network_speed_bytes</code>, <code class="literal">node_network_transmit_queue_length</code>, and <code class="literal">node_network_protocol_type</code>.
				</p><p>
					Appears in: <a class="link" href="#nodeexportercollectorconfig" title="8.10. NodeExporterCollectorConfig">NodeExporterCollectorConfig</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059288870896" scope="col" valign="top">Property</th><th align="left" id="idm140059288869808" scope="col" valign="top">Type</th><th align="left" id="idm140059288868720" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059288870896" valign="top"> <p>
									enabled
								</p>
</td><td align="left" headers="idm140059288869808" valign="top"> <p>
									bool
								</p>
</td><td align="left" headers="idm140059288868720" valign="top"> <p>
									A Boolean flag that enables or disables the <code class="literal">netclass</code> collector.
								</p>
</td></tr><tr><td align="left" headers="idm140059288870896" valign="top"> <p>
									useNetlink
								</p>
</td><td align="left" headers="idm140059288869808" valign="top"> <p>
									bool
								</p>
</td><td align="left" headers="idm140059288868720" valign="top"> <p>
									A Boolean flag that activates the <code class="literal">netlink</code> implementation of the <code class="literal">netclass</code> collector. The default value is <code class="literal">true</code>, which activates the <code class="literal">netlink</code> mode. This implementation improves the performance of the <code class="literal">netclass</code> collector.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="nodeexportercollectornetdevconfig"><div class="titlepage"><div><div><h3 class="title">8.15. NodeExporterCollectorNetDevConfig</h3></div></div></div><section class="section" id="description-14"><div class="titlepage"><div><div><h4 class="title">8.15.1. Description</h4></div></div></div><p>
					Use the <code class="literal">NodeExporterCollectorNetDevConfig</code> resource to enable or disable the <code class="literal">netdev</code> collector of the <code class="literal">node-exporter</code> agent. By default, the <code class="literal">netdev</code> collector is enabled. If disabled, these metrics become unavailable: <code class="literal">node_network_receive_bytes_total</code>, <code class="literal">node_network_receive_compressed_total</code>, <code class="literal">node_network_receive_drop_total</code>, <code class="literal">node_network_receive_errs_total</code>, <code class="literal">node_network_receive_fifo_total</code>, <code class="literal">node_network_receive_frame_total</code>, <code class="literal">node_network_receive_multicast_total</code>, <code class="literal">node_network_receive_nohandler_total</code>, <code class="literal">node_network_receive_packets_total</code>, <code class="literal">node_network_transmit_bytes_total</code>, <code class="literal">node_network_transmit_carrier_total</code>, <code class="literal">node_network_transmit_colls_total</code>, <code class="literal">node_network_transmit_compressed_total</code>, <code class="literal">node_network_transmit_drop_total</code>, <code class="literal">node_network_transmit_errs_total</code>, <code class="literal">node_network_transmit_fifo_total</code>, and <code class="literal">node_network_transmit_packets_total</code>.
				</p><p>
					Appears in: <a class="link" href="#nodeexportercollectorconfig" title="8.10. NodeExporterCollectorConfig">NodeExporterCollectorConfig</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059288832048" scope="col" valign="top">Property</th><th align="left" id="idm140059288830960" scope="col" valign="top">Type</th><th align="left" id="idm140059288829872" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059288832048" valign="top"> <p>
									enabled
								</p>
</td><td align="left" headers="idm140059288830960" valign="top"> <p>
									bool
								</p>
</td><td align="left" headers="idm140059288829872" valign="top"> <p>
									A Boolean flag that enables or disables the <code class="literal">netdev</code> collector.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="nodeexportercollectorprocessesconfig"><div class="titlepage"><div><div><h3 class="title">8.16. NodeExporterCollectorProcessesConfig</h3></div></div></div><section class="section" id="description-15"><div class="titlepage"><div><div><h4 class="title">8.16.1. Description</h4></div></div></div><p>
					Use the <code class="literal">NodeExporterCollectorProcessesConfig</code> resource to enable or disable the <code class="literal">processes</code> collector of the <code class="literal">node-exporter</code> agent. If the collector is enabled, the following metrics become available: <code class="literal">node_processes_max_processes</code>, <code class="literal">node_processes_pids</code>, <code class="literal">node_processes_state</code>, <code class="literal">node_processes_threads</code>, <code class="literal">node_processes_threads_state</code>. The metric <code class="literal">node_processes_state</code> and <code class="literal">node_processes_threads_state</code> can have up to five series each, depending on the state of the processes and threads. The possible states of a process or a thread are: <code class="literal">D</code> (UNINTERRUPTABLE_SLEEP), <code class="literal">R</code> (RUNNING &amp; RUNNABLE), <code class="literal">S</code> (INTERRUPTABLE_SLEEP), <code class="literal">T</code> (STOPPED), or <code class="literal">Z</code> (ZOMBIE). By default, the <code class="literal">processes</code> collector is disabled.
				</p><p>
					Appears in: <a class="link" href="#nodeexportercollectorconfig" title="8.10. NodeExporterCollectorConfig">NodeExporterCollectorConfig</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059286707552" scope="col" valign="top">Property</th><th align="left" id="idm140059286706464" scope="col" valign="top">Type</th><th align="left" id="idm140059286705376" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059286707552" valign="top"> <p>
									enabled
								</p>
</td><td align="left" headers="idm140059286706464" valign="top"> <p>
									bool
								</p>
</td><td align="left" headers="idm140059286705376" valign="top"> <p>
									A Boolean flag that enables or disables the <code class="literal">processes</code> collector.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="nodeexportercollectorsystemdconfig"><div class="titlepage"><div><div><h3 class="title">8.17. NodeExporterCollectorSystemdConfig</h3></div></div></div><section class="section" id="description-16"><div class="titlepage"><div><div><h4 class="title">8.17.1. Description</h4></div></div></div><p>
					Use the <code class="literal">NodeExporterCollectorSystemdConfig</code> resource to enable or disable the <code class="literal">systemd</code> collector of the <code class="literal">node-exporter</code> agent. By default, the <code class="literal">systemd</code> collector is disabled. If enabled, the following metrics become available: <code class="literal">node_systemd_system_running</code>, <code class="literal">node_systemd_units</code>, <code class="literal">node_systemd_version</code>. If the unit uses a socket, it also generates the following metrics: <code class="literal">node_systemd_socket_accepted_connections_total</code>, <code class="literal">node_systemd_socket_current_connections</code>, <code class="literal">node_systemd_socket_refused_connections_total</code>. You can use the <code class="literal">units</code> parameter to select the <code class="literal">systemd</code> units to be included by the <code class="literal">systemd</code> collector. The selected units are used to generate the <code class="literal">node_systemd_unit_state</code> metric, which shows the state of each <code class="literal">systemd</code> unit. However, this metric’s cardinality might be high (at least five series per unit per node). If you enable this collector with a long list of selected units, closely monitor the <code class="literal">prometheus-k8s</code> deployment for excessive memory usage. Note that the <code class="literal">node_systemd_timer_last_trigger_seconds</code> metric is only shown if you have configured the value of the <code class="literal">units</code> parameter as <code class="literal">logrotate.timer</code>.
				</p><p>
					Appears in: <a class="link" href="#nodeexportercollectorconfig" title="8.10. NodeExporterCollectorConfig">NodeExporterCollectorConfig</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059286676960" scope="col" valign="top">Property</th><th align="left" id="idm140059286675872" scope="col" valign="top">Type</th><th align="left" id="idm140059286674784" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059286676960" valign="top"> <p>
									enabled
								</p>
</td><td align="left" headers="idm140059286675872" valign="top"> <p>
									bool
								</p>
</td><td align="left" headers="idm140059286674784" valign="top"> <p>
									A Boolean flag that enables or disables the <code class="literal">systemd</code> collector.
								</p>
</td></tr><tr><td align="left" headers="idm140059286676960" valign="top"> <p>
									units
								</p>
</td><td align="left" headers="idm140059286675872" valign="top"> <p>
									[]string
								</p>
</td><td align="left" headers="idm140059286674784" valign="top"> <p>
									A list of regular expression (regex) patterns that match systemd units to be included by the <code class="literal">systemd</code> collector. By default, the list is empty, so the collector exposes no metrics for systemd units.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="nodeexportercollectortcpstatconfig"><div class="titlepage"><div><div><h3 class="title">8.18. NodeExporterCollectorTcpStatConfig</h3></div></div></div><section class="section" id="description-17"><div class="titlepage"><div><div><h4 class="title">8.18.1. Description</h4></div></div></div><p>
					The <code class="literal">NodeExporterCollectorTcpStatConfig</code> resource works as an on/off switch for the <code class="literal">tcpstat</code> collector of the <code class="literal">node-exporter</code> agent. By default, the <code class="literal">tcpstat</code> collector is disabled.
				</p><p>
					Appears in: <a class="link" href="#nodeexportercollectorconfig" title="8.10. NodeExporterCollectorConfig">NodeExporterCollectorConfig</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059288262624" scope="col" valign="top">Property</th><th align="left" id="idm140059288261536" scope="col" valign="top">Type</th><th align="left" id="idm140059288260448" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059288262624" valign="top"> <p>
									enabled
								</p>
</td><td align="left" headers="idm140059288261536" valign="top"> <p>
									bool
								</p>
</td><td align="left" headers="idm140059288260448" valign="top"> <p>
									A Boolean flag that enables or disables the <code class="literal">tcpstat</code> collector.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="nodeexporterconfig"><div class="titlepage"><div><div><h3 class="title">8.19. NodeExporterConfig</h3></div></div></div><section class="section" id="description-18"><div class="titlepage"><div><div><h4 class="title">8.19.1. Description</h4></div></div></div><p>
					The <code class="literal">NodeExporterConfig</code> resource defines settings for the <code class="literal">node-exporter</code> agent.
				</p><p>
					Appears in: <a class="link" href="#clustermonitoringconfiguration" title="8.5. ClusterMonitoringConfiguration">ClusterMonitoringConfiguration</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059288240976" scope="col" valign="top">Property</th><th align="left" id="idm140059288239888" scope="col" valign="top">Type</th><th align="left" id="idm140059288238800" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059288240976" valign="top"> <p>
									collectors
								</p>
</td><td align="left" headers="idm140059288239888" valign="top"> <p>
<a class="link" href="#nodeexportercollectorconfig" title="8.10. NodeExporterCollectorConfig">NodeExporterCollectorConfig</a>
</p>
</td><td align="left" headers="idm140059288238800" valign="top"> <p>
									Defines which collectors are enabled and their additional configuration parameters.
								</p>
</td></tr><tr><td align="left" headers="idm140059288240976" valign="top"> <p>
									maxProcs
								</p>
</td><td align="left" headers="idm140059288239888" valign="top"> <p>
									uint32
								</p>
</td><td align="left" headers="idm140059288238800" valign="top"> <p>
									The target number of CPUs on which the node-exporter’s process will run. The default value is <code class="literal">0</code>, which means that node-exporter runs on all CPUs. If a kernel deadlock occurs or if performance degrades when reading from <code class="literal">sysfs</code> concurrently, you can change this value to <code class="literal">1</code>, which limits node-exporter to running on one CPU. For nodes with a high CPU count, you can set the limit to a low number, which saves resources by preventing Go routines from being scheduled to run on all CPUs. However, I/O performance degrades if the <code class="literal">maxProcs</code> value is set too low and there are many metrics to collect.
								</p>
</td></tr><tr><td align="left" headers="idm140059288240976" valign="top"> <p>
									ignoredNetworkDevices
								</p>
</td><td align="left" headers="idm140059288239888" valign="top"> <p>
									*[]string
								</p>
</td><td align="left" headers="idm140059288238800" valign="top"> <p>
									A list of network devices, defined as regular expressions, that you want to exclude from the relevant collector configuration such as <code class="literal">netdev</code> and <code class="literal">netclass</code>. If no list is specified, the Cluster Monitoring Operator uses a predefined list of devices to be excluded to minimize the impact on memory usage. If the list is empty, no devices are excluded. If you modify this setting, monitor the <code class="literal">prometheus-k8s</code> deployment closely for excessive memory usage.
								</p>
</td></tr><tr><td align="left" headers="idm140059288240976" valign="top"> <p>
									resources
								</p>
</td><td align="left" headers="idm140059288239888" valign="top"> <p>
									*v1.ResourceRequirements
								</p>
</td><td align="left" headers="idm140059288238800" valign="top"> <p>
									Defines resource requests and limits for the <code class="literal">NodeExporter</code> container.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="openshiftstatemetricsconfig"><div class="titlepage"><div><div><h3 class="title">8.20. OpenShiftStateMetricsConfig</h3></div></div></div><section class="section" id="description-19"><div class="titlepage"><div><div><h4 class="title">8.20.1. Description</h4></div></div></div><p>
					The <code class="literal">OpenShiftStateMetricsConfig</code> resource defines settings for the <code class="literal">openshift-state-metrics</code> agent.
				</p><p>
					Appears in: <a class="link" href="#clustermonitoringconfiguration" title="8.5. ClusterMonitoringConfiguration">ClusterMonitoringConfiguration</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059286649120" scope="col" valign="top">Property</th><th align="left" id="idm140059286648032" scope="col" valign="top">Type</th><th align="left" id="idm140059286646944" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059286649120" valign="top"> <p>
									nodeSelector
								</p>
</td><td align="left" headers="idm140059286648032" valign="top"> <p>
									map[string]string
								</p>
</td><td align="left" headers="idm140059286646944" valign="top"> <p>
									Defines the nodes on which the pods are scheduled.
								</p>
</td></tr><tr><td align="left" headers="idm140059286649120" valign="top"> <p>
									resources
								</p>
</td><td align="left" headers="idm140059286648032" valign="top"> <p>
									*v1.ResourceRequirements
								</p>
</td><td align="left" headers="idm140059286646944" valign="top"> <p>
									Defines resource requests and limits for the <code class="literal">OpenShiftStateMetrics</code> container.
								</p>
</td></tr><tr><td align="left" headers="idm140059286649120" valign="top"> <p>
									tolerations
								</p>
</td><td align="left" headers="idm140059286648032" valign="top"> <p>
									[]v1.Toleration
								</p>
</td><td align="left" headers="idm140059286646944" valign="top"> <p>
									Defines tolerations for the pods.
								</p>
</td></tr><tr><td align="left" headers="idm140059286649120" valign="top"> <p>
									topologySpreadConstraints
								</p>
</td><td align="left" headers="idm140059286648032" valign="top"> <p>
									[]v1.TopologySpreadConstraint
								</p>
</td><td align="left" headers="idm140059286646944" valign="top"> <p>
									Defines the pod’s topology spread constraints.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="prometheusk8sconfig"><div class="titlepage"><div><div><h3 class="title">8.21. PrometheusK8sConfig</h3></div></div></div><section class="section" id="description-20"><div class="titlepage"><div><div><h4 class="title">8.21.1. Description</h4></div></div></div><p>
					The <code class="literal">PrometheusK8sConfig</code> resource defines settings for the Prometheus component.
				</p><p>
					Appears in: <a class="link" href="#clustermonitoringconfiguration" title="8.5. ClusterMonitoringConfiguration">ClusterMonitoringConfiguration</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059286612016" scope="col" valign="top">Property</th><th align="left" id="idm140059286610928" scope="col" valign="top">Type</th><th align="left" id="idm140059286609840" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059286612016" valign="top"> <p>
									additionalAlertmanagerConfigs
								</p>
</td><td align="left" headers="idm140059286610928" valign="top"> <p>
									[]<a class="link" href="#additionalalertmanagerconfig" title="8.2. AdditionalAlertmanagerConfig">AdditionalAlertmanagerConfig</a>
</p>
</td><td align="left" headers="idm140059286609840" valign="top"> <p>
									Configures additional Alertmanager instances that receive alerts from the Prometheus component. By default, no additional Alertmanager instances are configured.
								</p>
</td></tr><tr><td align="left" headers="idm140059286612016" valign="top"> <p>
									enforcedBodySizeLimit
								</p>
</td><td align="left" headers="idm140059286610928" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059286609840" valign="top"> <p>
									Enforces a body size limit for Prometheus scraped metrics. If a scraped target’s body response is larger than the limit, the scrape will fail. The following values are valid: an empty value to specify no limit, a numeric value in Prometheus size format (such as <code class="literal">64MB</code>), or the string <code class="literal">automatic</code>, which indicates that the limit will be automatically calculated based on cluster capacity. The default value is empty, which indicates no limit.
								</p>
</td></tr><tr><td align="left" headers="idm140059286612016" valign="top"> <p>
									externalLabels
								</p>
</td><td align="left" headers="idm140059286610928" valign="top"> <p>
									map[string]string
								</p>
</td><td align="left" headers="idm140059286609840" valign="top"> <p>
									Defines labels to be added to any time series or alerts when communicating with external systems such as federation, remote storage, and Alertmanager. By default, no labels are added.
								</p>
</td></tr><tr><td align="left" headers="idm140059286612016" valign="top"> <p>
									logLevel
								</p>
</td><td align="left" headers="idm140059286610928" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059286609840" valign="top"> <p>
									Defines the log level setting for Prometheus. The possible values are: <code class="literal">error</code>, <code class="literal">warn</code>, <code class="literal">info</code>, and <code class="literal">debug</code>. The default value is <code class="literal">info</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059286612016" valign="top"> <p>
									nodeSelector
								</p>
</td><td align="left" headers="idm140059286610928" valign="top"> <p>
									map[string]string
								</p>
</td><td align="left" headers="idm140059286609840" valign="top"> <p>
									Defines the nodes on which the pods are scheduled.
								</p>
</td></tr><tr><td align="left" headers="idm140059286612016" valign="top"> <p>
									queryLogFile
								</p>
</td><td align="left" headers="idm140059286610928" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059286609840" valign="top"> <p>
									Specifies the file to which PromQL queries are logged. This setting can be either a filename, in which case the queries are saved to an <code class="literal">emptyDir</code> volume at <code class="literal">/var/log/prometheus</code>, or a full path to a location where an <code class="literal">emptyDir</code> volume will be mounted and the queries saved. Writing to <code class="literal">/dev/stderr</code>, <code class="literal">/dev/stdout</code> or <code class="literal">/dev/null</code> is supported, but writing to any other <code class="literal">/dev/</code> path is not supported. Relative paths are also not supported. By default, PromQL queries are not logged.
								</p>
</td></tr><tr><td align="left" headers="idm140059286612016" valign="top"> <p>
									remoteWrite
								</p>
</td><td align="left" headers="idm140059286610928" valign="top"> <p>
									[]<a class="link" href="#remotewritespec" title="8.25. RemoteWriteSpec">RemoteWriteSpec</a>
</p>
</td><td align="left" headers="idm140059286609840" valign="top"> <p>
									Defines the remote write configuration, including URL, authentication, and relabeling settings.
								</p>
</td></tr><tr><td align="left" headers="idm140059286612016" valign="top"> <p>
									resources
								</p>
</td><td align="left" headers="idm140059286610928" valign="top"> <p>
									*v1.ResourceRequirements
								</p>
</td><td align="left" headers="idm140059286609840" valign="top"> <p>
									Defines resource requests and limits for the <code class="literal">Prometheus</code> container.
								</p>
</td></tr><tr><td align="left" headers="idm140059286612016" valign="top"> <p>
									retention
								</p>
</td><td align="left" headers="idm140059286610928" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059286609840" valign="top"> <p>
									Defines the duration for which Prometheus retains data. This definition must be specified using the following regular expression pattern: <code class="literal">[0-9]+(ms|s|m|h|d|w|y)</code> (ms = milliseconds, s= seconds,m = minutes, h = hours, d = days, w = weeks, y = years). The default value is <code class="literal">15d</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059286612016" valign="top"> <p>
									retentionSize
								</p>
</td><td align="left" headers="idm140059286610928" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059286609840" valign="top"> <p>
									Defines the maximum amount of disk space used by data blocks plus the write-ahead log (WAL). Supported values are <code class="literal">B</code>, <code class="literal">KB</code>, <code class="literal">KiB</code>, <code class="literal">MB</code>, <code class="literal">MiB</code>, <code class="literal">GB</code>, <code class="literal">GiB</code>, <code class="literal">TB</code>, <code class="literal">TiB</code>, <code class="literal">PB</code>, <code class="literal">PiB</code>, <code class="literal">EB</code>, and <code class="literal">EiB</code>. By default, no limit is defined.
								</p>
</td></tr><tr><td align="left" headers="idm140059286612016" valign="top"> <p>
									tolerations
								</p>
</td><td align="left" headers="idm140059286610928" valign="top"> <p>
									[]v1.Toleration
								</p>
</td><td align="left" headers="idm140059286609840" valign="top"> <p>
									Defines tolerations for the pods.
								</p>
</td></tr><tr><td align="left" headers="idm140059286612016" valign="top"> <p>
									topologySpreadConstraints
								</p>
</td><td align="left" headers="idm140059286610928" valign="top"> <p>
									[]v1.TopologySpreadConstraint
								</p>
</td><td align="left" headers="idm140059286609840" valign="top"> <p>
									Defines the pod’s topology spread constraints.
								</p>
</td></tr><tr><td align="left" headers="idm140059286612016" valign="top"> <p>
									collectionProfile
								</p>
</td><td align="left" headers="idm140059286610928" valign="top"> <p>
									CollectionProfile
								</p>
</td><td align="left" headers="idm140059286609840" valign="top"> <p>
									Defines the metrics collection profile that Prometheus uses to collect metrics from the platform components. Supported values are <code class="literal">full</code> or <code class="literal">minimal</code>. In the <code class="literal">full</code> profile (default), Prometheus collects all metrics that are exposed by the platform components. In the <code class="literal">minimal</code> profile, Prometheus only collects metrics necessary for the default platform alerts, recording rules, telemetry, and console dashboards.
								</p>
</td></tr><tr><td align="left" headers="idm140059286612016" valign="top"> <p>
									volumeClaimTemplate
								</p>
</td><td align="left" headers="idm140059286610928" valign="top"> <p>
									*monv1.EmbeddedPersistentVolumeClaim
								</p>
</td><td align="left" headers="idm140059286609840" valign="top"> <p>
									Defines persistent storage for Prometheus. Use this setting to configure the persistent volume claim, including storage class, volume size and name.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="prometheusoperatorconfig"><div class="titlepage"><div><div><h3 class="title">8.22. PrometheusOperatorConfig</h3></div></div></div><section class="section" id="description-21"><div class="titlepage"><div><div><h4 class="title">8.22.1. Description</h4></div></div></div><p>
					The <code class="literal">PrometheusOperatorConfig</code> resource defines settings for the Prometheus Operator component.
				</p><p>
					Appears in: <a class="link" href="#clustermonitoringconfiguration" title="8.5. ClusterMonitoringConfiguration">ClusterMonitoringConfiguration</a>, <a class="link" href="#userworkloadconfiguration" title="8.31. UserWorkloadConfiguration">UserWorkloadConfiguration</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059289183408" scope="col" valign="top">Property</th><th align="left" id="idm140059289182320" scope="col" valign="top">Type</th><th align="left" id="idm140059289181232" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059289183408" valign="top"> <p>
									logLevel
								</p>
</td><td align="left" headers="idm140059289182320" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059289181232" valign="top"> <p>
									Defines the log level settings for Prometheus Operator. The possible values are <code class="literal">error</code>, <code class="literal">warn</code>, <code class="literal">info</code>, and <code class="literal">debug</code>. The default value is <code class="literal">info</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059289183408" valign="top"> <p>
									nodeSelector
								</p>
</td><td align="left" headers="idm140059289182320" valign="top"> <p>
									map[string]string
								</p>
</td><td align="left" headers="idm140059289181232" valign="top"> <p>
									Defines the nodes on which the pods are scheduled.
								</p>
</td></tr><tr><td align="left" headers="idm140059289183408" valign="top"> <p>
									resources
								</p>
</td><td align="left" headers="idm140059289182320" valign="top"> <p>
									*v1.ResourceRequirements
								</p>
</td><td align="left" headers="idm140059289181232" valign="top"> <p>
									Defines resource requests and limits for the <code class="literal">PrometheusOperator</code> container.
								</p>
</td></tr><tr><td align="left" headers="idm140059289183408" valign="top"> <p>
									tolerations
								</p>
</td><td align="left" headers="idm140059289182320" valign="top"> <p>
									[]v1.Toleration
								</p>
</td><td align="left" headers="idm140059289181232" valign="top"> <p>
									Defines tolerations for the pods.
								</p>
</td></tr><tr><td align="left" headers="idm140059289183408" valign="top"> <p>
									topologySpreadConstraints
								</p>
</td><td align="left" headers="idm140059289182320" valign="top"> <p>
									[]v1.TopologySpreadConstraint
								</p>
</td><td align="left" headers="idm140059289181232" valign="top"> <p>
									Defines the pod’s topology spread constraints.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="prometheusoperatoradmissionwebhookconfig"><div class="titlepage"><div><div><h3 class="title">8.23. PrometheusOperatorAdmissionWebhookConfig</h3></div></div></div><section class="section" id="description-22"><div class="titlepage"><div><div><h4 class="title">8.23.1. Description</h4></div></div></div><p>
					The <code class="literal">PrometheusOperatorAdmissionWebhookConfig</code> resource defines settings for the admission webhook workload for Prometheus Operator.
				</p><p>
					Appears in: <a class="link" href="#clustermonitoringconfiguration" title="8.5. ClusterMonitoringConfiguration">ClusterMonitoringConfiguration</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059289138384" scope="col" valign="top">Property</th><th align="left" id="idm140059289137296" scope="col" valign="top">Type</th><th align="left" id="idm140059289136208" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059289138384" valign="top"> <p>
									resources
								</p>
</td><td align="left" headers="idm140059289137296" valign="top"> <p>
									*v1.ResourceRequirements
								</p>
</td><td align="left" headers="idm140059289136208" valign="top"> <p>
									Defines resource requests and limits for the <code class="literal">prometheus-operator-admission-webhook</code> container.
								</p>
</td></tr><tr><td align="left" headers="idm140059289138384" valign="top"> <p>
									topologySpreadConstraints
								</p>
</td><td align="left" headers="idm140059289137296" valign="top"> <p>
									[]v1.TopologySpreadConstraint
								</p>
</td><td align="left" headers="idm140059289136208" valign="top"> <p>
									Defines a pod’s topology spread constraints.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="prometheusrestrictedconfig"><div class="titlepage"><div><div><h3 class="title">8.24. PrometheusRestrictedConfig</h3></div></div></div><section class="section" id="description-23"><div class="titlepage"><div><div><h4 class="title">8.24.1. Description</h4></div></div></div><p>
					The <code class="literal">PrometheusRestrictedConfig</code> resource defines the settings for the Prometheus component that monitors user-defined projects.
				</p><p>
					Appears in: <a class="link" href="#userworkloadconfiguration" title="8.31. UserWorkloadConfiguration">UserWorkloadConfiguration</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059288415056" scope="col" valign="top">Property</th><th align="left" id="idm140059288413968" scope="col" valign="top">Type</th><th align="left" id="idm140059288412880" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059288415056" valign="top"> <p>
									scrapeInterval
								</p>
</td><td align="left" headers="idm140059288413968" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059288412880" valign="top"> <p>
									Configures the default interval between consecutive scrapes in case the <code class="literal">ServiceMonitor</code> or <code class="literal">PodMonitor</code> resource does not specify any value. The interval must be set between 5 seconds and 5 minutes. The value can be expressed in: seconds (for example <code class="literal">30s</code>), minutes (for example <code class="literal">1m</code>) or a mix of minutes and seconds (for example <code class="literal">1m30s</code>). The default value is <code class="literal">30s</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059288415056" valign="top"> <p>
									evaluationInterval
								</p>
</td><td align="left" headers="idm140059288413968" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059288412880" valign="top"> <p>
									Configures the default interval between rule evaluations in case the <code class="literal">PrometheusRule</code> resource does not specify any value. The interval must be set between 5 seconds and 5 minutes. The value can be expressed in: seconds (for example <code class="literal">30s</code>), minutes (for example <code class="literal">1m</code>) or a mix of minutes and seconds (for example <code class="literal">1m30s</code>). It only applies to <code class="literal">PrometheusRule</code> resources with the <code class="literal">openshift.io/prometheus-rule-evaluation-scope=\"leaf-prometheus\"</code> label. The default value is <code class="literal">30s</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059288415056" valign="top"> <p>
									additionalAlertmanagerConfigs
								</p>
</td><td align="left" headers="idm140059288413968" valign="top"> <p>
									[]<a class="link" href="#additionalalertmanagerconfig" title="8.2. AdditionalAlertmanagerConfig">AdditionalAlertmanagerConfig</a>
</p>
</td><td align="left" headers="idm140059288412880" valign="top"> <p>
									Configures additional Alertmanager instances that receive alerts from the Prometheus component. By default, no additional Alertmanager instances are configured.
								</p>
</td></tr><tr><td align="left" headers="idm140059288415056" valign="top"> <p>
									enforcedLabelLimit
								</p>
</td><td align="left" headers="idm140059288413968" valign="top"> <p>
									*uint64
								</p>
</td><td align="left" headers="idm140059288412880" valign="top"> <p>
									Specifies a per-scrape limit on the number of labels accepted for a sample. If the number of labels exceeds this limit after metric relabeling, the entire scrape is treated as failed. The default value is <code class="literal">0</code>, which means that no limit is set.
								</p>
</td></tr><tr><td align="left" headers="idm140059288415056" valign="top"> <p>
									enforcedLabelNameLengthLimit
								</p>
</td><td align="left" headers="idm140059288413968" valign="top"> <p>
									*uint64
								</p>
</td><td align="left" headers="idm140059288412880" valign="top"> <p>
									Specifies a per-scrape limit on the length of a label name for a sample. If the length of a label name exceeds this limit after metric relabeling, the entire scrape is treated as failed. The default value is <code class="literal">0</code>, which means that no limit is set.
								</p>
</td></tr><tr><td align="left" headers="idm140059288415056" valign="top"> <p>
									enforcedLabelValueLengthLimit
								</p>
</td><td align="left" headers="idm140059288413968" valign="top"> <p>
									*uint64
								</p>
</td><td align="left" headers="idm140059288412880" valign="top"> <p>
									Specifies a per-scrape limit on the length of a label value for a sample. If the length of a label value exceeds this limit after metric relabeling, the entire scrape is treated as failed. The default value is <code class="literal">0</code>, which means that no limit is set.
								</p>
</td></tr><tr><td align="left" headers="idm140059288415056" valign="top"> <p>
									enforcedSampleLimit
								</p>
</td><td align="left" headers="idm140059288413968" valign="top"> <p>
									*uint64
								</p>
</td><td align="left" headers="idm140059288412880" valign="top"> <p>
									Specifies a global limit on the number of scraped samples that will be accepted. This setting overrides the <code class="literal">SampleLimit</code> value set in any user-defined <code class="literal">ServiceMonitor</code> or <code class="literal">PodMonitor</code> object if the value is greater than <code class="literal">enforcedTargetLimit</code>. Administrators can use this setting to keep the overall number of samples under control. The default value is <code class="literal">0</code>, which means that no limit is set.
								</p>
</td></tr><tr><td align="left" headers="idm140059288415056" valign="top"> <p>
									enforcedTargetLimit
								</p>
</td><td align="left" headers="idm140059288413968" valign="top"> <p>
									*uint64
								</p>
</td><td align="left" headers="idm140059288412880" valign="top"> <p>
									Specifies a global limit on the number of scraped targets. This setting overrides the <code class="literal">TargetLimit</code> value set in any user-defined <code class="literal">ServiceMonitor</code> or <code class="literal">PodMonitor</code> object if the value is greater than <code class="literal">enforcedSampleLimit</code>. Administrators can use this setting to keep the overall number of targets under control. The default value is <code class="literal">0</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059288415056" valign="top"> <p>
									externalLabels
								</p>
</td><td align="left" headers="idm140059288413968" valign="top"> <p>
									map[string]string
								</p>
</td><td align="left" headers="idm140059288412880" valign="top"> <p>
									Defines labels to be added to any time series or alerts when communicating with external systems such as federation, remote storage, and Alertmanager. By default, no labels are added.
								</p>
</td></tr><tr><td align="left" headers="idm140059288415056" valign="top"> <p>
									logLevel
								</p>
</td><td align="left" headers="idm140059288413968" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059288412880" valign="top"> <p>
									Defines the log level setting for Prometheus. The possible values are <code class="literal">error</code>, <code class="literal">warn</code>, <code class="literal">info</code>, and <code class="literal">debug</code>. The default setting is <code class="literal">info</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059288415056" valign="top"> <p>
									nodeSelector
								</p>
</td><td align="left" headers="idm140059288413968" valign="top"> <p>
									map[string]string
								</p>
</td><td align="left" headers="idm140059288412880" valign="top"> <p>
									Defines the nodes on which the pods are scheduled.
								</p>
</td></tr><tr><td align="left" headers="idm140059288415056" valign="top"> <p>
									queryLogFile
								</p>
</td><td align="left" headers="idm140059288413968" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059288412880" valign="top"> <p>
									Specifies the file to which PromQL queries are logged. This setting can be either a filename, in which case the queries are saved to an <code class="literal">emptyDir</code> volume at <code class="literal">/var/log/prometheus</code>, or a full path to a location where an <code class="literal">emptyDir</code> volume will be mounted and the queries saved. Writing to <code class="literal">/dev/stderr</code>, <code class="literal">/dev/stdout</code> or <code class="literal">/dev/null</code> is supported, but writing to any other <code class="literal">/dev/</code> path is not supported. Relative paths are also not supported. By default, PromQL queries are not logged.
								</p>
</td></tr><tr><td align="left" headers="idm140059288415056" valign="top"> <p>
									remoteWrite
								</p>
</td><td align="left" headers="idm140059288413968" valign="top"> <p>
									[]<a class="link" href="#remotewritespec" title="8.25. RemoteWriteSpec">RemoteWriteSpec</a>
</p>
</td><td align="left" headers="idm140059288412880" valign="top"> <p>
									Defines the remote write configuration, including URL, authentication, and relabeling settings.
								</p>
</td></tr><tr><td align="left" headers="idm140059288415056" valign="top"> <p>
									resources
								</p>
</td><td align="left" headers="idm140059288413968" valign="top"> <p>
									*v1.ResourceRequirements
								</p>
</td><td align="left" headers="idm140059288412880" valign="top"> <p>
									Defines resource requests and limits for the Prometheus container.
								</p>
</td></tr><tr><td align="left" headers="idm140059288415056" valign="top"> <p>
									retention
								</p>
</td><td align="left" headers="idm140059288413968" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059288412880" valign="top"> <p>
									Defines the duration for which Prometheus retains data. This definition must be specified using the following regular expression pattern: <code class="literal">[0-9]+(ms|s|m|h|d|w|y)</code> (ms = milliseconds, s= seconds,m = minutes, h = hours, d = days, w = weeks, y = years). The default value is <code class="literal">24h</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059288415056" valign="top"> <p>
									retentionSize
								</p>
</td><td align="left" headers="idm140059288413968" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059288412880" valign="top"> <p>
									Defines the maximum amount of disk space used by data blocks plus the write-ahead log (WAL). Supported values are <code class="literal">B</code>, <code class="literal">KB</code>, <code class="literal">KiB</code>, <code class="literal">MB</code>, <code class="literal">MiB</code>, <code class="literal">GB</code>, <code class="literal">GiB</code>, <code class="literal">TB</code>, <code class="literal">TiB</code>, <code class="literal">PB</code>, <code class="literal">PiB</code>, <code class="literal">EB</code>, and <code class="literal">EiB</code>. The default value is <code class="literal">nil</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059288415056" valign="top"> <p>
									tolerations
								</p>
</td><td align="left" headers="idm140059288413968" valign="top"> <p>
									[]v1.Toleration
								</p>
</td><td align="left" headers="idm140059288412880" valign="top"> <p>
									Defines tolerations for the pods.
								</p>
</td></tr><tr><td align="left" headers="idm140059288415056" valign="top"> <p>
									topologySpreadConstraints
								</p>
</td><td align="left" headers="idm140059288413968" valign="top"> <p>
									[]v1.TopologySpreadConstraint
								</p>
</td><td align="left" headers="idm140059288412880" valign="top"> <p>
									Defines the pod’s topology spread constraints.
								</p>
</td></tr><tr><td align="left" headers="idm140059288415056" valign="top"> <p>
									volumeClaimTemplate
								</p>
</td><td align="left" headers="idm140059288413968" valign="top"> <p>
									*monv1.EmbeddedPersistentVolumeClaim
								</p>
</td><td align="left" headers="idm140059288412880" valign="top"> <p>
									Defines persistent storage for Prometheus. Use this setting to configure the storage class and size of a volume.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="remotewritespec"><div class="titlepage"><div><div><h3 class="title">8.25. RemoteWriteSpec</h3></div></div></div><section class="section" id="description-24"><div class="titlepage"><div><div><h4 class="title">8.25.1. Description</h4></div></div></div><p>
					The <code class="literal">RemoteWriteSpec</code> resource defines the settings for remote write storage.
				</p></section><section class="section" id="required-2"><div class="titlepage"><div><div><h4 class="title">8.25.2. Required</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<code class="literal">url</code>
</li></ul></div><p>
					Appears in: <a class="link" href="#prometheusk8sconfig" title="8.21. PrometheusK8sConfig">PrometheusK8sConfig</a>, <a class="link" href="#prometheusrestrictedconfig" title="8.24. PrometheusRestrictedConfig">PrometheusRestrictedConfig</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059285258656" scope="col" valign="top">Property</th><th align="left" id="idm140059285257568" scope="col" valign="top">Type</th><th align="left" id="idm140059285256480" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059285258656" valign="top"> <p>
									authorization
								</p>
</td><td align="left" headers="idm140059285257568" valign="top"> <p>
									*monv1.SafeAuthorization
								</p>
</td><td align="left" headers="idm140059285256480" valign="top"> <p>
									Defines the authorization settings for remote write storage.
								</p>
</td></tr><tr><td align="left" headers="idm140059285258656" valign="top"> <p>
									basicAuth
								</p>
</td><td align="left" headers="idm140059285257568" valign="top"> <p>
									*monv1.BasicAuth
								</p>
</td><td align="left" headers="idm140059285256480" valign="top"> <p>
									Defines Basic authentication settings for the remote write endpoint URL.
								</p>
</td></tr><tr><td align="left" headers="idm140059285258656" valign="top"> <p>
									bearerTokenFile
								</p>
</td><td align="left" headers="idm140059285257568" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059285256480" valign="top"> <p>
									Defines the file that contains the bearer token for the remote write endpoint. However, because you cannot mount secrets in a pod, in practice you can only reference the token of the service account.
								</p>
</td></tr><tr><td align="left" headers="idm140059285258656" valign="top"> <p>
									headers
								</p>
</td><td align="left" headers="idm140059285257568" valign="top"> <p>
									map[string]string
								</p>
</td><td align="left" headers="idm140059285256480" valign="top"> <p>
									Specifies the custom HTTP headers to be sent along with each remote write request. Headers set by Prometheus cannot be overwritten.
								</p>
</td></tr><tr><td align="left" headers="idm140059285258656" valign="top"> <p>
									metadataConfig
								</p>
</td><td align="left" headers="idm140059285257568" valign="top"> <p>
									*monv1.MetadataConfig
								</p>
</td><td align="left" headers="idm140059285256480" valign="top"> <p>
									Defines settings for sending series metadata to remote write storage.
								</p>
</td></tr><tr><td align="left" headers="idm140059285258656" valign="top"> <p>
									name
								</p>
</td><td align="left" headers="idm140059285257568" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059285256480" valign="top"> <p>
									Defines the name of the remote write queue. This name is used in metrics and logging to differentiate queues. If specified, this name must be unique.
								</p>
</td></tr><tr><td align="left" headers="idm140059285258656" valign="top"> <p>
									oauth2
								</p>
</td><td align="left" headers="idm140059285257568" valign="top"> <p>
									*monv1.OAuth2
								</p>
</td><td align="left" headers="idm140059285256480" valign="top"> <p>
									Defines OAuth2 authentication settings for the remote write endpoint.
								</p>
</td></tr><tr><td align="left" headers="idm140059285258656" valign="top"> <p>
									proxyUrl
								</p>
</td><td align="left" headers="idm140059285257568" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059285256480" valign="top"> <p>
									Defines an optional proxy URL. If the cluster-wide proxy is enabled, it replaces the proxyUrl setting. The cluster-wide proxy supports both HTTP and HTTPS proxies, with HTTPS taking precedence.
								</p>
</td></tr><tr><td align="left" headers="idm140059285258656" valign="top"> <p>
									queueConfig
								</p>
</td><td align="left" headers="idm140059285257568" valign="top"> <p>
									*monv1.QueueConfig
								</p>
</td><td align="left" headers="idm140059285256480" valign="top"> <p>
									Allows tuning configuration for remote write queue parameters.
								</p>
</td></tr><tr><td align="left" headers="idm140059285258656" valign="top"> <p>
									remoteTimeout
								</p>
</td><td align="left" headers="idm140059285257568" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059285256480" valign="top"> <p>
									Defines the timeout value for requests to the remote write endpoint.
								</p>
</td></tr><tr><td align="left" headers="idm140059285258656" valign="top"> <p>
									sendExemplars
								</p>
</td><td align="left" headers="idm140059285257568" valign="top"> <p>
									*bool
								</p>
</td><td align="left" headers="idm140059285256480" valign="top"> <p>
									Enables sending exemplars via remote write. When enabled, this setting configures Prometheus to store a maximum of 100,000 exemplars in memory. This setting only applies to user-defined monitoring and is not applicable to core platform monitoring.
								</p>
</td></tr><tr><td align="left" headers="idm140059285258656" valign="top"> <p>
									sigv4
								</p>
</td><td align="left" headers="idm140059285257568" valign="top"> <p>
									*monv1.Sigv4
								</p>
</td><td align="left" headers="idm140059285256480" valign="top"> <p>
									Defines AWS Signature Version 4 authentication settings.
								</p>
</td></tr><tr><td align="left" headers="idm140059285258656" valign="top"> <p>
									tlsConfig
								</p>
</td><td align="left" headers="idm140059285257568" valign="top"> <p>
									*monv1.SafeTLSConfig
								</p>
</td><td align="left" headers="idm140059285256480" valign="top"> <p>
									Defines TLS authentication settings for the remote write endpoint.
								</p>
</td></tr><tr><td align="left" headers="idm140059285258656" valign="top"> <p>
									url
								</p>
</td><td align="left" headers="idm140059285257568" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059285256480" valign="top"> <p>
									Defines the URL of the remote write endpoint to which samples will be sent.
								</p>
</td></tr><tr><td align="left" headers="idm140059285258656" valign="top"> <p>
									writeRelabelConfigs
								</p>
</td><td align="left" headers="idm140059285257568" valign="top"> <p>
									[]monv1.RelabelConfig
								</p>
</td><td align="left" headers="idm140059285256480" valign="top"> <p>
									Defines the list of remote write relabel configurations.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="tlsconfig"><div class="titlepage"><div><div><h3 class="title">8.26. TLSConfig</h3></div></div></div><section class="section" id="description-25"><div class="titlepage"><div><div><h4 class="title">8.26.1. Description</h4></div></div></div><p>
					The <code class="literal">TLSConfig</code> resource configures the settings for TLS connections.
				</p></section><section class="section" id="required-3"><div class="titlepage"><div><div><h4 class="title">8.26.2. Required</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<code class="literal">insecureSkipVerify</code>
</li></ul></div><p>
					Appears in: <a class="link" href="#additionalalertmanagerconfig" title="8.2. AdditionalAlertmanagerConfig">AdditionalAlertmanagerConfig</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059294297072" scope="col" valign="top">Property</th><th align="left" id="idm140059294295984" scope="col" valign="top">Type</th><th align="left" id="idm140059294294896" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059294297072" valign="top"> <p>
									ca
								</p>
</td><td align="left" headers="idm140059294295984" valign="top"> <p>
									*v1.SecretKeySelector
								</p>
</td><td align="left" headers="idm140059294294896" valign="top"> <p>
									Defines the secret key reference containing the Certificate Authority (CA) to use for the remote host.
								</p>
</td></tr><tr><td align="left" headers="idm140059294297072" valign="top"> <p>
									cert
								</p>
</td><td align="left" headers="idm140059294295984" valign="top"> <p>
									*v1.SecretKeySelector
								</p>
</td><td align="left" headers="idm140059294294896" valign="top"> <p>
									Defines the secret key reference containing the public certificate to use for the remote host.
								</p>
</td></tr><tr><td align="left" headers="idm140059294297072" valign="top"> <p>
									key
								</p>
</td><td align="left" headers="idm140059294295984" valign="top"> <p>
									*v1.SecretKeySelector
								</p>
</td><td align="left" headers="idm140059294294896" valign="top"> <p>
									Defines the secret key reference containing the private key to use for the remote host.
								</p>
</td></tr><tr><td align="left" headers="idm140059294297072" valign="top"> <p>
									serverName
								</p>
</td><td align="left" headers="idm140059294295984" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059294294896" valign="top"> <p>
									Used to verify the hostname on the returned certificate.
								</p>
</td></tr><tr><td align="left" headers="idm140059294297072" valign="top"> <p>
									insecureSkipVerify
								</p>
</td><td align="left" headers="idm140059294295984" valign="top"> <p>
									bool
								</p>
</td><td align="left" headers="idm140059294294896" valign="top"> <p>
									When set to <code class="literal">true</code>, disables the verification of the remote host’s certificate and name.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="telemeterclientconfig"><div class="titlepage"><div><div><h3 class="title">8.27. TelemeterClientConfig</h3></div></div></div><section class="section" id="description-26"><div class="titlepage"><div><div><h4 class="title">8.27.1. Description</h4></div></div></div><p>
<code class="literal">TelemeterClientConfig</code> defines settings for the Telemeter Client component.
				</p></section><section class="section" id="required-4"><div class="titlepage"><div><div><h4 class="title">8.27.2. Required</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<code class="literal">nodeSelector</code>
</li><li class="listitem">
<code class="literal">tolerations</code>
</li></ul></div><p>
					Appears in: <a class="link" href="#clustermonitoringconfiguration" title="8.5. ClusterMonitoringConfiguration">ClusterMonitoringConfiguration</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059294249408" scope="col" valign="top">Property</th><th align="left" id="idm140059294248320" scope="col" valign="top">Type</th><th align="left" id="idm140059294247232" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059294249408" valign="top"> <p>
									nodeSelector
								</p>
</td><td align="left" headers="idm140059294248320" valign="top"> <p>
									map[string]string
								</p>
</td><td align="left" headers="idm140059294247232" valign="top"> <p>
									Defines the nodes on which the pods are scheduled.
								</p>
</td></tr><tr><td align="left" headers="idm140059294249408" valign="top"> <p>
									resources
								</p>
</td><td align="left" headers="idm140059294248320" valign="top"> <p>
									*v1.ResourceRequirements
								</p>
</td><td align="left" headers="idm140059294247232" valign="top"> <p>
									Defines resource requests and limits for the <code class="literal">TelemeterClient</code> container.
								</p>
</td></tr><tr><td align="left" headers="idm140059294249408" valign="top"> <p>
									tolerations
								</p>
</td><td align="left" headers="idm140059294248320" valign="top"> <p>
									[]v1.Toleration
								</p>
</td><td align="left" headers="idm140059294247232" valign="top"> <p>
									Defines tolerations for the pods.
								</p>
</td></tr><tr><td align="left" headers="idm140059294249408" valign="top"> <p>
									topologySpreadConstraints
								</p>
</td><td align="left" headers="idm140059294248320" valign="top"> <p>
									[]v1.TopologySpreadConstraint
								</p>
</td><td align="left" headers="idm140059294247232" valign="top"> <p>
									Defines the pod’s topology spread constraints.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="thanosquerierconfig"><div class="titlepage"><div><div><h3 class="title">8.28. ThanosQuerierConfig</h3></div></div></div><section class="section" id="description-27"><div class="titlepage"><div><div><h4 class="title">8.28.1. Description</h4></div></div></div><p>
					The <code class="literal">ThanosQuerierConfig</code> resource defines settings for the Thanos Querier component.
				</p><p>
					Appears in: <a class="link" href="#clustermonitoringconfiguration" title="8.5. ClusterMonitoringConfiguration">ClusterMonitoringConfiguration</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059285771040" scope="col" valign="top">Property</th><th align="left" id="idm140059285769952" scope="col" valign="top">Type</th><th align="left" id="idm140059285768864" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059285771040" valign="top"> <p>
									enableRequestLogging
								</p>
</td><td align="left" headers="idm140059285769952" valign="top"> <p>
									bool
								</p>
</td><td align="left" headers="idm140059285768864" valign="top"> <p>
									A Boolean flag that enables or disables request logging. The default value is <code class="literal">false</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059285771040" valign="top"> <p>
									logLevel
								</p>
</td><td align="left" headers="idm140059285769952" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059285768864" valign="top"> <p>
									Defines the log level setting for Thanos Querier. The possible values are <code class="literal">error</code>, <code class="literal">warn</code>, <code class="literal">info</code>, and <code class="literal">debug</code>. The default value is <code class="literal">info</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059285771040" valign="top"> <p>
									enableCORS
								</p>
</td><td align="left" headers="idm140059285769952" valign="top"> <p>
									bool
								</p>
</td><td align="left" headers="idm140059285768864" valign="top"> <p>
									A Boolean flag that enables setting CORS headers. The headers allow access from any origin. The default value is <code class="literal">false</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059285771040" valign="top"> <p>
									nodeSelector
								</p>
</td><td align="left" headers="idm140059285769952" valign="top"> <p>
									map[string]string
								</p>
</td><td align="left" headers="idm140059285768864" valign="top"> <p>
									Defines the nodes on which the pods are scheduled.
								</p>
</td></tr><tr><td align="left" headers="idm140059285771040" valign="top"> <p>
									resources
								</p>
</td><td align="left" headers="idm140059285769952" valign="top"> <p>
									*v1.ResourceRequirements
								</p>
</td><td align="left" headers="idm140059285768864" valign="top"> <p>
									Defines resource requests and limits for the Thanos Querier container.
								</p>
</td></tr><tr><td align="left" headers="idm140059285771040" valign="top"> <p>
									tolerations
								</p>
</td><td align="left" headers="idm140059285769952" valign="top"> <p>
									[]v1.Toleration
								</p>
</td><td align="left" headers="idm140059285768864" valign="top"> <p>
									Defines tolerations for the pods.
								</p>
</td></tr><tr><td align="left" headers="idm140059285771040" valign="top"> <p>
									topologySpreadConstraints
								</p>
</td><td align="left" headers="idm140059285769952" valign="top"> <p>
									[]v1.TopologySpreadConstraint
								</p>
</td><td align="left" headers="idm140059285768864" valign="top"> <p>
									Defines the pod’s topology spread constraints.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="thanosrulerconfig"><div class="titlepage"><div><div><h3 class="title">8.29. ThanosRulerConfig</h3></div></div></div><section class="section" id="description-28"><div class="titlepage"><div><div><h4 class="title">8.29.1. Description</h4></div></div></div><p>
					The <code class="literal">ThanosRulerConfig</code> resource defines configuration for the Thanos Ruler instance for user-defined projects.
				</p><p>
					Appears in: <a class="link" href="#userworkloadconfiguration" title="8.31. UserWorkloadConfiguration">UserWorkloadConfiguration</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059285715168" scope="col" valign="top">Property</th><th align="left" id="idm140059285714080" scope="col" valign="top">Type</th><th align="left" id="idm140059285712992" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059285715168" valign="top"> <p>
									additionalAlertmanagerConfigs
								</p>
</td><td align="left" headers="idm140059285714080" valign="top"> <p>
									[]<a class="link" href="#additionalalertmanagerconfig" title="8.2. AdditionalAlertmanagerConfig">AdditionalAlertmanagerConfig</a>
</p>
</td><td align="left" headers="idm140059285712992" valign="top"> <p>
									Configures how the Thanos Ruler component communicates with additional Alertmanager instances. The default value is <code class="literal">nil</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059285715168" valign="top"> <p>
									evaluationInterval
								</p>
</td><td align="left" headers="idm140059285714080" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059285712992" valign="top"> <p>
									Configures the default interval between Prometheus rule evaluations in case the <code class="literal">PrometheusRule</code> resource does not specify any value. The interval must be set between 5 seconds and 5 minutes. The value can be expressed in: seconds (for example <code class="literal">30s</code>), minutes (for example <code class="literal">1m</code>) or a mix of minutes and seconds (for example <code class="literal">1m30s</code>). It applies to <code class="literal">PrometheusRule</code> resources without the <code class="literal">openshift.io/prometheus-rule-evaluation-scope=\"leaf-prometheus\"</code> label. The default value is <code class="literal">15s</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059285715168" valign="top"> <p>
									logLevel
								</p>
</td><td align="left" headers="idm140059285714080" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059285712992" valign="top"> <p>
									Defines the log level setting for Thanos Ruler. The possible values are <code class="literal">error</code>, <code class="literal">warn</code>, <code class="literal">info</code>, and <code class="literal">debug</code>. The default value is <code class="literal">info</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059285715168" valign="top"> <p>
									nodeSelector
								</p>
</td><td align="left" headers="idm140059285714080" valign="top"> <p>
									map[string]string
								</p>
</td><td align="left" headers="idm140059285712992" valign="top"> <p>
									Defines the nodes on which the Pods are scheduled.
								</p>
</td></tr><tr><td align="left" headers="idm140059285715168" valign="top"> <p>
									resources
								</p>
</td><td align="left" headers="idm140059285714080" valign="top"> <p>
									*v1.ResourceRequirements
								</p>
</td><td align="left" headers="idm140059285712992" valign="top"> <p>
									Defines resource requests and limits for the Alertmanager container.
								</p>
</td></tr><tr><td align="left" headers="idm140059285715168" valign="top"> <p>
									retention
								</p>
</td><td align="left" headers="idm140059285714080" valign="top"> <p>
									string
								</p>
</td><td align="left" headers="idm140059285712992" valign="top"> <p>
									Defines the duration for which Prometheus retains data. This definition must be specified using the following regular expression pattern: <code class="literal">[0-9]+(ms|s|m|h|d|w|y)</code> (ms = milliseconds, s= seconds,m = minutes, h = hours, d = days, w = weeks, y = years). The default value is <code class="literal">15d</code>.
								</p>
</td></tr><tr><td align="left" headers="idm140059285715168" valign="top"> <p>
									tolerations
								</p>
</td><td align="left" headers="idm140059285714080" valign="top"> <p>
									[]v1.Toleration
								</p>
</td><td align="left" headers="idm140059285712992" valign="top"> <p>
									Defines tolerations for the pods.
								</p>
</td></tr><tr><td align="left" headers="idm140059285715168" valign="top"> <p>
									topologySpreadConstraints
								</p>
</td><td align="left" headers="idm140059285714080" valign="top"> <p>
									[]v1.TopologySpreadConstraint
								</p>
</td><td align="left" headers="idm140059285712992" valign="top"> <p>
									Defines the pod’s topology spread constraints.
								</p>
</td></tr><tr><td align="left" headers="idm140059285715168" valign="top"> <p>
									volumeClaimTemplate
								</p>
</td><td align="left" headers="idm140059285714080" valign="top"> <p>
									*monv1.EmbeddedPersistentVolumeClaim
								</p>
</td><td align="left" headers="idm140059285712992" valign="top"> <p>
									Defines persistent storage for Thanos Ruler. Use this setting to configure the storage class and size of a volume.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="userworkloadconfig"><div class="titlepage"><div><div><h3 class="title">8.30. UserWorkloadConfig</h3></div></div></div><section class="section" id="description-29"><div class="titlepage"><div><div><h4 class="title">8.30.1. Description</h4></div></div></div><p>
					The <code class="literal">UserWorkloadConfig</code> resource defines settings for the monitoring of user-defined projects.
				</p><p>
					Appears in: <a class="link" href="#clustermonitoringconfiguration" title="8.5. ClusterMonitoringConfiguration">ClusterMonitoringConfiguration</a>
</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059287727664" scope="col" valign="top">Property</th><th align="left" id="idm140059287726576" scope="col" valign="top">Type</th><th align="left" id="idm140059287725488" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059287727664" valign="top"> <p>
									rulesWithoutLabelEnforcementAllowed
								</p>
</td><td align="left" headers="idm140059287726576" valign="top"> <p>
									*bool
								</p>
</td><td align="left" headers="idm140059287725488" valign="top"> <p>
									A Boolean flag that enables or disables the ability to deploy user-defined <code class="literal">PrometheusRules</code> objects for which the <code class="literal">namespace</code> label is not enforced to the namespace of the object. Such objects should be created in a namespace configured under the <code class="literal">namespacesWithoutLabelEnforcement</code> property of the <code class="literal">UserWorkloadConfiguration</code> resource. The default value is <code class="literal">true</code>.
								</p>
</td></tr></tbody></table></rh-table></section></section><section class="section" id="userworkloadconfiguration"><div class="titlepage"><div><div><h3 class="title">8.31. UserWorkloadConfiguration</h3></div></div></div><section class="section" id="description-30"><div class="titlepage"><div><div><h4 class="title">8.31.1. Description</h4></div></div></div><p>
					The <code class="literal">UserWorkloadConfiguration</code> resource defines the settings responsible for user-defined projects in the <code class="literal">user-workload-monitoring-config</code> config map in the <code class="literal">openshift-user-workload-monitoring</code> namespace. You can only enable <code class="literal">UserWorkloadConfiguration</code> after you have set <code class="literal">enableUserWorkload</code> to <code class="literal">true</code> in the <code class="literal">cluster-monitoring-config</code> config map under the <code class="literal">openshift-monitoring</code> namespace.
				</p><rh-table><table class="lt-4-cols lt-7-rows"><colgroup><col class="col_1" style="width: 33%; "/><!--Empty--><col class="col_2" style="width: 33%; "/><!--Empty--><col class="col_3" style="width: 33%; "/><!--Empty--></colgroup><thead><tr><th align="left" id="idm140059287702064" scope="col" valign="top">Property</th><th align="left" id="idm140059287700976" scope="col" valign="top">Type</th><th align="left" id="idm140059287699888" scope="col" valign="top">Description</th></tr></thead><tbody><tr><td align="left" headers="idm140059287702064" valign="top"> <p>
									alertmanager
								</p>
</td><td align="left" headers="idm140059287700976" valign="top"> <p>
									*<a class="link" href="#alertmanageruserworkloadconfig" title="8.4. AlertmanagerUserWorkloadConfig">AlertmanagerUserWorkloadConfig</a>
</p>
</td><td align="left" headers="idm140059287699888" valign="top"> <p>
									Defines the settings for the Alertmanager component in user workload monitoring.
								</p>
</td></tr><tr><td align="left" headers="idm140059287702064" valign="top"> <p>
									prometheus
								</p>
</td><td align="left" headers="idm140059287700976" valign="top"> <p>
									*<a class="link" href="#prometheusrestrictedconfig" title="8.24. PrometheusRestrictedConfig">PrometheusRestrictedConfig</a>
</p>
</td><td align="left" headers="idm140059287699888" valign="top"> <p>
									Defines the settings for the Prometheus component in user workload monitoring.
								</p>
</td></tr><tr><td align="left" headers="idm140059287702064" valign="top"> <p>
									prometheusOperator
								</p>
</td><td align="left" headers="idm140059287700976" valign="top"> <p>
									*<a class="link" href="#prometheusoperatorconfig" title="8.22. PrometheusOperatorConfig">PrometheusOperatorConfig</a>
</p>
</td><td align="left" headers="idm140059287699888" valign="top"> <p>
									Defines the settings for the Prometheus Operator component in user workload monitoring.
								</p>
</td></tr><tr><td align="left" headers="idm140059287702064" valign="top"> <p>
									thanosRuler
								</p>
</td><td align="left" headers="idm140059287700976" valign="top"> <p>
									*<a class="link" href="#thanosrulerconfig" title="8.29. ThanosRulerConfig">ThanosRulerConfig</a>
</p>
</td><td align="left" headers="idm140059287699888" valign="top"> <p>
									Defines the settings for the Thanos Ruler component in user workload monitoring.
								</p>
</td></tr><tr><td align="left" headers="idm140059287702064" valign="top"> <p>
									namespacesWithoutLabelEnforcement
								</p>
</td><td align="left" headers="idm140059287700976" valign="top"> <p>
									[]string
								</p>
</td><td align="left" headers="idm140059287699888" valign="top"> <p>
									Defines the list of namespaces for which Prometheus and Thanos Ruler in user-defined monitoring do not enforce the <code class="literal">namespace</code> label value in <code class="literal">PrometheusRule</code> objects.
								</p>
<p>
									The <code class="literal">namespacesWithoutLabelEnforcement</code> property allows users to define recording and alerting rules that can query across multiple projects (not limited to user-defined projects) instead of deploying identical <code class="literal">PrometheusRule</code> objects in each user project.
								</p>
<p>
									To make the resulting alerts and metrics visible to project users, the query expressions should return a <code class="literal">namespace</code> label with a non-empty value.
								</p>
</td></tr></tbody></table></rh-table></section></section></section><section class="chapter" id="shiftstack-prometheus-configuration"><div class="titlepage"><div><div><h2 class="title">Chapter 9. Monitoring clusters that run on RHOSO</h2></div></div></div><p>
			You can correlate observability metrics for clusters that run on Red Hat OpenStack Services on OpenShift (RHOSO). By collecting metrics from both environments, you can monitor and troubleshoot issues across the infrastructure and application layers.
		</p><p>
			There are two supported methods for metric correlation for clusters that run on RHOSO:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<a class="link" href="https://prometheus.io/docs/practices/remote_write/#remote-write-tuning">Remote writing</a> to an external Prometheus instance.
				</li><li class="listitem">
					Collecting data from the OpenShift Container Platform federation endpoint to the RHOSO observability stack.
				</li></ul></div><section class="section" id="monitoring-configuring-shiftstack-remotewrite_shiftstack-prometheus-configuration"><div class="titlepage"><div><div><h3 class="title">9.1. Remote writing to an external Prometheus instance</h3></div></div></div><p>
				Use remote write with both Red Hat OpenStack Services on OpenShift (RHOSO) and OpenShift Container Platform to push their metrics to an external Prometheus instance.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have access to an external Prometheus instance.
					</li><li class="listitem">
						You have administrative access to RHOSO and your cluster.
					</li><li class="listitem">
						You have certificates for secure communication with mTLS.
					</li><li class="listitem">
						Your Prometheus instance is configured for client TLS certificates and has been set up as a remote write receiver.
					</li><li class="listitem">
						The Cluster Observability Operator is installed on your RHOSO cluster.
					</li><li class="listitem">
						The monitoring stack for your RHOSO cluster is configured to collect the metrics that you are interested in.
					</li><li class="listitem"><p class="simpara">
						Telemetry is enabled in the RHOSO environment.
					</p><rh-alert class="admonition note" state="info"><div class="admonition_header" slot="header">Note</div><div><p>
							To verify that the telemetry service is operating normally, entering the following command:
						</p><pre class="programlisting language-shell">$ oc -n openstack get monitoringstacks metric-storage -o yaml</pre><p>
							The <code class="literal">monitoringstacks</code> CRD indicates whether telemetry is enabled correctly.
						</p></div></rh-alert></li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem"><p class="simpara">
						Configure your RHOSO management cluster to send metrics to Prometheus:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Create a secret that is named <code class="literal">mtls-bundle</code> in the <code class="literal">openstack</code> namespace that contains HTTPS client certificates for authentication to Prometheus by entering the following command:
							</p><pre class="programlisting language-shell">$ oc --namespace openstack \
    create secret generic mtls-bundle \
        --from-file=./ca.crt \
        --from-file=osp-client.crt \
        --from-file=osp-client.key</pre></li><li class="listitem"><p class="simpara">
								Open the <code class="literal">controlplane</code> configuration for editing by running the following command:
							</p><pre class="programlisting language-shell">$ oc -n openstack edit openstackcontrolplane/controlplane</pre></li><li class="listitem"><p class="simpara">
								With the configuration open, replace the <code class="literal">.spec.telemetry.template.metricStorage</code> section so that RHOSO sends metrics to Prometheus. As an example:
							</p><pre class="programlisting language-yaml">      metricStorage:
        customMonitoringStack:
          alertmanagerConfig:
            disabled: false
          logLevel: info
          prometheusConfig:
            scrapeInterval: 30s
            remoteWrite:
            - url: https://external-prometheus.example.com/api/v1/write <span id="CO90-1"><!--Empty--></span><span class="callout">1</span>
              tlsConfig:
                ca:
                  secret:
                    name: mtls-bundle
                    key: ca.crt
                cert:
                  secret:
                    name: mtls-bundle
                    key: ocp-client.crt
                keySecret:
                  name: mtls-bundle
                  key: ocp-client.key
            replicas: 2
          resourceSelector:
            matchLabels:
              service: metricStorage
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 256Mi
          retention: 1d <span id="CO90-2"><!--Empty--></span><span class="callout">2</span>
        dashboardsEnabled: false
        dataplaneNetwork: ctlplane
        enabled: true
        prometheusTls: {}</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO90-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Replace this URL with the URL of your Prometheus instance.
									</div></dd><dt><a href="#CO90-2"><span class="callout">2</span></a> </dt><dd><div class="para">
										Set a retention period. Optionally, you can reduce retention for local metrics because of external collection.
									</div></dd></dl></div></li></ol></div></li><li class="listitem"><p class="simpara">
						Configure the tenant cluster on which your workloads run to send metrics to Prometheus:
					</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p class="simpara">
								Create a cluster monitoring config map as a YAML file. The map must include a remote write configuration and cluster identifiers. As an example:
							</p><pre class="programlisting language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      retention: 1d <span id="CO91-1"><!--Empty--></span><span class="callout">1</span>
      remoteWrite:
      - url: "https://external-prometheus.example.com/api/v1/write"
        writeRelabelConfigs:
        - sourceLabels:
          - __tmp_openshift_cluster_id__
          targetLabel: cluster_id
          action: replace
        tlsConfig:
          ca:
            secret:
              name: mtls-bundle
              key: ca.crt
          cert:
            secret:
              name: mtls-bundle
              key: ocp-client.crt
          keySecret:
            name: mtls-bundle
            key: ocp-client.key</pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO91-1"><span class="callout">1</span></a> </dt><dd><div class="para">
										Set a retention period. Optionally, you can reduce retention for local metrics because of external collection.
									</div></dd></dl></div></li><li class="listitem">
								Save the config map as a file called <code class="literal">cluster-monitoring-config.yaml</code>.
							</li><li class="listitem"><p class="simpara">
								Create a secret that is named <code class="literal">mtls-bundle</code> in the <code class="literal">openshift-monitoring</code> namespace that contains HTTPS client certificates for authentication to Prometheus by entering the following command:
							</p><pre class="programlisting language-terminal">$ oc --namespace openshift-monitoring \
    create secret generic mtls-bundle \
        --from-file=./ca.crt \
        --from-file=ocp-client.crt \
        --from-file=ocp-client.key</pre></li><li class="listitem"><p class="simpara">
								Apply the cluster monitoring configuration by running the following command:
							</p><pre class="programlisting language-terminal">$ oc apply -f cluster-monitoring-config.yaml</pre></li></ol></div></li></ol></div><p>
				After the changes propagate, you can see aggregated metrics in your external Prometheus instance.
			</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#configuring-remote-write-storage_configuring-metrics-uwm" title="4.4.1. Configuring remote write storage">Configuring remote write storage</a>
</li><li class="listitem">
<a class="link" href="#adding-cluster-id-labels-to-metrics_key-concepts" title="1.3.3.2. Adding cluster ID labels to metrics">Adding cluster ID labels to metrics</a>
</li></ul></div></section><section class="section" id="monitoring-configuring-shiftstack-scraping_shiftstack-prometheus-configuration"><div class="titlepage"><div><div><h3 class="title">9.2. Collecting cluster metrics from the federation endpoint</h3></div></div></div><p>
				You can employ the federation endpoint of your OpenShift Container Platform cluster to make metrics available to a Red Hat OpenStack Services on OpenShift (RHOSO) cluster to practice pull-based monitoring.
			</p><div class="itemizedlist"><p class="title"><strong>Prerequisites</strong></p><ul class="itemizedlist" type="disc"><li class="listitem">
						You have administrative access to RHOSO and the tenant cluster that is running on it.
					</li><li class="listitem">
						Telemetry is enabled in the RHOSO environment.
					</li><li class="listitem">
						The Cluster Observability Operator is installed on your cluster.
					</li><li class="listitem">
						The monitoring stack for your cluster is configured.
					</li><li class="listitem">
						Your cluster has its federation endpoint exposed.
					</li></ul></div><div class="orderedlist"><p class="title"><strong>Procedure</strong></p><ol class="orderedlist" type="1"><li class="listitem">
						Connect to your cluster by using a username and password; do not log in by using a <code class="literal">kubeconfig</code> file that was generated by the installation program.
					</li><li class="listitem"><p class="simpara">
						To retrieve a token from the OpenShift Container Platform cluster, run the following command on it:
					</p><pre class="programlisting language-terminal">$ oc whoami -t</pre></li><li class="listitem"><p class="simpara">
						Make the token available as a secret in the <code class="literal">openstack</code> namespace in the RHOSO management cluster by running the following command:
					</p><pre class="programlisting language-terminal">$ oc -n openstack create secret generic ocp-federated --from-literal=token=&lt;the_token_fetched_previously&gt;</pre></li><li class="listitem"><p class="simpara">
						To get the Prometheus federation route URL from your OpenShift Container Platform cluster, run the following command:
					</p><pre class="programlisting language-terminal">$ oc -n openshift-monitoring get route prometheus-k8s-federate -ojsonpath={'.status.ingress[].host'}</pre></li><li class="listitem"><p class="simpara">
						Write a manifest for a scrape configuration and save it as a file called <code class="literal">cluster-scrape-config.yaml</code>. As an example:
					</p><pre class="programlisting language-yaml">apiVersion: monitoring.rhobs/v1alpha1
kind: ScrapeConfig
metadata:
  labels:
    service: metricStorage
  name: sos1-federated
  namespace: openstack
spec:
  params:
    'match[]':
    - '{__name__=~"kube_node_info|kube_persistentvolume_info|cluster:master_nodes"}' <span id="CO92-1"><!--Empty--></span><span class="callout">1</span>
  metricsPath: '/federate'
  authorization:
    type: Bearer
    credentials:
      name: ocp-federated <span id="CO92-2"><!--Empty--></span><span class="callout">2</span>
      key: token
  scheme: HTTPS # or HTTP
  scrapeInterval: 30s <span id="CO92-3"><!--Empty--></span><span class="callout">3</span>
  staticConfigs:
  - targets:
    - prometheus-k8s-federate-openshift-monitoring.apps.openshift.example <span id="CO92-4"><!--Empty--></span><span class="callout">4</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO92-1"><span class="callout">1</span></a> </dt><dd><div class="para">
								Add metrics here. In this example, only the metrics <code class="literal">kube_node_info</code>, <code class="literal">kube_persistentvolume_info</code>, and <code class="literal">cluster:master_nodes</code> are requested.
							</div></dd><dt><a href="#CO92-2"><span class="callout">2</span></a> </dt><dd><div class="para">
								Insert the previously generated secret name here.
							</div></dd><dt><a href="#CO92-3"><span class="callout">3</span></a> </dt><dd><div class="para">
								Limit scraping to fewer than 1000 samples for each request with a maximum frequency of once every 30 seconds.
							</div></dd><dt><a href="#CO92-4"><span class="callout">4</span></a> </dt><dd><div class="para">
								Insert the URL you fetched previously here. If the endpoint is HTTPS and uses a custom certificate authority, add a <code class="literal">tlsConfig</code> section after it.
							</div></dd></dl></div></li><li class="listitem"><p class="simpara">
						While connected to the RHOSO management cluster, apply the manifest by running the following command:
					</p><pre class="programlisting language-terminal">$ oc apply -f cluster-scrape-config.yaml</pre></li></ol></div><p>
				After the config propagates, the cluster metrics are accessible for querying in the OpenShift Container Platform UI in RHOSO.
			</p><div class="itemizedlist _additional-resources"><p class="title"><strong>Additional resources</strong></p><ul class="itemizedlist _additional-resources" type="disc"><li class="listitem">
<a class="link" href="#monitoring-querying-metrics-by-using-the-federation-endpoint-for-prometheus_accessing-monitoring-apis-by-using-the-cli" title="5.3.3. Querying metrics by using the federation endpoint for Prometheus">Querying metrics by using the federation endpoint for Prometheus</a>
</li></ul></div></section><section class="section" id="monitoring-shiftstack-metrics.adoc_shiftstack-prometheus-configuration"><div class="titlepage"><div><div><h3 class="title">9.3. Available metrics for clusters that run on RHOSO</h3></div></div></div><p>
				To query metrics and identifying resources across the stack, there are helper metrics that establish a correlation between Red Hat OpenStack Services on OpenShift (RHOSO) infrastructure resources and their representations in the tenant OpenShift Container Platform cluster.
			</p><p>
				To map nodes with RHOSO compute instances, in the metric <code class="literal">kube_node_info</code>:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<code class="literal">node</code> is the Kubernetes node name.
					</li><li class="listitem">
<code class="literal">provider_id</code> contains the identifier of the corresponding compute service instance.
					</li></ul></div><p>
				To map persistent volumes with RHOSO block storage or shared filesystems shares, in the metric <code class="literal">kube_persistentvolume_info</code>:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
<code class="literal">persistentvolume</code> is the volume name.
					</li><li class="listitem">
<code class="literal">csi_volume_handle</code> is the block storage volume or share identifier.
					</li></ul></div><p>
				By default, the compute machines that back the cluster control plane nodes are created in a server group with a soft anti-affinity policy. As a result, the compute service creates them on separate hypervisors on a best-effort basis. However, if the state of the RHOSO cluster is not appropriate for this distribution, the machines are created anyway.
			</p><p>
				In combination with the default soft anti-affinity policy, you can configure an alert that activates when a hypervisor hosts more than one control plane node of a given cluster to highlight the degraded level of high availability.
			</p><p>
				As an example, this PromQL query returns the number of OpenShift Container Platform master nodes per RHOSP host:
			</p><pre class="programlisting language-promql">sum by (vm_instance) (
  group by (vm_instance, resource) (ceilometer_cpu)
    / on (resource) group_right(vm_instance) (
      group by (node, resource) (
        label_replace(kube_node_info, "resource", "$1", "system_uuid", "(.+)")
      )
    / on (node) group_left group by (node) (
      cluster:master_nodes
    )
  )
)</pre></section></section></body></html>
